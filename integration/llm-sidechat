#!/bin/sh
''''exec "$HOME/.local/share/uv/tools/llm/bin/python3" "$0" "$@" # '''
"""
llm-sidechat - Terminal AI Assistant for Terminator

A Terminator-integrated AI assistant that provides:
- Automatic Exec terminal creation
- Terminal content capture and analysis
- Command execution with approval
- Watch Mode for proactive monitoring
- Context squashing to manage token limits
- Streaming responses with markdown rendering
- Robust error handling and recovery

Inspired by TmuxAI but designed for Terminator terminal emulator.

Author: c0ffee0wl
License: GPL v2 only
"""

# Add system site-packages to path for Terminator libraries
import sys
sys.path.insert(0, '/usr/lib/python3/dist-packages')

import llm
from llm.cli import load_template
import os
import re
import time
import asyncio
import threading
import dbus
import fcntl
import signal
import atexit
from pathlib import Path
from rich.console import Console
from rich.live import Live
from rich.markdown import Markdown
from rich.panel import Panel
from rich.prompt import Confirm, Prompt
from rich.text import Text
from typing import List, Optional, Tuple, Dict


class PromptDetector:
    """Detect shell prompts to determine command completion"""

    # Common prompt patterns
    BASH_PROMPT = re.compile(r'[\$#]\s*$')
    ZSH_PROMPT = re.compile(r'[%❯→➜]\s*$')
    # Kali two-line prompt
    KALI_PROMPT = re.compile(r'┌──.*\n└─[\$#]\s*$')
    # Generic fallback
    GENERIC_PROMPT = re.compile(r'^[\w@\-]+[:#\$>]\s*$', re.MULTILINE)

    @staticmethod
    def detect_prompt(text: str) -> bool:
        """Check if text ends with a shell prompt"""
        if not text or len(text.strip()) == 0:
            return False

        # Get last few lines
        lines = text.strip().split('\n')
        last_line = lines[-1] if lines else ""
        last_two = '\n'.join(lines[-2:]) if len(lines) >= 2 else last_line

        # Check patterns
        if PromptDetector.BASH_PROMPT.search(last_line):
            return True
        if PromptDetector.ZSH_PROMPT.search(last_line):
            return True
        if PromptDetector.KALI_PROMPT.search(last_two):
            return True
        if PromptDetector.GENERIC_PROMPT.search(last_line):
            return True

        return False


class TerminatorSidechatSession:
    """Main sidechat session manager for Terminator"""

    def __init__(self, model_name: Optional[str] = None):
        self.console = Console()

        # Initialize shutdown state and lock file handle
        self._shutdown_initiated = False
        self.lock_file = None

        # Acquire instance lock FIRST (before any other initialization)
        self._acquire_instance_lock()

        # Register shutdown handlers EARLY (before creating resources)
        self._register_shutdown_handlers()

        self.model_name = model_name or self._get_default_model()

        try:
            self.model = llm.get_model(self.model_name)
        except Exception as e:
            self.console.print(f"[red]Error loading model '{self.model_name}': {e}[/]")
            self.console.print("[yellow]Available models:[/]")
            for model in llm.get_models():
                self.console.print(f"  - {model.model_id}")
            sys.exit(1)

        self.conversation = llm.Conversation(model=self.model)

        # Store system prompt separately for reuse
        try:
            template = load_template("terminator-sidechat")
            # Use system if available, otherwise use prompt
            self.system_prompt = template.system or template.prompt
            if not self.system_prompt:
                raise ValueError("Template has no system or prompt field")
        except Exception as e:
            self.console.print(f"[yellow]⚠ Could not load terminator-sidechat template: {e}[/]")
            self.console.print("[yellow]Using basic system prompt. Install template with: ./install-llm-tools.sh[/]")
            self.system_prompt = "You are an AI terminal assistant. Provide commands in ```bash blocks."

        # Store original system prompt for context squashing
        # This prevents infinite growth when squashing multiple times
        self.original_system_prompt = self.system_prompt

        # Terminal tracking
        self.chat_terminal_uuid = None
        self.exec_terminal_uuid = None

        # Context management (like tmuxai)
        self.max_context_size = 100000  # tokens
        self.context_squash_threshold = 0.8  # 80%

        # Watch mode (thread-safe)
        self.watch_mode = False
        self.watch_goal = None
        self.watch_thread = None
        self.watch_interval = 5  # seconds
        self.watch_lock = threading.Lock()
        self.event_loop = None

        # Plugin and D-Bus connections
        self.plugin = None
        self.dbus_service = None

    def _acquire_instance_lock(self):
        """
        Ensure only one sidechat instance per user session.
        Uses file locking with automatic cleanup on process exit.
        """
        # Use runtime directory for lock file (XDG_RUNTIME_DIR or /tmp)
        runtime_dir = os.environ.get('XDG_RUNTIME_DIR', '/tmp')
        lock_path = Path(runtime_dir) / 'llm-sidechat.lock'

        try:
            # Open lock file (create if doesn't exist)
            self.lock_file = open(lock_path, 'w')

            # Try to acquire exclusive lock (non-blocking)
            fcntl.flock(self.lock_file.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)

            # Write PID for debugging (optional but helpful)
            self.lock_file.write(f"{os.getpid()}\n")
            self.lock_file.flush()

        except BlockingIOError:
            # Lock is held by another instance
            self.console.print("[red]Error: Another sidechat instance is already running[/]")
            self.console.print("[yellow]Only one sidechat session can run at a time.[/]")
            self.console.print(f"[yellow]Lock file: {lock_path}[/]")

            # Try to read PID from lock file
            try:
                with open(lock_path, 'r') as f:
                    pid = f.read().strip()
                    if pid:
                        self.console.print(f"[yellow]Existing instance PID: {pid}[/]")
            except:
                pass

            sys.exit(1)

        except Exception as e:
            self.console.print(f"[red]Error acquiring lock: {e}[/]")
            sys.exit(1)

    def _release_instance_lock(self):
        """Release the instance lock (automatic on process exit, but explicit is better)"""
        if self.lock_file:
            try:
                fcntl.flock(self.lock_file.fileno(), fcntl.LOCK_UN)
                self.lock_file.close()
                self.lock_file = None
            except Exception:
                pass  # Lock will be released by kernel anyway

    def _register_shutdown_handlers(self):
        """Register signal handlers and atexit hook for cleanup"""
        # Register atexit fallback (runs on normal exit)
        atexit.register(self._shutdown)

        # Register signal handlers for graceful shutdown
        signal.signal(signal.SIGINT, self._signal_handler)   # Ctrl+C
        signal.signal(signal.SIGTERM, self._signal_handler)  # kill command
        signal.signal(signal.SIGHUP, self._signal_handler)   # Terminal closed

    def _signal_handler(self, signum, frame):
        """Handle termination signals"""
        signal_names = {
            signal.SIGINT: "SIGINT (Ctrl+C)",
            signal.SIGTERM: "SIGTERM",
            signal.SIGHUP: "SIGHUP (terminal closed)"
        }
        signal_name = signal_names.get(signum, f"signal {signum}")

        self.console.print(f"\n[yellow]Received {signal_name}, shutting down...[/]")
        self._shutdown()
        sys.exit(0)  # Exit cleanly

    def _shutdown(self):
        """
        Unified shutdown method - called by signal handlers, atexit, or manual exit.
        Idempotent - safe to call multiple times.
        """
        # Prevent double-cleanup
        if self._shutdown_initiated:
            return
        self._shutdown_initiated = True

        try:
            # STEP 1: Stop watch mode (most critical - prevents new threads)
            if hasattr(self, 'watch_mode') and self.watch_mode:
                try:
                    with self.watch_lock:
                        self.watch_mode = False
                        if self.event_loop:
                            try:
                                self.event_loop.call_soon_threadsafe(self.event_loop.stop)
                            except Exception:
                                pass

                    # Wait for watch thread to finish (with timeout)
                    if self.watch_thread and self.watch_thread.is_alive():
                        self.watch_thread.join(timeout=2.0)
                except Exception:
                    # Don't let watch mode cleanup prevent other cleanup
                    pass

            # STEP 2: Clear plugin cache (if available)
            if hasattr(self, 'plugin_dbus'):
                try:
                    self.plugin_dbus.clear_cache()
                except Exception:
                    pass

            # STEP 3: Close D-Bus connections (graceful disconnect)
            # D-Bus connections don't need explicit cleanup in Python - garbage collected
            # But we can dereference them to signal intent
            if hasattr(self, 'dbus_service'):
                self.dbus_service = None
            if hasattr(self, 'plugin_dbus'):
                self.plugin_dbus = None

            # STEP 4: Release instance lock (most important)
            self._release_instance_lock()

            # STEP 5: Save conversation state (if needed)
            # llm library auto-saves, but we can explicitly flush here
            if hasattr(self, 'conversation'):
                try:
                    # Force any pending writes to complete
                    # llm Conversation doesn't have explicit save(), it auto-saves
                    pass
                except Exception:
                    pass

        except Exception as e:
            # Ensure we log errors but don't prevent shutdown
            try:
                self.console.print(f"[yellow]Warning during shutdown: {e}[/]")
            except:
                # If console fails, write to stderr
                print(f"Warning during shutdown: {e}", file=sys.stderr)

    def _get_default_model(self) -> str:
        """Get default model from llm configuration"""
        try:
            return llm.get_default_model()
        except Exception:
            return "azure/gpt-4.1-mini"

    def _normalize_uuid(self, uuid_value) -> str:
        """Normalize UUID to Python string (ensures dbus.String -> str conversion)"""
        if uuid_value is None or uuid_value == '':
            return None
        # Explicitly convert to Python str (handles dbus.String)
        return str(uuid_value)

    def _reconnect_dbus(self) -> bool:
        """Attempt to reconnect to Terminator D-Bus"""
        try:
            bus = dbus.SessionBus()

            # Discover actual Terminator service name (includes UUID suffix)
            service_name = None
            for name in bus.list_names():
                if name.startswith('net.tenshu.Terminator2'):
                    service_name = name
                    break

            if not service_name:
                service_name = 'net.tenshu.Terminator2'  # Fallback for older versions

            self.dbus_service = bus.get_object(service_name, '/net/tenshu/Terminator2')
            return True
        except Exception as e:
            self.console.print(f"[red]D-Bus reconnection failed: {e}[/]")
            return False

    def _check_dbus_connection(self) -> bool:
        """Verify D-Bus is still connected"""
        try:
            # Try a simple D-Bus operation
            self.dbus_service.get_terminals()
            return True
        except Exception:
            return False

    def _connect_to_terminator(self):
        """Connect to Terminator and plugin via D-Bus"""
        # Connect to Terminator's main D-Bus service for terminal management
        if not self._reconnect_dbus():
            self.console.print("[red]Error: Could not connect to Terminator D-Bus service[/]")
            self.console.print("Ensure Terminator is running with D-Bus enabled")
            sys.exit(1)

        # Connect to plugin's D-Bus service for terminal content/commands
        if not self._connect_to_plugin_dbus():
            self.console.print("[red]Error: Plugin D-Bus service not available[/]")
            self.console.print("Ensure TerminatorSidechatPlugin is:")
            self.console.print("  1. Installed in ~/.config/terminator/plugins/")
            self.console.print("  2. Enabled in Terminator Preferences > Plugins")
            self.console.print("  3. Terminator has been restarted after enabling")
            sys.exit(1)

    def _connect_to_plugin_dbus(self) -> bool:
        """Connect to plugin's D-Bus service"""
        try:
            bus = dbus.SessionBus()
            self.plugin_dbus = bus.get_object(
                'net.tenshu.Terminator2.Sidechat',
                '/net/tenshu/Terminator2/Sidechat'
            )
            return True
        except Exception as e:
            self.console.print(f"[yellow]Plugin D-Bus connection failed: {e}[/]")
            return False

    def _check_plugin_available(self) -> bool:
        """Verify plugin D-Bus service is available"""
        try:
            # Try a simple D-Bus call to check if service is alive
            self.plugin_dbus.get_focused_terminal_uuid()
            return True
        except Exception:
            return False

    def _reconnect_plugin(self) -> bool:
        """Attempt to reconnect to plugin D-Bus service"""
        return self._connect_to_plugin_dbus()

    def setup_terminals(self):
        """Auto-create Exec terminal with retry logic"""
        self.console.print("[cyan]Setting up terminals...[/]")

        max_retries = 3
        for attempt in range(max_retries):
            try:
                # Check D-Bus connection
                if not self._check_dbus_connection():
                    if not self._reconnect_dbus():
                        raise Exception("D-Bus reconnection failed")

                # Get current (Chat) terminal UUID
                focused_terminal = self.dbus_service.get_focused_terminal()
                if not focused_terminal:
                    raise Exception("No terminal is currently focused. Please focus a terminal and try again.")
                self.chat_terminal_uuid = self._normalize_uuid(focused_terminal)

                # Split vertically to create Exec terminal (to the right)
                exec_uuid = self.dbus_service.vsplit(
                    self.chat_terminal_uuid,
                    dbus.Dictionary({
                        'title': 'Sidechat: Exec'
                    }, signature='ss')
                )
                if str(exec_uuid).startswith('ERROR'):
                    raise Exception(f"Failed to split terminal: {exec_uuid}")
                self.exec_terminal_uuid = self._normalize_uuid(exec_uuid)

                self.console.print(f"[green]✓[/] Chat terminal: {self.chat_terminal_uuid[:8]}...")
                self.console.print(f"[green]✓[/] Exec terminal: {self.exec_terminal_uuid[:8]}...")
                return  # Success

            except Exception as e:
                if attempt < max_retries - 1:
                    self.console.print(f"[yellow]Retry {attempt+1}/{max_retries}: {e}[/]")
                    time.sleep(1)
                else:
                    self.console.print(f"[red]Failed to setup terminals after {max_retries} attempts: {e}[/]")
                    sys.exit(1)

    def _verify_exec_terminal(self) -> bool:
        """Check if exec terminal still exists"""
        try:
            terminals = self.plugin_dbus.get_all_terminals_metadata()

            # DEBUG: Show what we're comparing
            self.console.print(f"[yellow]DEBUG: Looking for exec UUID:[/] {repr(self.exec_terminal_uuid)} (type: {type(self.exec_terminal_uuid).__name__})")
            self.console.print(f"[yellow]DEBUG: Plugin returned {len(terminals)} terminals[/]")
            for t in terminals:
                self.console.print(f"[yellow]  - {repr(t['uuid'])} (type: {type(t['uuid']).__name__}) | Title: {t.get('title', 'N/A')}[/]")
                if t['uuid'] == self.exec_terminal_uuid:
                    self.console.print(f"[green]    ✓ EXACT MATCH![/]")

            # Normalize both sides for comparison
            exec_uuid_normalized = self._normalize_uuid(self.exec_terminal_uuid)
            return any(self._normalize_uuid(t['uuid']) == exec_uuid_normalized for t in terminals)
        except Exception as e:
            self.console.print(f"[red]DEBUG: Verification error: {e}[/]")
            return False

    def _recreate_exec_terminal(self) -> bool:
        """Recreate exec terminal if closed"""
        try:
            self.console.print("[yellow]Recreating Exec terminal...[/]")

            # Get current focused terminal
            focused_terminal = self.dbus_service.get_focused_terminal()
            if not focused_terminal:
                raise Exception("No terminal is currently focused. Cannot recreate Exec terminal.")
            current_uuid = str(focused_terminal)

            # Create new exec terminal (split to the right)
            exec_uuid = self.dbus_service.vsplit(
                current_uuid,
                dbus.Dictionary({'title': 'Sidechat: Exec (Restored)'}, signature='ss')
            )
            if str(exec_uuid).startswith('ERROR'):
                raise Exception(f"Failed to split terminal: {exec_uuid}")

            self.exec_terminal_uuid = self._normalize_uuid(exec_uuid)
            self.console.print(f"[green]✓[/] Exec terminal restored: {exec_uuid[:8]}...")
            return True

        except Exception as e:
            self.console.print(f"[red]Failed to recreate exec terminal: {e}[/]")
            return False

    def capture_context(self, include_exec_output=False) -> str:
        """
        Capture visible content from all terminals (like tmuxai).

        Excludes:
        - Chat terminal (where sidechat is running) - to avoid self-reference
        - Exec terminal (unless include_exec_output=True)

        Returns:
            Formatted context string with XML-wrapped terminal content
        """
        try:
            terminals = self.plugin_dbus.get_all_terminals_metadata()
            context_parts = []

            for term in terminals:
                # Skip chat terminal (self-awareness)
                if term['uuid'] == self.chat_terminal_uuid:
                    continue

                # Optionally skip exec terminal
                if not include_exec_output and term['uuid'] == self.exec_terminal_uuid:
                    continue

                # Capture visible content (auto-detect viewport size)
                content = self.plugin_dbus.capture_terminal_content(term['uuid'], -1)

                if content and not content.startswith('ERROR'):
                    # Format like tmux-fragments
                    context_parts.append(f'''<terminal uuid="{term['uuid']}" title="{term['title']}" cwd="{term['cwd']}">
{content}
</terminal>''')

            return "\n\n".join(context_parts)
        except Exception as e:
            self.console.print(f"[red]Error capturing context: {e}[/]")
            return ""

    def estimate_tokens(self) -> int:
        """Estimate token count from conversation responses and system prompt"""
        total_chars = 0
        try:
            # Count system prompt
            total_chars += len(self.system_prompt)

            # Count all responses (includes prompts and AI responses)
            for response in self.conversation.responses:
                # Count the prompt
                if hasattr(response, 'prompt'):
                    prompt_obj = response.prompt
                    if hasattr(prompt_obj, 'prompt'):
                        total_chars += len(str(prompt_obj.prompt))
                    if hasattr(prompt_obj, 'system') and prompt_obj.system:
                        total_chars += len(str(prompt_obj.system))

                # Count the response text
                if hasattr(response, '_chunks'):
                    total_chars += sum(len(chunk) for chunk in response._chunks)
        except Exception as e:
            self.console.print(f"[yellow]Warning: Token estimation failed: {e}[/]")
        return total_chars // 4

    def check_and_squash_context(self):
        """Auto-squash when context reaches threshold (like tmuxai)"""
        current_tokens = self.estimate_tokens()

        if current_tokens >= self.max_context_size * self.context_squash_threshold:
            self.console.print("[yellow]Context approaching limit, auto-squashing...[/]")
            self.squash_context()

    def squash_context(self):
        """Compress earlier messages into summary (like tmuxai)"""
        if len(self.conversation.responses) <= 5:  # Keep at least 5 recent exchanges
            self.console.print("[yellow]Too few messages to squash[/]")
            return

        try:
            # Get responses to squash (all but last 3 - we'll re-execute those)
            responses_to_squash = self.conversation.responses[:-3]

            # Build summary from old responses
            summary_parts = []
            for i, response in enumerate(responses_to_squash, 1):
                # Extract prompt text
                prompt_text = ""
                if hasattr(response, 'prompt') and hasattr(response.prompt, '_prompt'):
                    prompt_text = response.prompt._prompt or ""

                # Extract response text
                response_text = ""
                if hasattr(response, '_chunks'):
                    response_text = "".join(response._chunks)

                if prompt_text:
                    summary_parts.append(f"{i}. User: {prompt_text[:200]}...")
                if response_text:
                    summary_parts.append(f"{i}. AI: {response_text[:200]}...")

            # Generate summary using a standalone prompt (not in conversation)
            summary_prompt = f"""Summarize this conversation history concisely, focusing on:
- Key technical issues discussed
- Commands executed and their results
- Important decisions or insights
- Relevant context for continuing the conversation

Previous messages:
{chr(10).join(summary_parts)}

Provide a brief but comprehensive summary."""

            summary_response = self.model.prompt(summary_prompt)
            summary = summary_response.text()

            # Create new conversation and update system prompt
            # Build enhanced system prompt from ORIGINAL, not current
            # This prevents infinite growth when squashing multiple times
            self.system_prompt = f"""{self.original_system_prompt}

[Previous conversation summary]
{summary}
[End of summary]"""

            # Create completely fresh conversation
            # We'll pass the new system prompt on the next user interaction
            self.conversation = llm.Conversation(model=self.model)

            new_tokens = self.estimate_tokens()
            self.console.print(f"[green]✓[/] Context squashed: ~{new_tokens} tokens")
            self.console.print(f"[cyan]Started fresh conversation with summary in system prompt[/]")

        except Exception as e:
            self.console.print(f"[red]Error squashing context: {e}[/]")

    def extract_commands(self, text: str) -> List[str]:
        """Extract bash commands from ```bash code blocks"""
        # Flexible pattern: allows whitespace after ```bash and optional newline before closing ```
        pattern = r'```bash\s*\n(.*?)\n?```'
        matches = re.findall(pattern, text, re.DOTALL)
        return [cmd.strip() for cmd in matches if cmd.strip()]

    def wait_for_command_completion(self, timeout=30) -> Tuple[bool, str]:
        """
        Wait for command to complete by detecting prompt return.

        Returns:
            (completed, output): Boolean + captured output
        """
        start_time = time.time()
        last_output = ""
        stable_count = 0
        poll_interval = 0.2  # 200ms

        while time.time() - start_time < timeout:
            # Capture current exec terminal content
            try:
                # Clear cache to get fresh content
                try:
                    self.plugin_dbus.clear_cache()
                except:
                    pass  # Ignore if method doesn't exist

                current_output = self.plugin_dbus.capture_terminal_content(
                    self.exec_terminal_uuid,
                    500  # Capture 500 lines including scrollback
                )

                if current_output.startswith('ERROR'):
                    return False, current_output

                # Check for prompt
                if PromptDetector.detect_prompt(current_output):
                    return True, current_output

                # Check for stable output (no changes)
                if current_output == last_output:
                    stable_count += 1
                    if stable_count >= 5:  # 1 second of no changes
                        return True, current_output
                else:
                    stable_count = 0
                    last_output = current_output

                time.sleep(poll_interval)
            except Exception as e:
                self.console.print(f"[yellow]Warning during wait: {e}[/]")
                time.sleep(poll_interval)

        # Timeout reached
        return False, last_output

    def execute_command(self, command: str) -> bool:
        """
        Execute command in Exec terminal with user approval and intelligent completion detection.

        Returns:
            True if executed, False if skipped
        """
        self.console.print(Panel(
            Text(command, style="bold cyan"),
            title="[bold]Command to Execute[/]",
            border_style="cyan"
        ))

        # Ask for approval
        choice = Prompt.ask(
            "Execute in Exec terminal?",
            choices=["y", "n", "e"],  # yes, no, edit
            default="y"
        )

        if choice == "n":
            return False

        if choice == "e":
            # Allow editing
            edited = Prompt.ask("Edit command", default=command)
            command = edited

        # Verify exec terminal exists
        if not self._verify_exec_terminal():
            self.console.print("[yellow]Exec terminal not found[/]")
            if not self._recreate_exec_terminal():
                return False

        # Send to Exec terminal
        try:
            success = self.plugin_dbus.send_keys_to_terminal(
                self.exec_terminal_uuid,
                command,
                execute=True
            )

            if success:
                self.console.print("[green]✓[/] Command sent to Exec terminal")

                # Intelligent wait with progress indicator
                with self.console.status("[cyan]Waiting for command to complete..."):
                    completed, output = self.wait_for_command_completion(timeout=30)

                if completed:
                    self.console.print("[green]✓[/] Command completed")
                    return True, output  # Return the captured output
                else:
                    self.console.print("[yellow]⚠[/] Command still running or timed out")
                    return True, output  # Return whatever we captured

                return True, ""  # Fallback
            else:
                self.console.print("[red]✗[/] Failed to send command")
                return False, ""
        except Exception as e:
            self.console.print(f"[red]Error executing command: {e}[/]")
            return False, ""

    def _start_watch_mode_thread(self):
        """Start watch mode in a background thread with its own event loop"""
        def watch_thread_target():
            self.event_loop = asyncio.new_event_loop()
            asyncio.set_event_loop(self.event_loop)
            try:
                self.event_loop.run_until_complete(self.watch_loop())
            except Exception as e:
                self.console.print(f"[red]Watch mode error: {e}[/]")
            finally:
                self.event_loop.close()

        self.watch_thread = threading.Thread(target=watch_thread_target, daemon=True)
        self.watch_thread.start()

    async def watch_loop(self):
        """Background monitoring of all terminals (like tmuxai watch mode)"""
        while self.watch_mode:
            try:
                # Capture all terminal content (including exec output for watch)
                context = self.capture_context(include_exec_output=True)

                if not context.strip():
                    await asyncio.sleep(self.watch_interval)
                    continue

                # Send to AI with watch goal
                prompt = f"""[Watch Mode] Goal: {self.watch_goal}

Current terminal state:
{context}

Based on the watch goal, analyze the terminal activity and provide suggestions ONLY if you observe something relevant to the goal. If everything looks normal, respond with "OK" only."""

                # Use conversation.prompt() for watch mode with thread safety
                # Note: We don't pass system prompt here as it should already be in conversation
                try:
                    # Keep lock held for entire response lifecycle to ensure thread safety
                    with self.watch_lock:
                        response = self.conversation.prompt(prompt)
                        # Consume response while holding lock to prevent concurrent modifications
                        response_text = response.text()

                    # Only show if AI has actionable feedback (not just "OK")
                    if response_text.strip() and response_text.strip().lower() != "ok":
                        self.console.print()
                        self.console.print(Panel(
                            Markdown(response_text),
                            title="[bold yellow]⚠ Watch Mode Alert[/]",
                            border_style="yellow"
                        ))
                        self.console.print()
                except Exception as response_error:
                    self.console.print(f"[yellow]Watch mode response error: {response_error}[/]")
                    # Continue watching despite error

            except Exception as e:
                self.console.print(f"[red]Watch mode error: {e}[/]")

            await asyncio.sleep(self.watch_interval)

    def handle_slash_command(self, command: str) -> bool:
        """
        Handle slash commands.

        Returns:
            True if should continue REPL, False to exit
        """
        parts = command.split(maxsplit=1)
        cmd = parts[0].lower()
        args = parts[1] if len(parts) > 1 else ""

        if cmd == "/help":
            self.console.print(Panel("""
[bold]Available Commands:[/]

/help              Show this help message
/clear             Clear conversation history
/reset             Clear conversation and reset terminal states
/model [name]      Switch model or list available models
/info              Show session information
/watch <goal>      Enable watch mode with goal
/watch off         Disable watch mode
/watch status      Show watch mode status
/squash            Manually compress conversation context
/quit or /exit     Exit sidechat

[bold]Usage:[/]
- Type messages to chat with AI
- AI provides commands in ```bash blocks
- Commands are sent to Exec terminal with approval
""", title="Sidechat Help", border_style="cyan"))
            return True

        elif cmd == "/clear":
            # Reset conversation (system prompt will be passed on next interaction)
            try:
                self.conversation = llm.Conversation(model=self.model)
                self.console.print("[green]✓[/] Conversation cleared")
            except Exception as e:
                self.console.print(f"[red]Error clearing conversation: {e}[/]")
            return True

        elif cmd == "/reset":
            # Clear conversation and reset terminal states (like tmuxai /reset)
            try:
                # Clear conversation
                self.conversation = llm.Conversation(model=self.model)

                # Reset system prompt to original
                self.system_prompt = self.original_system_prompt

                # Disable watch mode if active
                if self.watch_mode:
                    with self.watch_lock:
                        self.watch_mode = False
                        self.watch_goal = None
                        if self.event_loop:
                            try:
                                self.event_loop.call_soon_threadsafe(self.event_loop.stop)
                            except Exception:
                                pass

                # Clear plugin cache
                if self.plugin:
                    self.plugin_dbus.clear_cache()

                self.console.print("[green]✓[/] Conversation cleared and terminal states reset")
            except Exception as e:
                self.console.print(f"[red]Error resetting: {e}[/]")
            return True

        elif cmd == "/model":
            if not args:
                # List available models
                self.console.print("[bold]Available models:[/]")
                for model in llm.get_models():
                    current = " [green](current)[/]" if model.model_id == self.model_name else ""
                    self.console.print(f"  - {model.model_id}{current}")
            else:
                # Switch model
                try:
                    self.model = llm.get_model(args)
                    self.model_name = args
                    self.console.print(f"[green]✓[/] Switched to model: {args}")

                    # Update conversation model
                    self.conversation.model = self.model
                except Exception as e:
                    self.console.print(f"[red]Error switching model: {e}[/]")
            return True

        elif cmd == "/info":
            tokens = self.estimate_tokens()
            percentage = (tokens * 100 // self.max_context_size) if self.max_context_size > 0 else 0
            self.console.print(Panel(f"""
[bold]Session Information:[/]

Model: {self.model_name}
Context size: ~{tokens:,} tokens / {self.max_context_size:,} ({percentage}%)
Exchanges: {len(self.conversation.responses)}
Watch mode: {"enabled" if self.watch_mode else "disabled"}
{f"Watch goal: {self.watch_goal}" if self.watch_mode else ""}

Chat terminal: {self.chat_terminal_uuid[:16]}...
Exec terminal: {self.exec_terminal_uuid[:16]}...
""", title="Session Info", border_style="cyan"))
            return True

        elif cmd == "/watch":
            if not args or args.lower() == "off":
                # Disable watch mode
                if self.watch_mode:
                    with self.watch_lock:
                        self.watch_mode = False
                        if self.event_loop:
                            # Stop the loop gracefully
                            try:
                                self.event_loop.call_soon_threadsafe(self.event_loop.stop)
                            except Exception:
                                pass
                    self.console.print("[yellow]Watch mode disabled[/]")
                else:
                    self.console.print("[yellow]Watch mode is already off[/]")
            elif args.lower() == "status":
                # Show watch mode status
                if self.watch_mode:
                    self.console.print(f"[green]Watch mode: enabled[/]")
                    self.console.print(f"Goal: {self.watch_goal}")
                    self.console.print(f"Interval: {self.watch_interval}s")
                else:
                    self.console.print("[yellow]Watch mode: disabled[/]")
            else:
                # Enable watch mode with goal
                with self.watch_lock:
                    self.watch_mode = True
                    self.watch_goal = args
                    # Start watch thread immediately
                    self._start_watch_mode_thread()
                self.console.print(f"[green]✓[/] Watch mode enabled")
                self.console.print(f"Goal: {self.watch_goal}")
                self.console.print(f"Monitoring all terminals every {self.watch_interval}s...")
            return True

        elif cmd == "/squash":
            self.squash_context()
            return True

        elif cmd in ["/quit", "/exit"]:
            self._shutdown()  # Explicit cleanup before exit
            return False

        else:
            self.console.print(f"[red]Unknown command: {cmd}[/]")
            self.console.print("Type /help for available commands")
            return True

    def run(self):
        """Main REPL loop with health checks"""
        # Connect to Terminator
        self._connect_to_terminator()

        # Setup terminals
        self.setup_terminals()

        # Display welcome message
        self.console.print(Panel.fit(
            f"""[bold green]Terminator Sidechat Started[/]

Model: [cyan]{self.model_name}[/]
Type your message or /help for commands
""",
            title="llm-sidechat",
            border_style="green"
        ))

        # Main REPL loop with periodic health checks
        check_counter = 0
        try:
            while True:
                # Periodic health check every 10 iterations
                check_counter += 1
                if check_counter >= 10:
                    # Check plugin availability
                    if not self._check_plugin_available():
                        self.console.print("[yellow]Plugin unavailable, attempting reconnect...[/]")
                        if not self._reconnect_plugin():
                            self.console.print("[red]Plugin reconnection failed. Please restart sidechat.[/]")
                            break

                    # Check D-Bus connection
                    if not self._check_dbus_connection():
                        self.console.print("[yellow]D-Bus disconnected, attempting reconnect...[/]")
                        if not self._reconnect_dbus():
                            self.console.print("[red]D-Bus reconnection failed. Please restart sidechat.[/]")
                            break

                    check_counter = 0

                # Get user input
                try:
                    user_input = Prompt.ask("\n[bold cyan]you[/]").strip()
                except (KeyboardInterrupt, EOFError):
                    self.console.print("\n[yellow]Exiting...[/]")
                    break

                if not user_input:
                    continue

                # Handle slash commands
                if user_input.startswith('/'):
                    should_continue = self.handle_slash_command(user_input)
                    if not should_continue:
                        break
                    continue

                # Clear plugin cache to ensure fresh capture
                try:
                    self.plugin_dbus.clear_cache()
                except Exception:
                    pass  # Ignore if clear_cache not available

                # Capture context from terminals
                context = self.capture_context(include_exec_output=False)

                # Build prompt with context
                if context:
                    full_prompt = f"""{user_input}

[Current terminal context]
{context}
"""
                else:
                    full_prompt = user_input

                # Check if we need to squash context
                self.check_and_squash_context()

                # Send to AI with streaming
                # Pass system prompt on first interaction or when conversation is empty
                response_text = ""
                stream_success = False

                try:
                    self.console.print("\n[bold green]ai[/]")

                    # Thread-safe conversation access
                    with self.watch_lock:
                        # Pass system prompt if this is the first interaction
                        if len(self.conversation.responses) == 0:
                            response = self.conversation.prompt(full_prompt, system=self.system_prompt)
                        else:
                            response = self.conversation.prompt(full_prompt)

                        # Stream the response with Rich Live display for proper markdown formatting
                        # Using Live display to update markdown rendering as chunks arrive
                        with Live(Markdown(""), console=self.console, refresh_per_second=10) as live:
                            for chunk in response:
                                response_text += chunk
                                live.update(Markdown(response_text))

                    # No need for extra newline - Rich Live handles formatting
                    stream_success = True

                except Exception as e:
                    print()  # Ensure newline even on error
                    self.console.print(f"\n[red]Streaming error: {e}[/]")
                    self.console.print("[yellow]Response may be incomplete. Please try again.[/]")
                    # Don't process commands or update conversation on stream failure
                    continue

                # Only process commands if streaming succeeded
                if stream_success:
                    try:
                        # Extract and execute commands
                        commands = self.extract_commands(response_text)
                        if commands:
                            self.console.print(f"\n[cyan]Found {len(commands)} command(s)[/]")

                            for i, cmd in enumerate(commands, 1):
                                if len(commands) > 1:
                                    self.console.print(f"\n[bold]Command {i}/{len(commands)}[/]")

                                executed, exec_content = self.execute_command(cmd)

                                if executed:
                                    # Use the output captured during wait_for_command_completion
                                    # No need to capture again - we already have fresh content!

                                    # DEBUG: Show what was captured
                                    self.console.print(f"\n[yellow]═══ DEBUG: Captured Content ═══[/]")
                                    self.console.print(f"[yellow]UUID: {self.exec_terminal_uuid}[/]")
                                    self.console.print(f"[yellow]Length: {len(exec_content)} chars[/]")
                                    self.console.print(f"[yellow]Is ERROR: {exec_content.startswith('ERROR') if exec_content else 'N/A (empty)'}[/]")
                                    self.console.print(f"[yellow]First 300 chars:[/]")
                                    self.console.print(f"[cyan]{exec_content[:300] if exec_content else '(empty)'}[/]")
                                    self.console.print(f"[yellow]═══════════════════════════════[/]\n")

                                    # Build prompt with command execution context
                                    exec_context_prompt = f"""Command executed in Exec terminal:
```bash
{cmd}
```

Output:
```
{exec_content}
```

Please analyze the output and provide feedback if needed."""

                                    # Send the exec output to the conversation so AI can see results
                                    # Thread-safe conversation access
                                    with self.watch_lock:
                                        followup_response = self.conversation.prompt(exec_context_prompt)
                                        # Consume response while holding lock
                                        followup_text = followup_response.text()

                                    # DEBUG: Show AI response handling
                                    self.console.print(f"\n[yellow]═══ DEBUG: AI Response ═══[/]")
                                    self.console.print(f"[yellow]Length: {len(followup_text)} chars[/]")
                                    self.console.print(f"[yellow]Stripped lowercase: '{followup_text.strip().lower()[:100]}'[/]")
                                    self.console.print(f"[yellow]Will be filtered (hidden): {followup_text.strip().lower() in ['ok', 'okay', 'done']}[/]")
                                    self.console.print(f"[yellow]═══════════════════════════[/]\n")

                                    # Display the AI's analysis
                                    if followup_text.strip() and followup_text.strip().lower() not in ["ok", "okay", "done"]:
                                        self.console.print("\n[bold green]ai[/]")
                                        self.console.print(Markdown(followup_text))

                    except Exception as e:
                        self.console.print(f"\n[red]Command execution error: {e}[/]")

        finally:
            # Unified cleanup - handles all resources
            self._shutdown()


def main():
    """Entry point"""
    import argparse

    parser = argparse.ArgumentParser(
        description="Terminator AI Sidechat - Terminal assistant for pair programming"
    )
    parser.add_argument(
        'model',
        nargs='?',
        help='LLM model to use (default: azure/gpt-4.1-mini)'
    )
    args = parser.parse_args()

    session = TerminatorSidechatSession(model_name=args.model)
    session.run()


if __name__ == "__main__":
    main()
