#!/usr/bin/env python3
"""
llm-terminator-sidechat - Terminal AI Assistant for Terminator

A Terminator-integrated AI assistant that provides:
- Automatic Exec terminal creation
- Terminal content capture and analysis
- Command execution with approval
- Watch Mode for proactive monitoring
- Context squashing to manage token limits
- Streaming responses with markdown rendering
- Robust error handling and recovery

Inspired by TmuxAI but designed for Terminator terminal emulator.

Author: c0ffee0wl
License: GPL v2 only
"""

import llm
from llm.cli import load_template
import sys
import os
import re
import time
import asyncio
import threading
import dbus
from pathlib import Path
from rich.console import Console
from rich.markdown import Markdown
from rich.panel import Panel
from rich.prompt import Confirm, Prompt
from rich.text import Text
from typing import List, Optional, Tuple, Dict


class PromptDetector:
    """Detect shell prompts to determine command completion"""

    # Common prompt patterns
    BASH_PROMPT = re.compile(r'[\$#]\s*$')
    ZSH_PROMPT = re.compile(r'[%❯→➜]\s*$')
    # Kali two-line prompt
    KALI_PROMPT = re.compile(r'┌──.*\n└─[\$#]\s*$')
    # Generic fallback
    GENERIC_PROMPT = re.compile(r'^[\w@\-]+[:#\$>]\s*$', re.MULTILINE)

    @staticmethod
    def detect_prompt(text: str) -> bool:
        """Check if text ends with a shell prompt"""
        if not text or len(text.strip()) == 0:
            return False

        # Get last few lines
        lines = text.strip().split('\n')
        last_line = lines[-1] if lines else ""
        last_two = '\n'.join(lines[-2:]) if len(lines) >= 2 else last_line

        # Check patterns
        if PromptDetector.BASH_PROMPT.search(last_line):
            return True
        if PromptDetector.ZSH_PROMPT.search(last_line):
            return True
        if PromptDetector.KALI_PROMPT.search(last_two):
            return True
        if PromptDetector.GENERIC_PROMPT.search(last_line):
            return True

        return False


class TerminatorSidechatSession:
    """Main sidechat session manager for Terminator"""

    def __init__(self, model_name: Optional[str] = None):
        self.console = Console()
        self.model_name = model_name or self._get_default_model()

        try:
            self.model = llm.get_model(self.model_name)
        except Exception as e:
            self.console.print(f"[red]Error loading model '{self.model_name}': {e}[/]")
            self.console.print("[yellow]Available models:[/]")
            for model in llm.get_models():
                self.console.print(f"  - {model.model_id}")
            sys.exit(1)

        self.conversation = llm.Conversation(model=self.model)

        # Store system prompt separately for reuse
        try:
            template = load_template("terminator-sidechat")
            # Use system if available, otherwise use prompt
            self.system_prompt = template.system or template.prompt
            if not self.system_prompt:
                raise ValueError("Template has no system or prompt field")
        except Exception as e:
            self.console.print(f"[yellow]⚠ Could not load terminator-sidechat template: {e}[/]")
            self.console.print("[yellow]Using basic system prompt. Install template with: ./install-llm-tools.sh[/]")
            self.system_prompt = "You are an AI terminal assistant. Provide commands in ```bash blocks."

        # Store original system prompt for context squashing
        # This prevents infinite growth when squashing multiple times
        self.original_system_prompt = self.system_prompt

        # Terminal tracking
        self.chat_terminal_uuid = None
        self.exec_terminal_uuid = None

        # Context management (like tmuxai)
        self.max_context_size = 100000  # tokens
        self.context_squash_threshold = 0.8  # 80%

        # Watch mode (thread-safe)
        self.watch_mode = False
        self.watch_goal = None
        self.watch_thread = None
        self.watch_interval = 5  # seconds
        self.watch_lock = threading.Lock()
        self.event_loop = None

        # Plugin and D-Bus connections
        self.plugin = None
        self.dbus_service = None

    def _get_default_model(self) -> str:
        """Get default model from llm configuration"""
        try:
            return llm.get_default_model()
        except Exception:
            return "azure/gpt-4.1-mini"

    def _reconnect_dbus(self) -> bool:
        """Attempt to reconnect to Terminator D-Bus"""
        try:
            bus = dbus.SessionBus()
            self.dbus_service = bus.get_object(
                'net.tenshu.Terminator2',
                '/net/tenshu/Terminator2'
            )
            return True
        except Exception as e:
            self.console.print(f"[red]D-Bus reconnection failed: {e}[/]")
            return False

    def _check_dbus_connection(self) -> bool:
        """Verify D-Bus is still connected"""
        try:
            # Try a simple D-Bus operation
            self.dbus_service.get_terminals()
            return True
        except Exception:
            return False

    def _connect_to_terminator(self):
        """Connect to Terminator via plugin and D-Bus with retry logic"""
        # Get plugin via PluginRegistry
        try:
            from terminatorlib.plugin import PluginRegistry
            registry = PluginRegistry()
            registry.load_plugins()
            self.plugin = registry.instances.get('TerminatorSidechatPlugin')

            if not self.plugin:
                self.console.print("[red]Error: TerminatorSidechatPlugin not loaded[/]")
                self.console.print("Ensure the plugin is installed in ~/.config/terminator/plugins/")
                self.console.print("and enabled in Terminator Preferences > Plugins")
                sys.exit(1)

        except ImportError:
            self.console.print("[red]Error: Could not import Terminator libraries[/]")
            self.console.print("Ensure this script is run within Terminator terminal")
            sys.exit(1)

        # Connect to D-Bus for terminal management
        if not self._reconnect_dbus():
            self.console.print("[red]Error: Could not connect to Terminator D-Bus service[/]")
            self.console.print("Ensure Terminator is running with D-Bus enabled")
            sys.exit(1)

    def _check_plugin_available(self) -> bool:
        """Verify plugin is still loaded"""
        try:
            from terminatorlib.plugin import PluginRegistry
            registry = PluginRegistry()
            registry.load_plugins()
            return 'TerminatorSidechatPlugin' in registry.instances
        except Exception:
            return False

    def _reconnect_plugin(self) -> bool:
        """Attempt to reload plugin"""
        try:
            from terminatorlib.plugin import PluginRegistry
            registry = PluginRegistry()
            registry.load_plugins()
            self.plugin = registry.instances.get('TerminatorSidechatPlugin')
            return self.plugin is not None
        except Exception as e:
            self.console.print(f"[red]Plugin reconnection failed: {e}[/]")
            return False

    def setup_terminals(self):
        """Auto-create Exec terminal with retry logic"""
        self.console.print("[cyan]Setting up terminals...[/]")

        max_retries = 3
        for attempt in range(max_retries):
            try:
                # Check D-Bus connection
                if not self._check_dbus_connection():
                    if not self._reconnect_dbus():
                        raise Exception("D-Bus reconnection failed")

                # Get current (Chat) terminal UUID
                focused_terminal = self.dbus_service.get_focused_terminal()
                if not focused_terminal:
                    raise Exception("No terminal is currently focused. Please focus a terminal and try again.")
                self.chat_terminal_uuid = str(focused_terminal)

                # Split horizontally to create Exec terminal
                exec_uuid = self.dbus_service.hsplit(
                    self.chat_terminal_uuid,
                    dbus.Dictionary({
                        'title': 'Sidechat: Exec'
                    }, signature='ss')
                )
                self.exec_terminal_uuid = str(exec_uuid)

                self.console.print(f"[green]✓[/] Chat terminal: {self.chat_terminal_uuid[:8]}...")
                self.console.print(f"[green]✓[/] Exec terminal: {self.exec_terminal_uuid[:8]}...")
                return  # Success

            except Exception as e:
                if attempt < max_retries - 1:
                    self.console.print(f"[yellow]Retry {attempt+1}/{max_retries}: {e}[/]")
                    time.sleep(1)
                else:
                    self.console.print(f"[red]Failed to setup terminals after {max_retries} attempts: {e}[/]")
                    sys.exit(1)

    def _verify_exec_terminal(self) -> bool:
        """Check if exec terminal still exists"""
        try:
            terminals = self.plugin.get_all_terminals_metadata()
            return any(t['uuid'] == self.exec_terminal_uuid for t in terminals)
        except Exception:
            return False

    def _recreate_exec_terminal(self) -> bool:
        """Recreate exec terminal if closed"""
        try:
            self.console.print("[yellow]Recreating Exec terminal...[/]")

            # Get current focused terminal
            focused_terminal = self.dbus_service.get_focused_terminal()
            if not focused_terminal:
                raise Exception("No terminal is currently focused. Cannot recreate Exec terminal.")
            current_uuid = str(focused_terminal)

            # Create new exec terminal
            exec_uuid = self.dbus_service.hsplit(
                current_uuid,
                dbus.Dictionary({'title': 'Sidechat: Exec (Restored)'}, signature='ss')
            )

            self.exec_terminal_uuid = str(exec_uuid)
            self.console.print(f"[green]✓[/] Exec terminal restored: {exec_uuid[:8]}...")
            return True

        except Exception as e:
            self.console.print(f"[red]Failed to recreate exec terminal: {e}[/]")
            return False

    def capture_context(self, include_exec_output=False) -> str:
        """
        Capture visible content from all terminals (like tmuxai).

        Excludes:
        - Chat terminal (where sidechat is running) - to avoid self-reference
        - Exec terminal (unless include_exec_output=True)

        Returns:
            Formatted context string with XML-wrapped terminal content
        """
        try:
            terminals = self.plugin.get_all_terminals_metadata()
            context_parts = []

            for term in terminals:
                # Skip chat terminal (self-awareness)
                if term['uuid'] == self.chat_terminal_uuid:
                    continue

                # Optionally skip exec terminal
                if not include_exec_output and term['uuid'] == self.exec_terminal_uuid:
                    continue

                # Capture visible content (auto-detect viewport size)
                content = self.plugin.capture_terminal_content(term['uuid'])

                if content and not content.startswith('ERROR'):
                    # Format like tmux-fragments
                    context_parts.append(f'''<terminal uuid="{term['uuid']}" title="{term['title']}" cwd="{term['cwd']}">
{content}
</terminal>''')

            return "\n\n".join(context_parts)
        except Exception as e:
            self.console.print(f"[red]Error capturing context: {e}[/]")
            return ""

    def estimate_tokens(self) -> int:
        """Estimate token count from conversation responses and system prompt"""
        total_chars = 0
        try:
            # Count system prompt
            total_chars += len(self.system_prompt)

            # Count all responses (includes prompts and AI responses)
            for response in self.conversation.responses:
                # Count the prompt
                if hasattr(response, 'prompt'):
                    prompt_obj = response.prompt
                    if hasattr(prompt_obj, 'prompt'):
                        total_chars += len(str(prompt_obj.prompt))
                    if hasattr(prompt_obj, 'system') and prompt_obj.system:
                        total_chars += len(str(prompt_obj.system))

                # Count the response text
                if hasattr(response, '_chunks'):
                    total_chars += sum(len(chunk) for chunk in response._chunks)
        except Exception as e:
            self.console.print(f"[yellow]Warning: Token estimation failed: {e}[/]")
        return total_chars // 4

    def check_and_squash_context(self):
        """Auto-squash when context reaches threshold (like tmuxai)"""
        current_tokens = self.estimate_tokens()

        if current_tokens >= self.max_context_size * self.context_squash_threshold:
            self.console.print("[yellow]Context approaching limit, auto-squashing...[/]")
            self.squash_context()

    def squash_context(self):
        """Compress earlier messages into summary (like tmuxai)"""
        if len(self.conversation.responses) <= 5:  # Keep at least 5 recent exchanges
            self.console.print("[yellow]Too few messages to squash[/]")
            return

        try:
            # Get responses to squash (all but last 3 - we'll re-execute those)
            responses_to_squash = self.conversation.responses[:-3]

            # Build summary from old responses
            summary_parts = []
            for i, response in enumerate(responses_to_squash, 1):
                # Extract prompt text
                prompt_text = ""
                if hasattr(response, 'prompt') and hasattr(response.prompt, '_prompt'):
                    prompt_text = response.prompt._prompt or ""

                # Extract response text
                response_text = ""
                if hasattr(response, '_chunks'):
                    response_text = "".join(response._chunks)

                if prompt_text:
                    summary_parts.append(f"{i}. User: {prompt_text[:200]}...")
                if response_text:
                    summary_parts.append(f"{i}. AI: {response_text[:200]}...")

            # Generate summary using a standalone prompt (not in conversation)
            summary_prompt = f"""Summarize this conversation history concisely, focusing on:
- Key technical issues discussed
- Commands executed and their results
- Important decisions or insights
- Relevant context for continuing the conversation

Previous messages:
{chr(10).join(summary_parts)}

Provide a brief but comprehensive summary."""

            summary_response = self.model.prompt(summary_prompt)
            summary = summary_response.text()

            # Create new conversation and update system prompt
            # Build enhanced system prompt from ORIGINAL, not current
            # This prevents infinite growth when squashing multiple times
            self.system_prompt = f"""{self.original_system_prompt}

[Previous conversation summary]
{summary}
[End of summary]"""

            # Create completely fresh conversation
            # We'll pass the new system prompt on the next user interaction
            self.conversation = llm.Conversation(model=self.model)

            new_tokens = self.estimate_tokens()
            self.console.print(f"[green]✓[/] Context squashed: ~{new_tokens} tokens")
            self.console.print(f"[cyan]Started fresh conversation with summary in system prompt[/]")

        except Exception as e:
            self.console.print(f"[red]Error squashing context: {e}[/]")

    def extract_commands(self, text: str) -> List[str]:
        """Extract bash commands from ```bash code blocks"""
        # Flexible pattern: allows whitespace after ```bash and optional newline before closing ```
        pattern = r'```bash\s*\n(.*?)\n?```'
        matches = re.findall(pattern, text, re.DOTALL)
        return [cmd.strip() for cmd in matches if cmd.strip()]

    def wait_for_command_completion(self, timeout=30) -> Tuple[bool, str]:
        """
        Wait for command to complete by detecting prompt return.

        Returns:
            (completed, output): Boolean + captured output
        """
        start_time = time.time()
        last_output = ""
        stable_count = 0
        poll_interval = 0.2  # 200ms

        while time.time() - start_time < timeout:
            # Capture current exec terminal content
            try:
                current_output = self.plugin.capture_terminal_content(
                    self.exec_terminal_uuid
                )

                if current_output.startswith('ERROR'):
                    return False, current_output

                # Check for prompt
                if PromptDetector.detect_prompt(current_output):
                    return True, current_output

                # Check for stable output (no changes)
                if current_output == last_output:
                    stable_count += 1
                    if stable_count >= 5:  # 1 second of no changes
                        return True, current_output
                else:
                    stable_count = 0
                    last_output = current_output

                time.sleep(poll_interval)
            except Exception as e:
                self.console.print(f"[yellow]Warning during wait: {e}[/]")
                time.sleep(poll_interval)

        # Timeout reached
        return False, last_output

    def execute_command(self, command: str) -> bool:
        """
        Execute command in Exec terminal with user approval and intelligent completion detection.

        Returns:
            True if executed, False if skipped
        """
        self.console.print(Panel(
            Text(command, style="bold cyan"),
            title="[bold]Command to Execute[/]",
            border_style="cyan"
        ))

        # Ask for approval
        choice = Prompt.ask(
            "Execute in Exec terminal?",
            choices=["y", "n", "e"],  # yes, no, edit
            default="y"
        )

        if choice == "n":
            return False

        if choice == "e":
            # Allow editing
            edited = Prompt.ask("Edit command", default=command)
            command = edited

        # Verify exec terminal exists
        if not self._verify_exec_terminal():
            self.console.print("[yellow]Exec terminal not found[/]")
            if not self._recreate_exec_terminal():
                return False

        # Send to Exec terminal
        try:
            success = self.plugin.send_keys_to_terminal(
                self.exec_terminal_uuid,
                command,
                execute=True
            )

            if success:
                self.console.print("[green]✓[/] Command sent to Exec terminal")

                # Intelligent wait with progress indicator
                with self.console.status("[cyan]Waiting for command to complete..."):
                    completed, output = self.wait_for_command_completion(timeout=30)

                if completed:
                    self.console.print("[green]✓[/] Command completed")
                else:
                    self.console.print("[yellow]⚠[/] Command still running or timed out")

                return True
            else:
                self.console.print("[red]✗[/] Failed to send command")
                return False
        except Exception as e:
            self.console.print(f"[red]Error executing command: {e}[/]")
            return False

    def _start_watch_mode_thread(self):
        """Start watch mode in a background thread with its own event loop"""
        def watch_thread_target():
            self.event_loop = asyncio.new_event_loop()
            asyncio.set_event_loop(self.event_loop)
            try:
                self.event_loop.run_until_complete(self.watch_loop())
            except Exception as e:
                self.console.print(f"[red]Watch mode error: {e}[/]")
            finally:
                self.event_loop.close()

        self.watch_thread = threading.Thread(target=watch_thread_target, daemon=True)
        self.watch_thread.start()

    async def watch_loop(self):
        """Background monitoring of all terminals (like tmuxai watch mode)"""
        while self.watch_mode:
            try:
                # Capture all terminal content (including exec output for watch)
                context = self.capture_context(include_exec_output=True)

                if not context.strip():
                    await asyncio.sleep(self.watch_interval)
                    continue

                # Send to AI with watch goal
                prompt = f"""[Watch Mode] Goal: {self.watch_goal}

Current terminal state:
{context}

Based on the watch goal, analyze the terminal activity and provide suggestions ONLY if you observe something relevant to the goal. If everything looks normal, respond with "OK" only."""

                # Use conversation.prompt() for watch mode with thread safety
                # Note: We don't pass system prompt here as it should already be in conversation
                try:
                    # Keep lock held for entire response lifecycle to ensure thread safety
                    with self.watch_lock:
                        response = self.conversation.prompt(prompt)
                        # Consume response while holding lock to prevent concurrent modifications
                        response_text = response.text()

                    # Only show if AI has actionable feedback (not just "OK")
                    if response_text.strip() and response_text.strip().lower() != "ok":
                        self.console.print()
                        self.console.print(Panel(
                            Markdown(response_text),
                            title="[bold yellow]⚠ Watch Mode Alert[/]",
                            border_style="yellow"
                        ))
                        self.console.print()
                except Exception as response_error:
                    self.console.print(f"[yellow]Watch mode response error: {response_error}[/]")
                    # Continue watching despite error

            except Exception as e:
                self.console.print(f"[red]Watch mode error: {e}[/]")

            await asyncio.sleep(self.watch_interval)

    def handle_slash_command(self, command: str) -> bool:
        """
        Handle slash commands.

        Returns:
            True if should continue REPL, False to exit
        """
        parts = command.split(maxsplit=1)
        cmd = parts[0].lower()
        args = parts[1] if len(parts) > 1 else ""

        if cmd == "/help":
            self.console.print(Panel("""
[bold]Available Commands:[/]

/help              Show this help message
/clear             Clear conversation history
/reset             Clear conversation and reset terminal states
/model [name]      Switch model or list available models
/info              Show session information
/watch <goal>      Enable watch mode with goal
/watch off         Disable watch mode
/watch status      Show watch mode status
/squash            Manually compress conversation context
/quit or /exit     Exit sidechat

[bold]Usage:[/]
- Type messages to chat with AI
- AI provides commands in ```bash blocks
- Commands are sent to Exec terminal with approval
""", title="Sidechat Help", border_style="cyan"))
            return True

        elif cmd == "/clear":
            # Reset conversation (system prompt will be passed on next interaction)
            try:
                self.conversation = llm.Conversation(model=self.model)
                self.console.print("[green]✓[/] Conversation cleared")
            except Exception as e:
                self.console.print(f"[red]Error clearing conversation: {e}[/]")
            return True

        elif cmd == "/reset":
            # Clear conversation and reset terminal states (like tmuxai /reset)
            try:
                # Clear conversation
                self.conversation = llm.Conversation(model=self.model)

                # Reset system prompt to original
                self.system_prompt = self.original_system_prompt

                # Disable watch mode if active
                if self.watch_mode:
                    with self.watch_lock:
                        self.watch_mode = False
                        self.watch_goal = None
                        if self.event_loop:
                            try:
                                self.event_loop.call_soon_threadsafe(self.event_loop.stop)
                            except Exception:
                                pass

                # Clear plugin cache
                if self.plugin:
                    self.plugin.clear_cache()

                self.console.print("[green]✓[/] Conversation cleared and terminal states reset")
            except Exception as e:
                self.console.print(f"[red]Error resetting: {e}[/]")
            return True

        elif cmd == "/model":
            if not args:
                # List available models
                self.console.print("[bold]Available models:[/]")
                for model in llm.get_models():
                    current = " [green](current)[/]" if model.model_id == self.model_name else ""
                    self.console.print(f"  - {model.model_id}{current}")
            else:
                # Switch model
                try:
                    self.model = llm.get_model(args)
                    self.model_name = args
                    self.console.print(f"[green]✓[/] Switched to model: {args}")

                    # Update conversation model
                    self.conversation.model = self.model
                except Exception as e:
                    self.console.print(f"[red]Error switching model: {e}[/]")
            return True

        elif cmd == "/info":
            tokens = self.estimate_tokens()
            percentage = (tokens * 100 // self.max_context_size) if self.max_context_size > 0 else 0
            self.console.print(Panel(f"""
[bold]Session Information:[/]

Model: {self.model_name}
Context size: ~{tokens:,} tokens / {self.max_context_size:,} ({percentage}%)
Exchanges: {len(self.conversation.responses)}
Watch mode: {"enabled" if self.watch_mode else "disabled"}
{f"Watch goal: {self.watch_goal}" if self.watch_mode else ""}

Chat terminal: {self.chat_terminal_uuid[:16]}...
Exec terminal: {self.exec_terminal_uuid[:16]}...
""", title="Session Info", border_style="cyan"))
            return True

        elif cmd == "/watch":
            if not args or args.lower() == "off":
                # Disable watch mode
                if self.watch_mode:
                    with self.watch_lock:
                        self.watch_mode = False
                        if self.event_loop:
                            # Stop the loop gracefully
                            try:
                                self.event_loop.call_soon_threadsafe(self.event_loop.stop)
                            except Exception:
                                pass
                    self.console.print("[yellow]Watch mode disabled[/]")
                else:
                    self.console.print("[yellow]Watch mode is already off[/]")
            elif args.lower() == "status":
                # Show watch mode status
                if self.watch_mode:
                    self.console.print(f"[green]Watch mode: enabled[/]")
                    self.console.print(f"Goal: {self.watch_goal}")
                    self.console.print(f"Interval: {self.watch_interval}s")
                else:
                    self.console.print("[yellow]Watch mode: disabled[/]")
            else:
                # Enable watch mode with goal
                with self.watch_lock:
                    self.watch_mode = True
                    self.watch_goal = args
                    # Start watch thread immediately
                    self._start_watch_mode_thread()
                self.console.print(f"[green]✓[/] Watch mode enabled")
                self.console.print(f"Goal: {self.watch_goal}")
                self.console.print(f"Monitoring all terminals every {self.watch_interval}s...")
            return True

        elif cmd == "/squash":
            self.squash_context()
            return True

        elif cmd in ["/quit", "/exit"]:
            return False

        else:
            self.console.print(f"[red]Unknown command: {cmd}[/]")
            self.console.print("Type /help for available commands")
            return True

    def run(self):
        """Main REPL loop with health checks"""
        # Connect to Terminator
        self._connect_to_terminator()

        # Setup terminals
        self.setup_terminals()

        # Display welcome message
        self.console.print(Panel.fit(
            f"""[bold green]Terminator Sidechat Started[/]

Model: [cyan]{self.model_name}[/]
Type your message or /help for commands
""",
            title="llm-terminator-sidechat",
            border_style="green"
        ))

        # Main REPL loop with periodic health checks
        check_counter = 0
        try:
            while True:
                # Periodic health check every 10 iterations
                check_counter += 1
                if check_counter >= 10:
                    # Check plugin availability
                    if not self._check_plugin_available():
                        self.console.print("[yellow]Plugin unavailable, attempting reconnect...[/]")
                        if not self._reconnect_plugin():
                            self.console.print("[red]Plugin reconnection failed. Please restart sidechat.[/]")
                            break

                    # Check D-Bus connection
                    if not self._check_dbus_connection():
                        self.console.print("[yellow]D-Bus disconnected, attempting reconnect...[/]")
                        if not self._reconnect_dbus():
                            self.console.print("[red]D-Bus reconnection failed. Please restart sidechat.[/]")
                            break

                    check_counter = 0

                # Get user input
                try:
                    user_input = Prompt.ask("\n[bold cyan]you[/]").strip()
                except (KeyboardInterrupt, EOFError):
                    self.console.print("\n[yellow]Exiting...[/]")
                    break

                if not user_input:
                    continue

                # Handle slash commands
                if user_input.startswith('/'):
                    should_continue = self.handle_slash_command(user_input)
                    if not should_continue:
                        break
                    continue

                # Capture context from terminals
                context = self.capture_context(include_exec_output=False)

                # Build prompt with context
                if context:
                    full_prompt = f"""{user_input}

[Current terminal context]
{context}
"""
                else:
                    full_prompt = user_input

                # Check if we need to squash context
                self.check_and_squash_context()

                # Send to AI with streaming
                # Pass system prompt on first interaction or when conversation is empty
                response_text = ""
                stream_success = False

                try:
                    self.console.print("\n[bold green]ai[/]")

                    # Thread-safe conversation access
                    with self.watch_lock:
                        # Pass system prompt if this is the first interaction
                        if len(self.conversation.responses) == 0:
                            response = self.conversation.prompt(full_prompt, system=self.system_prompt)
                        else:
                            response = self.conversation.prompt(full_prompt)

                        # Stream the response
                        # Note: Using print() instead of self.console.print() for streaming
                        # because Rich Console is not designed for character-by-character output
                        # Response objects are iterable via __iter__, no .stream() method
                        for chunk in response:
                            print(chunk, end='', flush=True)
                            response_text += chunk

                    print()  # Newline after streaming
                    stream_success = True

                except Exception as e:
                    print()  # Ensure newline even on error
                    self.console.print(f"\n[red]Streaming error: {e}[/]")
                    self.console.print("[yellow]Response may be incomplete. Please try again.[/]")
                    # Don't process commands or update conversation on stream failure
                    continue

                # Only process commands if streaming succeeded
                if stream_success:
                    try:
                        # Extract and execute commands
                        commands = self.extract_commands(response_text)
                        if commands:
                            self.console.print(f"\n[cyan]Found {len(commands)} command(s)[/]")

                            for i, cmd in enumerate(commands, 1):
                                if len(commands) > 1:
                                    self.console.print(f"\n[bold]Command {i}/{len(commands)}[/]")

                                executed = self.execute_command(cmd)

                                if executed:
                                    # Capture exec output for next iteration
                                    exec_content = self.plugin.capture_terminal_content(
                                        self.exec_terminal_uuid
                                    )

                                    # Build prompt with command execution context
                                    exec_context_prompt = f"""Command executed in Exec terminal:
```bash
{cmd}
```

Output:
```
{exec_content}
```

Please analyze the output and provide feedback if needed."""

                                    # Send the exec output to the conversation so AI can see results
                                    # Thread-safe conversation access
                                    with self.watch_lock:
                                        followup_response = self.conversation.prompt(exec_context_prompt)
                                        # Consume response while holding lock
                                        followup_text = followup_response.text()

                                    # Display the AI's analysis
                                    if followup_text.strip() and followup_text.strip().lower() not in ["ok", "okay", "done"]:
                                        self.console.print("\n[bold green]ai[/]")
                                        self.console.print(Markdown(followup_text))

                    except Exception as e:
                        self.console.print(f"\n[red]Command execution error: {e}[/]")

        finally:
            # Cleanup watch mode
            if self.watch_mode:
                with self.watch_lock:
                    self.watch_mode = False
                    if self.event_loop:
                        try:
                            self.event_loop.call_soon_threadsafe(self.event_loop.stop)
                        except Exception:
                            pass


def main():
    """Entry point"""
    import argparse

    parser = argparse.ArgumentParser(
        description="Terminator AI Sidechat - Terminal assistant for pair programming"
    )
    parser.add_argument(
        'model',
        nargs='?',
        help='LLM model to use (default: azure/gpt-4.1-mini)'
    )
    args = parser.parse_args()

    session = TerminatorSidechatSession(model_name=args.model)
    session.run()


if __name__ == "__main__":
    main()
