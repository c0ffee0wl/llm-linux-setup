#!/bin/sh
''''exec "$HOME/.local/share/uv/tools/llm/bin/python3" "$0" "$@" # '''
"""
llm-assistant - Terminal AI Assistant for Terminator

A Terminator-integrated AI assistant that provides:
- Automatic Exec terminal creation
- Terminal content capture and analysis
- Command execution with approval
- Watch Mode for proactive monitoring
- Context squashing to manage token limits
- Streaming responses with markdown rendering
- Robust error handling and recovery

Inspired by TmuxAI but designed for Terminator terminal emulator.

Author: c0ffee0wl
License: GPL v2 only
"""

import sys
import base64
import json
import uuid

import llm
from llm import Tool, ToolResult, ToolOutput, Attachment
from llm.cli import load_template, process_fragments_in_chat, logs_db_path
import sqlite_utils

# Add system site-packages to path for dbus and other system-only packages
# (Must be AFTER llm imports to avoid typing_extensions conflicts)
sys.path.insert(0, '/usr/lib/python3/dist-packages')
# Add user site-packages for llm_tools module (uv's isolated env doesn't include it)
import site
sys.path.insert(0, site.getusersitepackages())
import os
import re
import readline  # Required for \001/\002 prompt markers to work with input()
import time
import hashlib
import tempfile
import asyncio
import threading
import dbus
import fcntl
import signal
import atexit
from pathlib import Path
from collections import deque
from rich.console import Console
from rich.live import Live
from rich.markdown import Markdown
from rich.panel import Panel
from rich.prompt import Confirm as _RichConfirm, InvalidResponse, Prompt
from rich.status import Status
from rich.text import Text
from typing import List, Optional, Tuple, Dict, Union


# Voice input (optional - graceful degradation if not installed)
try:
    import sounddevice as sd
    import numpy as np
    VOICE_AVAILABLE = True
except ImportError:
    VOICE_AVAILABLE = False

# TTS output (optional - requires google-cloud-texttospeech and Vertex credentials)
try:
    from google.cloud import texttospeech
    from google.oauth2 import service_account
    from google.auth import default as google_auth_default
    from google.api_core.client_options import ClientOptions
    import queue
    TTS_AVAILABLE = True
except ImportError:
    TTS_AVAILABLE = False

# Markdown stripping for TTS (removes formatting before speech synthesis)
# Regex to remove fenced code blocks (```...```) only, preserves inline code
_CODE_BLOCK_RE = re.compile(r'```[\s\S]*?```', re.MULTILINE)
# Pattern to match complete code blocks (for preserving code during markdown stripping)
_CODE_BLOCK_PATTERN = re.compile(r'```\w*\n(.*?)```', re.DOTALL)

try:
    from strip_markdown import strip_markdown as _strip_markdown
    def strip_markdown_for_tts(text: str) -> str:
        """Strip markdown formatting for TTS. Removes code blocks entirely."""
        text = _CODE_BLOCK_RE.sub('', text)
        return _strip_markdown(text)
except ImportError:
    def strip_markdown_for_tts(text: str) -> str:
        """Fallback: just remove code blocks."""
        return _CODE_BLOCK_RE.sub('', text)

# Clipboard support for /copy command
try:
    import pyperclip
    CLIPBOARD_AVAILABLE = True
except ImportError:
    CLIPBOARD_AVAILABLE = False

# Web companion (optional - graceful degradation if not installed)
try:
    from fastapi import FastAPI, WebSocket
    from fastapi.responses import HTMLResponse
    import uvicorn
    import webbrowser
    WEB_AVAILABLE = True
except ImportError:
    WEB_AVAILABLE = False

def strip_markdown_for_clipboard(text: str) -> str:
    """Strip markdown but fully preserve code block content."""
    # Extract code blocks and replace with null-byte placeholders
    code_blocks = []
    def save_code(match):
        code_blocks.append(match.group(1))
        return f'\x00CODE{len(code_blocks)-1}\x00'

    text_with_placeholders = _CODE_BLOCK_PATTERN.sub(save_code, text)

    # Strip markdown from non-code parts only
    try:
        stripped = _strip_markdown(text_with_placeholders)
    except NameError:
        stripped = text_with_placeholders

    # Restore code blocks (content only, fences removed)
    for i, code in enumerate(code_blocks):
        stripped = stripped.replace(f'\x00CODE{i}\x00', code.rstrip('\n'))

    return stripped

# prompt_toolkit for keybindings (Ctrl+Space for voice) and completion
from prompt_toolkit import PromptSession
from prompt_toolkit.key_binding import KeyBindings
from prompt_toolkit.styles import Style as PTStyle
from prompt_toolkit.completion import Completer, Completion


# Slash command definitions for tab completion
# Structure: command -> {subcommands: [...], dynamic: str|None, description: str}
SLASH_COMMANDS = {
    "/help": {"subcommands": [], "dynamic": None, "description": "Show help"},
    "/clear": {"subcommands": [], "dynamic": None, "description": "Clear conversation"},
    "/reset": {"subcommands": [], "dynamic": None, "description": "Full reset"},
    "/refresh": {"subcommands": [], "dynamic": None, "description": "Refresh context"},
    "/info": {"subcommands": [], "dynamic": None, "description": "Session info"},
    "/quit": {"subcommands": [], "dynamic": None, "description": "Exit"},
    "/exit": {"subcommands": [], "dynamic": None, "description": "Exit"},
    "/model": {"subcommands": ["-q"], "dynamic": "models", "description": "Switch or list models"},
    "/watch": {"subcommands": ["off", "status"], "dynamic": None, "description": "Watch mode control"},
    "/squash": {"subcommands": [], "dynamic": None, "description": "Compress context"},
    "/kb": {"subcommands": ["load", "unload", "reload"], "dynamic": "kb", "description": "Knowledge base control"},
    "/auto": {"subcommands": ["full", "off", "status"], "dynamic": None, "description": "Auto mode control"},
    "/voice": {"subcommands": ["auto", "off", "status"], "dynamic": None, "description": "Voice input control"},
    "/speech": {"subcommands": ["on", "off", "status"], "dynamic": None, "description": "TTS output control"},
    "/rag": {"subcommands": ["add", "search", "rebuild", "delete", "off", "status", "top-k", "mode"], "dynamic": "rag_collections", "description": "RAG search integration"},
    "/assistant": {"subcommands": [], "dynamic": None, "description": "Switch to assistant mode (conservative)"},
    "/agent": {"subcommands": [], "dynamic": None, "description": "Switch to agent mode (agentic)"},
    "/copy": {"subcommands": ["raw", "all"], "dynamic": None, "description": "Copy response(s) to clipboard"},
    "/web": {"subcommands": ["stop"], "dynamic": None, "description": "Open web companion"},
}


class SlashCommandCompleter(Completer):
    """Tab completion for slash commands in llm-assistant.

    Supports:
    - Command names (when typing /)
    - Subcommands (off, status, load, etc.)
    - Dynamic completions (model names, KB names)
    """

    def __init__(self, session: 'TerminatorAssistantSession' = None):
        """Initialize completer with optional session reference.

        Args:
            session: TerminatorAssistantSession instance for dynamic completions.
                     Can be set later via set_session().
        """
        self.session = session

    def set_session(self, session: 'TerminatorAssistantSession'):
        """Set session reference (for deferred initialization)."""
        self.session = session

    def get_completions(self, document, complete_event):
        """Yield completions based on current input."""
        text = document.text_before_cursor

        # Only complete if starts with /
        if not text.startswith('/'):
            return

        parts = text.split()

        # parts will always have at least 1 element since text starts with '/'
        # (split() on "/" returns ['/'], on "/mo" returns ['/mo'], etc.)

        if len(parts) == 1 and not text.endswith(' '):
            # Completing command name: "/" -> all commands, "/mo" -> "/model"
            yield from self._complete_commands(parts[0])

        elif len(parts) >= 1:
            # Completing after command: "/model " or "/kb load "
            cmd = parts[0].lower()

            if cmd not in SLASH_COMMANDS:
                return

            cmd_info = SLASH_COMMANDS[cmd]

            if len(parts) == 1 and text.endswith(' '):
                # Just command + space: "/model " - show subcommands or dynamic
                yield from self._complete_first_arg(cmd, cmd_info, "")

            elif len(parts) == 2 and not text.endswith(' '):
                # Partial first arg: "/model az" or "/kb lo"
                partial = parts[1]
                yield from self._complete_first_arg(cmd, cmd_info, partial)

            elif len(parts) == 2 and text.endswith(' '):
                # First arg complete, need second: "/kb load "
                subcommand = parts[1].lower()
                yield from self._complete_second_arg(cmd, subcommand, "")

            elif len(parts) == 3 and not text.endswith(' '):
                # Partial second arg: "/kb load pro"
                subcommand = parts[1].lower()
                partial = parts[2]
                yield from self._complete_second_arg(cmd, subcommand, partial)

    def _complete_commands(self, partial: str):
        """Complete command names."""
        partial_lower = partial.lower()
        for cmd, info in SLASH_COMMANDS.items():
            if cmd.lower().startswith(partial_lower):
                yield Completion(
                    cmd,
                    start_position=-len(partial),
                    display_meta=info.get("description", "")
                )

    def _complete_first_arg(self, cmd: str, cmd_info: dict, partial: str):
        """Complete first argument (subcommand or dynamic value)."""
        partial_lower = partial.lower()

        # Subcommands first
        for sub in cmd_info.get("subcommands", []):
            if sub.lower().startswith(partial_lower):
                yield Completion(
                    sub,
                    start_position=-len(partial)
                )

        # Dynamic completions
        dynamic_type = cmd_info.get("dynamic")
        if dynamic_type == "models":
            yield from self._complete_models(partial)
        elif dynamic_type == "kb" and cmd == "/kb":
            # For /kb without subcommand, don't show KB names yet
            pass
        elif dynamic_type == "rag_collections" and cmd == "/rag":
            # For /rag without subcommand, show collection names for activation
            yield from self._complete_rag_collections(partial)

    def _complete_second_arg(self, cmd: str, subcommand: str, partial: str):
        """Complete second argument (KB names after load/unload, RAG collections)."""
        if cmd == "/kb":
            if subcommand == "load":
                yield from self._complete_available_kbs(partial)
            elif subcommand == "unload":
                yield from self._complete_loaded_kbs(partial)
        elif cmd == "/rag":
            if subcommand in ("add", "search", "rebuild", "delete"):
                yield from self._complete_rag_collections(partial)

    def _complete_models(self, partial: str):
        """Complete model names dynamically."""
        partial_lower = partial.lower()
        try:
            for model in llm.get_models():
                model_id = model.model_id
                if model_id.lower().startswith(partial_lower):
                    yield Completion(
                        model_id,
                        start_position=-len(partial)
                    )
        except Exception:
            pass  # Graceful degradation if llm unavailable

    def _complete_available_kbs(self, partial: str):
        """Complete available KB names for /kb load."""
        if not self.session:
            return
        partial_lower = partial.lower()
        try:
            kb_dir = self.session._get_kb_dir()
            for kb_file in kb_dir.glob("*.md"):
                name = kb_file.stem
                # Only show KBs not already loaded
                if name not in self.session.loaded_kbs:
                    if name.lower().startswith(partial_lower):
                        yield Completion(
                            name,
                            start_position=-len(partial)
                        )
        except Exception:
            pass

    def _complete_loaded_kbs(self, partial: str):
        """Complete loaded KB names for /kb unload."""
        if not self.session:
            return
        partial_lower = partial.lower()
        try:
            for name in self.session.loaded_kbs.keys():
                if name.lower().startswith(partial_lower):
                    yield Completion(
                        name,
                        start_position=-len(partial)
                    )
        except Exception:
            pass

    def _complete_rag_collections(self, partial: str):
        """Complete RAG collection names."""
        partial_lower = partial.lower()
        try:
            from llm_tools_rag import get_collection_list
            for coll in get_collection_list():
                name = coll['name']
                if name.lower().startswith(partial_lower):
                    yield Completion(
                        name,
                        start_position=-len(partial)
                    )
        except ImportError:
            pass  # llm-tools-rag not installed
        except Exception:
            pass  # Graceful degradation


class VoiceInput:
    """Speech-to-text input using onnx-asr and sounddevice."""

    def __init__(self, console):
        self.console = console
        self.recording = False
        self.audio_chunks = []
        self.sample_rate = 16000
        self.model = None
        self.stream = None
        self.preserved_text = ""  # Text to preserve across recording
        self.status_message = ""  # For dynamic prompt display
        self._app = None  # prompt_toolkit app for invalidate()

    def _lazy_load_model(self) -> bool:
        """Load ASR model on first use (avoids startup delay)."""
        if self.model is None:
            try:
                import onnx_asr
            except ImportError:
                self.console.print("[red]onnx-asr not installed. Re-run install-llm-tools.sh[/]")
                return False
            try:
                # Suppress HuggingFace download progress bars (model cached after first download)
                os.environ["HF_HUB_DISABLE_PROGRESS_BARS"] = "1"
                self.model = onnx_asr.load_model("nemo-parakeet-tdt-0.6b-v3")
            except Exception as e:
                self.console.print(f"[red]Failed to load speech model: {e}[/]")
                return False
        return True

    def _audio_callback(self, indata, frames, time_info, status):
        """Callback for sounddevice stream."""
        if self.recording:
            self.audio_chunks.append(indata.copy())

    def start(self) -> bool:
        """Start recording audio."""
        if not VOICE_AVAILABLE:
            self.console.print("[red]Voice input unavailable. Re-run install-llm-tools.sh[/]")
            return False

        if self.recording:
            return False

        self.audio_chunks = []
        self.recording = True

        try:
            self.stream = sd.InputStream(
                samplerate=self.sample_rate,
                channels=1,
                dtype=np.float32,
                callback=self._audio_callback
            )
            self.stream.start()
            # Status shown via dynamic prompt (⏺ symbol)
            self.status_message = "⏺ Recording..."
            return True
        except Exception as e:
            self.console.print(f"[red]Failed to start recording: {e}[/]")
            self.recording = False
            return False

    def stop(self) -> Optional[str]:
        """Stop recording and transcribe."""
        if not self.recording:
            return None

        self.recording = False

        if self.stream:
            self.stream.stop()
            self.stream.close()
            self.stream = None

        if not self.audio_chunks:
            self.status_message = ""
            return None

        # Combine audio chunks
        audio = np.concatenate(self.audio_chunks, axis=0).flatten()

        # Show transcribing status and force immediate render
        self.status_message = "⟳ Transcribing..."
        if self._app:
            self._app.renderer.render(self._app, self._app.layout)

        text = None
        if not self._lazy_load_model():
            self.status_message = ""
            pass  # Will return None below
        else:
            try:
                result = self.model.recognize(audio, sample_rate=self.sample_rate)
                text = result.strip() if result else None
            except Exception as e:
                self.status_message = ""
                self.console.print(f"[red]Transcription failed: {e}[/]")
                return None

        self.status_message = ""
        return text

    def toggle(self) -> Tuple[bool, Optional[str]]:
        """Toggle recording state. Returns (is_recording, transcribed_text)."""
        if self.recording:
            text = self.stop()
            return (False, text)
        else:
            started = self.start()
            return (started, None)


class SentenceBuffer:
    """Buffer LLM tokens and yield complete sentences for TTS.

    Accumulates streaming tokens and returns complete sentences when
    sentence-ending punctuation is detected (.!?).
    """

    SENTENCE_END = re.compile(r'[.!?]\s*$')

    def __init__(self):
        self.buffer = ""

    def add(self, token: str) -> Optional[str]:
        """Add token to buffer. Returns sentence if one is complete."""
        self.buffer += token

        # Check for sentence boundary
        if self.SENTENCE_END.search(self.buffer):
            sentence = self.buffer.strip()
            self.buffer = ""
            return sentence
        return None

    def flush(self) -> Optional[str]:
        """Flush remaining buffer content (end of response)."""
        if self.buffer.strip():
            sentence = self.buffer.strip()
            self.buffer = ""
            return sentence
        return None


def get_tts_credentials():
    """
    Get Google Cloud credentials for TTS.

    Resolution order:
    1. GOOGLE_APPLICATION_CREDENTIALS env var
    2. vertex-credentials-path from llm config (set via 'llm vertex set-credentials')
    3. Application Default Credentials (ADC)

    Returns tuple (credentials, method_name) where method_name indicates the source.
    Returns (None, None) if credentials cannot be obtained.
    """
    if not TTS_AVAILABLE:
        return None, None

    # 1. Check GOOGLE_APPLICATION_CREDENTIALS env var
    creds_path = os.environ.get("GOOGLE_APPLICATION_CREDENTIALS")
    if creds_path and os.path.exists(creds_path):
        try:
            credentials = service_account.Credentials.from_service_account_file(
                creds_path,
                scopes=["https://www.googleapis.com/auth/cloud-platform"]
            )
            return credentials, "service account (env)"
        except Exception:
            pass  # Fall through to next method

    # 2. Check vertex-credentials-path from llm config
    try:
        config_path = llm.get_key("", "vertex-credentials-path", "")
        if config_path and os.path.exists(config_path):
            credentials = service_account.Credentials.from_service_account_file(
                config_path,
                scopes=["https://www.googleapis.com/auth/cloud-platform"]
            )
            return credentials, "service account (llm config)"
    except Exception:
        pass  # Fall through to ADC

    # 3. Fall back to Application Default Credentials
    try:
        credentials, _ = google_auth_default(
            scopes=["https://www.googleapis.com/auth/cloud-platform"]
        )
        return credentials, "ADC"
    except Exception:
        return None, None


class SpeechOutput:
    """Text-to-speech output using Google Cloud TTS (Chirp3-HD).

    Synthesizes sentences as they complete during LLM streaming.
    Both synthesis and playback happen in background threads to avoid
    blocking the main LLM streaming loop.
    """

    def __init__(self, console):
        self.console = console
        self.client = None
        self.voice_name = "de-DE-Chirp3-HD-Laomedeia"  # German HD voice
        self.language_code = "de-DE"
        self.enabled = False
        self.audio_queue = None  # For synthesized audio
        self.worker_thread = None
        self._executor = None  # ThreadPoolExecutor for async synthesis
        self.cred_method = None  # Credential method used (for status display)

    def _lazy_load_client(self) -> bool:
        """Initialize TTS client on first use."""
        if self.client is not None:
            return True

        if not TTS_AVAILABLE:
            self.console.print("[red]google-cloud-texttospeech not installed. Re-run install-llm-tools.sh[/]")
            return False

        try:
            from concurrent.futures import ThreadPoolExecutor
            credentials, self.cred_method = get_tts_credentials()
            # Use EU endpoint for data residency compliance
            client_options = ClientOptions(api_endpoint="eu-texttospeech.googleapis.com")
            self.client = texttospeech.TextToSpeechClient(
                credentials=credentials,
                client_options=client_options
            )
            # If credentials was None but client succeeded, it used ADC internally
            if self.cred_method is None:
                self.cred_method = "ADC (implicit)"
            self.audio_queue = queue.Queue()
            self._executor = ThreadPoolExecutor(max_workers=1)  # Must be 1 to preserve sentence order
            return True
        except Exception as e:
            self.console.print(f"[red]Failed to initialize TTS client: {e}[/]")
            self.console.print("[dim]Configure via: llm vertex set-credentials /path/to/sa.json[/]")
            self.console.print("[dim]Or run: gcloud auth application-default login[/]")
            return False

    def _synthesize_and_queue(self, sentence: str):
        """Synthesize speech using streaming API and queue chunks for immediate playback."""
        try:
            # Strip markdown formatting (removes code blocks entirely)
            clean_text = strip_markdown_for_tts(sentence).strip()
            if not clean_text:
                return  # Skip empty text after stripping

            # Streaming synthesis config
            streaming_config = texttospeech.StreamingSynthesizeConfig(
                voice=texttospeech.VoiceSelectionParams(
                    name=self.voice_name,
                    language_code=self.language_code,
                )
            )

            # Request generator for streaming API
            def request_generator():
                yield texttospeech.StreamingSynthesizeRequest(
                    streaming_config=streaming_config
                )
                yield texttospeech.StreamingSynthesizeRequest(
                    input=texttospeech.StreamingSynthesisInput(text=clean_text)
                )

            # Queue each audio chunk immediately as it arrives (true streaming)
            for response in self.client.streaming_synthesize(request_generator()):
                if response.audio_content:
                    self.audio_queue.put(response.audio_content)
        except Exception:
            pass  # Silently skip on error

    def _playback_loop(self):
        """Background thread for sequential audio playback."""
        while True:
            audio_data = self.audio_queue.get()
            if audio_data is None:  # Sentinel to stop
                break
            try:
                audio_array = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0
                sd.play(audio_array, samplerate=24000)
                sd.wait()
            except Exception:
                pass

    def speak_sentence(self, sentence: str):
        """Queue sentence for async synthesis and playback (non-blocking)."""
        if not self._lazy_load_client():
            return

        # Start playback thread if needed
        if self.worker_thread is None or not self.worker_thread.is_alive():
            self.worker_thread = threading.Thread(target=self._playback_loop, daemon=True)
            self.worker_thread.start()

        # Submit synthesis to thread pool (non-blocking)
        self._executor.submit(self._synthesize_and_queue, sentence)

    def stop(self):
        """Stop playback and clear queue."""
        if self.audio_queue:
            # Clear queue
            while not self.audio_queue.empty():
                try:
                    self.audio_queue.get_nowait()
                except:
                    break
            # Stop current playback
            try:
                sd.stop()
            except:
                pass


class Confirm(_RichConfirm):
    """Extended Confirm that accepts 'yes'/'no' in addition to 'y'/'n'."""

    def process_response(self, value: str) -> bool:
        value = value.strip().lower()
        if value in ("y", "yes"):
            return True
        elif value in ("n", "no"):
            return False
        else:
            raise InvalidResponse(self.validate_error_message)


# Import shared prompt detection module
try:
    from llm_tools.prompt_detection import PromptDetector
except ImportError:
    # Fallback for development - import from context directory
    sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'context'))
    from prompt_detection import PromptDetector

# Import system info detection (local copy)
sys.path.insert(0, os.path.dirname(__file__))
from system_info import detect_os, detect_shell, detect_environment


# Common TUI applications that use screenshot capture instead of text
# All lowercase for case-insensitive matching
TUI_COMMANDS = {
    # System monitors
    'htop', 'top', 'btop', 'gtop', 'glances', 'atop', 'nmon',
    'iotop', 'iftop', 'nethogs', 'bmon', 'vnstat', 'procs',
    # Editors
    'vim', 'vi', 'nvim', 'nano', 'emacs', 'helix', 'micro',
    'joe', 'pico', 'jed', 'ne', 'mg', 'kakoune', 'kak',
    # Pagers
    'less', 'more', 'most', 'bat',
    # File managers
    'mc', 'ranger', 'nnn', 'lf', 'vifm', 'fff', 'broot',
    'ncdu', 'duf', 'dust',
    # Terminal multiplexers
    'tmux', 'screen', 'byobu', 'zellij',
    # Git TUIs
    'tig', 'lazygit', 'gitui',
    # Container/K8s TUIs
    'k9s', 'lazydocker', 'dive', 'ctop',
    # Fuzzy finders (when run standalone)
    'fzf', 'sk', 'peco',
    # Periodic execution
    'watch',
    # Audio
    'alsamixer', 'pulsemixer',
    # Email/IRC
    'mutt', 'neomutt', 'aerc',
    'weechat', 'irssi',
    # Music players
    'cmus', 'ncmpcpp', 'moc', 'mocp',
    # Web browsers
    'lynx', 'w3m', 'links', 'elinks',
    # Task management
    'taskwarrior-tui', 'taskell',
    # Calendar
    'calcurse', 'khal',
}


# =============================================================================
# Spinner Animation (using Rich's Status for console compatibility)
# =============================================================================

# Module-level console for spinner (avoids passing console instance everywhere)
_spinner_console = Console()

class Spinner:
    """Animated spinner for long-running operations (context manager or manual control).

    Uses Rich's Status component for proper integration with Rich console output.
    This prevents issues where raw stdout writes interfere with Rich's rendering.
    """

    CHARS = ["⠋", "⠙", "⠹", "⠸", "⠼", "⠴", "⠦", "⠧", "⠇", "⠏"]

    def __init__(self, message: str = "", console: Console = None):
        self.message = message
        self._console = console or _spinner_console
        self._status = None
        self._idx = 0

    def __enter__(self):
        self.start()
        return self

    def __exit__(self, *args):
        self.stop()

    def start(self):
        """Start the spinner animation using Rich Status."""
        self._status = Status(f"[cyan]{self.message}[/]", console=self._console, spinner="dots", spinner_style="cyan")
        self._status.start()

    def stop(self):
        """Stop the spinner."""
        if self._status:
            self._status.stop()
            self._status = None

    def clear(self):
        """Clear the spinner (stop is sufficient with Rich Status)."""
        self.stop()

    def update(self, message: str):
        """Update the spinner message."""
        self.message = message
        if self._status:
            self._status.update(f"[cyan]{message}[/]")

    def tick(self, message: str = None):
        """Manual tick for polling loops - update spinner once without threading.

        For Rich-based spinner, this just updates the message since Rich handles animation.
        """
        if message is not None:
            self.message = message
        if self._status:
            self._status.update(f"[cyan]{self.message}[/]")
        else:
            # Fallback for when used outside context manager
            spinner = self.CHARS[self._idx % len(self.CHARS)]
            self._console.print(f"[cyan]{spinner} {self.message}[/]", end="\r")
            self._idx += 1


# =============================================================================
# Tool Setup
# =============================================================================
# External tool plugins that assistant can dispatch (easy to extend)
# Just add the plugin name here to enable a new external tool
EXTERNAL_TOOL_PLUGINS = ('search_google', 'fetch_url', 'fragment_bridge', 'fabric', 'llm-tools-mcp', 'capture_screen', 'imagemage', 'sandboxed_python')

_all_tools = llm.get_tools()

# Pre-load MCP tools from llm-tools-mcp plugin (dynamic toolbox)
# This makes MCP tools visible to the AI model at startup
try:
    from llm_tools_mcp.register_tools import MCP
    _mcp_toolbox = MCP()  # Instantiate to load tools from configured MCP servers
    for tool in _mcp_toolbox.tools():  # tools() is a method, not property
        _all_tools[tool.name] = tool
except ImportError:
    pass  # llm-tools-mcp not installed
except Exception:
    pass  # MCP config missing or server unavailable

# Build ASSISTANT_TOOLS from plugins - includes core assistant tools + external tools
ASSISTANT_TOOLS = [
    tool for tool in _all_tools.values()
    if isinstance(tool, Tool) and getattr(tool, 'plugin', None) in ('assistant',) + EXTERNAL_TOOL_PLUGINS
]

# Build dict of external tool implementations for auto-dispatch
# These are tools with real implementations (not assistant stubs)
EXTERNAL_TOOLS = {
    name: tool.implementation
    for name, tool in _all_tools.items()
    if isinstance(tool, Tool)
    and hasattr(tool, 'implementation') and tool.implementation is not None
    and getattr(tool, 'plugin', None) in EXTERNAL_TOOL_PLUGINS
}

# Display configuration for external tools: tool_name -> (param_name, action_verb, brief_description)
# Tools not in this dict will use generic "Calling {tool_name}..." message
EXTERNAL_TOOL_DISPLAY = {
    'search_google': ('query', 'Searching', 'Search the web'),
    'fetch_url': ('url', 'Fetching', 'Fetch web page content'),
    # Fragment bridge tools
    'load_yt': ('argument', 'Loading transcript', 'Load YouTube transcript'),
    'load_github': ('argument', 'Loading repo', 'Load GitHub repository'),
    'load_pdf': ('argument', 'Extracting PDF', 'Extract PDF content'),
    # Fabric pattern tool
    'prompt_fabric': ('task', 'Processing with Fabric', 'Execute Fabric AI pattern'),
    # Microsoft Learn MCP tools
    'microsoft_docs_search': ('query', 'Searching Microsoft Learn', 'Search Microsoft documentation'),
    'microsoft_docs_fetch': ('url', 'Fetching Microsoft documentation', 'Fetch Microsoft documentation page'),
    'microsoft_code_sample_search': ('query', 'Searching Microsoft code samples', 'Search Microsoft code examples'),
    # Screen capture tool
    'capture_screen': ('mode', 'Capturing screenshot', 'Capture screen or window'),
    # Image generation tool (imagemage)
    'generate_image': ('prompt', 'Generating image', 'Generate or edit images with Gemini'),
    # Blueprint MCP browser automation tools (30 tools)
    # Connection Management
    'enable': ('', 'Enabling browser automation', 'Activate browser automation'),
    'disable': ('', 'Disabling browser automation', 'Deactivate browser automation'),
    'status': ('', 'Checking connection status', 'Check browser connection status'),
    'auth': ('', 'Authenticating to PRO', 'Login to Blueprint PRO account'),
    # Tab Management
    'browser_tabs': ('action', 'Managing tabs', 'List/create/attach/close browser tabs'),
    # Navigation
    'browser_navigate': ('url', 'Navigating to', 'Navigate to URL in browser'),
    'browser_navigate_back': ('', 'Going back', 'Go back in browser history'),
    # Content & Inspection
    'browser_snapshot': ('', 'Getting page snapshot', 'Get accessible page content'),
    'browser_take_screenshot': ('', 'Taking screenshot', 'Capture browser screenshot'),
    'browser_console_messages': ('', 'Getting console logs', 'Get browser console messages'),
    'browser_network_requests': ('urlPattern', 'Monitoring network', 'Monitor/filter network requests'),
    'browser_extract_content': ('', 'Extracting content', 'Extract page content as markdown'),
    # Interaction
    'browser_interact': ('actions', 'Interacting with page', 'Perform multiple actions in sequence'),
    'browser_click': ('selector', 'Clicking', 'Click on element'),
    'browser_type': ('text', 'Typing', 'Type text into input'),
    'browser_hover': ('selector', 'Hovering over', 'Hover over element'),
    'browser_select_option': ('value', 'Selecting option', 'Select dropdown option'),
    'browser_fill_form': ('fields', 'Filling form', 'Fill multiple form fields'),
    'browser_press_key': ('key', 'Pressing key', 'Press keyboard key'),
    'browser_drag': ('source', 'Dragging element', 'Drag and drop element'),
    # Advanced
    'browser_evaluate': ('code', 'Executing JavaScript', 'Run JavaScript in page context'),
    'browser_handle_dialog': ('action', 'Handling dialog', 'Handle alert/confirm/prompt'),
    'browser_file_upload': ('path', 'Uploading file', 'Upload file through input'),
    'browser_window': ('action', 'Managing window', 'Resize/minimize/maximize window'),
    'browser_pdf_save': ('', 'Saving as PDF', 'Save page as PDF'),
    'browser_performance_metrics': ('', 'Getting metrics', 'Get performance metrics'),
    'browser_verify_text_visible': ('text', 'Verifying text', 'Verify text is visible'),
    'browser_verify_element_visible': ('selector', 'Verifying element', 'Verify element is visible'),
    # Extension Management
    'browser_list_extensions': ('', 'Listing extensions', 'List installed browser extensions'),
    'browser_reload_extensions': ('', 'Reloading extensions', 'Reload unpacked extensions'),
    # Sandboxed Python execution
    'execute_python': ('code', 'Running Python', 'Execute Python code in sandbox'),
}


def is_tui_command(command: str) -> bool:
    """
    Detect if a command will launch a TUI application.

    Handles piped commands by checking the rightmost command,
    since that's what actually displays in the terminal.

    Args:
        command: Full command string (e.g., "htop -d 5" or "git log | less")

    Returns:
        True if command is a known TUI application
    """
    if not command.strip():
        return False

    # For piped commands, check the rightmost command (that's what displays)
    # e.g., "cat file | less" -> check "less"
    # e.g., "git log | head" -> check "head" (not TUI)
    if '|' in command:
        parts = command.split('|')
        command = parts[-1].strip()

    # Extract the base command (first word)
    base_cmd = command.split()[0] if command.split() else ""

    # Remove path if present (e.g., /usr/bin/htop -> htop)
    # Use lowercase for case-insensitive matching
    base_cmd = os.path.basename(base_cmd).lower()

    return base_cmd in TUI_COMMANDS


class TerminatorAssistantSession:
    """Main assistant session manager for Terminator"""

    def __init__(self, model_name: Optional[str] = None, debug: bool = False, max_context_size: int = 800000,
                 continue_: bool = False, conversation_id: Optional[str] = None, no_log: bool = False,
                 agent_mode: bool = False):
        self.console = Console()

        # Debug mode flag
        self.debug = debug

        # Conversation persistence settings
        self.logging_enabled = not no_log
        self.continue_ = continue_
        self.conversation_id = conversation_id

        # Initialize shutdown state and lock file handle
        self._shutdown_initiated = False
        self._last_interrupt_time = 0.0  # For double-press exit protection
        self.lock_file = None

        # Early D-Bus detection - require Terminator
        self.early_terminal_uuid = self._get_current_terminal_uuid_early()

        if not self.early_terminal_uuid:
            self.console.print("[red]Error: llm-assistant requires Terminator terminal[/]")
            self.console.print("[yellow]Please run this from inside a Terminator terminal.[/]")
            sys.exit(1)

        # Fast check: Is there already an assistant in this tab?
        if self._check_existing_assistant_in_tab(self.early_terminal_uuid):
            self.console.print("[red]Error: An assistant is already running in this tab[/]")
            self.console.print("[yellow]You can run assistant in a different Terminator tab.[/]")
            sys.exit(1)

        # Acquire global lock (brief, only during initialization)
        self._acquire_instance_lock()

        # Double-check under lock (catches race condition)
        if self._check_existing_assistant_in_tab(self.early_terminal_uuid):
            self._release_instance_lock()
            self.console.print("[red]Error: An assistant is already running in this tab[/]")
            self.console.print("[yellow]You can run assistant in a different Terminator tab.[/]")
            sys.exit(1)

        # Register shutdown handlers EARLY (before creating resources)
        self._register_shutdown_handlers()

        self.model_name = model_name or self._get_default_model()

        try:
            self.model = llm.get_model(self.model_name)
        except Exception as e:
            self.console.print(f"[red]Error loading model '{self.model_name}': {e}[/]")
            self.console.print("[yellow]Available models:[/]")
            for model in llm.get_models():
                self.console.print(f"  - {model.model_id}")
            sys.exit(1)

        self._init_conversation()

        # Store system prompt separately for reuse
        try:
            template = load_template("terminator-assistant")
            # Use system if available, otherwise use prompt
            self.system_prompt = template.system or template.prompt
            if not self.system_prompt:
                raise ValueError("Template has no system or prompt field")
            self._debug(f"Template loaded: terminator-assistant ({len(self.system_prompt)} chars)")
        except Exception as e:
            self.console.print(f"[yellow]⚠ Could not load terminator-assistant template: {e}[/]")
            self.console.print("[yellow]Using basic system prompt. Install template with: ./install-llm-tools.sh[/]")
            self._debug(f"Template load failed: {e}")
            self.system_prompt = "You are an AI terminal assistant. Provide commands in <EXECUTE>command</EXECUTE> XML tags."

        # Store original system prompt for context squashing
        # This prevents infinite growth when squashing multiple times
        self.original_system_prompt = self.system_prompt

        # Terminal tracking
        self.chat_terminal_uuid = None
        self.exec_terminal_uuid = None

        # Context management (like tmuxai)
        self.max_context_size = max_context_size  # from CLI --max-context
        self.context_squash_threshold = 0.8  # 80%

        # Capture size limits (prevent memory exhaustion)
        self.MAX_CAPTURE_ROWS = 10000      # Maximum rows to capture
        self.MAX_CAPTURE_BYTES = 10_000_000  # 10MB byte limit
        self.MAX_CAPTURE_CHARS = 2_000_000   # 2M character limit (~500k tokens)

        # Tool token overhead (estimated at startup, cached for session)
        self._tool_token_overhead = self._estimate_tool_schema_tokens()

        # Watch mode (thread-safe)
        self.watch_mode = False
        self.watch_goal = None
        self.watch_thread = None
        self.watch_task = None  # Asyncio task for graceful cancellation
        self.watch_interval = 5  # seconds
        self.watch_lock = threading.Lock()
        self.event_loop = None
        # Watch mode intelligent change detection
        self.previous_watch_context_hash = None  # SHA256 hash for deduplication
        self.previous_watch_iteration_count = 0   # Track iterations for prompt

        # Web companion (real-time browser view)
        self.web_server = None          # uvicorn server instance
        self.web_server_thread = None   # Thread running the server
        self.web_clients: set = set()   # Connected WebSocket clients
        self.web_event_loop = None      # Event loop for web server thread
        self.web_port = 8765            # Default port for web companion

        # Per-terminal content hash tracking for deduplication
        # Enables [Content unchanged] placeholder when terminal content hasn't changed
        self.terminal_content_hashes: Dict[str, str] = {}  # uuid -> content hash

        # D-Bus connections
        self.dbus_service = None

        # Track screenshot files for cleanup (use dedicated temp directory)
        self.screenshot_dir = self._setup_screenshot_dir()
        self.screenshot_files = []

        # Pending attachments for multi-modal viewing (deferred to next turn)
        self.pending_attachments = []

        # Pending summary from context squash (prepended to next user message)
        self.pending_summary: Optional[str] = None

        # Knowledge Base system (TmuxAI-style)
        self.loaded_kbs: Dict[str, str] = {}  # name -> content
        self._load_auto_kbs()  # Load from config on startup

        # RAG system (llm-tools-rag integration)
        self.active_rag_collection: Optional[str] = None  # Active collection (persistent mode)
        self.rag_top_k: int = 5                           # Number of results to retrieve
        self.rag_search_mode: str = "hybrid"              # hybrid|vector|keyword
        self.pending_rag_context: Optional[str] = None    # One-shot search result

        # Auto mode: LLM-judged autonomous command execution
        # False = off, "normal" = safe only, "full" = safe + caution
        self.auto_mode: Union[bool, str] = False

        # Operating mode: "assistant" (conservative) or "agent" (agentic)
        # Controls tool iteration limits and system prompt content
        self.mode: str = "agent" if agent_mode else "assistant"

        # Voice auto-submit: automatically send transcribed text
        self.voice_auto_submit: bool = False
        self.auto_command_history: deque = deque(maxlen=3)  # recent commands for judge context
        self.DANGEROUS_PATTERNS = [
            r"rm\s+-rf\s+/(?:\s|$)",      # rm -rf /
            r"rm\s+-rf\s+/\*",             # rm -rf /*
            r"rm\s+-rf\s+~",               # rm -rf ~
            r"dd\s+if=.*of=/dev/",         # dd to device
            r"mkfs\.",                     # format filesystem
            r">\s*/dev/sd",                # redirect to disk
            r":\(\)\{\s*:\|:&\s*\};:",     # fork bomb
            r"chmod\s+-R\s+777\s+/(?:\s|$)",  # chmod 777 /
            r"chown\s+-R.*:\s*/(?:\s|$)",  # chown / recursively
            r"curl.*\|\s*(ba)?sh",         # curl pipe to shell
            r"wget.*\|\s*(ba)?sh",         # wget pipe to shell
        ]

        # Voice input (STT) - lazy-loaded
        self.voice_input = VoiceInput(self.console) if VOICE_AVAILABLE else None

        # Speech output (TTS) - lazy-loaded, only for Vertex models
        self.speech_output = SpeechOutput(self.console) if TTS_AVAILABLE else None

        # prompt_toolkit session with Ctrl+Space voice toggle
        self.prompt_session = self._create_prompt_session()

        # Set session reference for slash command completer (dynamic completions need access to self)
        if hasattr(self.prompt_session, 'completer') and self.prompt_session.completer:
            self.prompt_session.completer.set_session(self)

    def _create_prompt_session(self) -> PromptSession:
        """Create prompt_toolkit session with voice toggle keybinding and slash command completion."""
        kb = KeyBindings()

        # Store reference to self for closure
        session = self

        @kb.add('c-space')
        def _(event):
            """Toggle voice recording on Ctrl+Space."""
            if not session.voice_input:
                session.console.print("[dim]Voice input not available[/]")
                return

            buffer = event.app.current_buffer

            # If not currently recording, save text before starting
            if not session.voice_input.recording:
                session.voice_input.preserved_text = buffer.text

            # Give VoiceInput access to app for invalidate() during transcription
            session.voice_input._app = event.app

            is_recording, text = session.voice_input.toggle()

            if is_recording:
                # Recording just started - invalidate to show ⏺ prompt
                event.app.invalidate()
            else:
                # Recording stopped - restore display
                existing_text = session.voice_input.preserved_text

                if text:
                    # Construct new text from preserved + transcribed
                    if existing_text:
                        if existing_text.endswith(' '):
                            new_text = existing_text + text
                        else:
                            new_text = existing_text + ' ' + text
                    else:
                        new_text = text
                    buffer.text = new_text
                    buffer.cursor_position = len(new_text)
                    # Auto-submit if enabled
                    if session.voice_auto_submit:
                        buffer.validate_and_handle()
                elif existing_text:
                    # Recording stopped but no transcription - restore existing text
                    buffer.text = existing_text
                    buffer.cursor_position = len(existing_text)

                # Invalidate to show > prompt and updated text
                event.app.invalidate()

        @kb.add('escape')
        def _(event):
            """Stop TTS playback on Escape."""
            if session.speech_output and session.speech_output.enabled:
                session.speech_output.stop()

        # Style for the prompt
        style = PTStyle.from_dict({
            'prompt': 'ansicyan bold',
            'prompt.recording': 'ansired',
            'prompt.transcribing': '#ff8800',  # orange
            'continuation': 'ansigray',
        })

        # Create completer for slash commands (session reference set after __init__)
        completer = SlashCommandCompleter()

        return PromptSession(
            key_bindings=kb,
            style=style,
            completer=completer,
            complete_while_typing=False,  # Only complete on Tab
        )

    def _acquire_instance_lock(self):
        """
        Acquire brief global lock during initialization.
        Used with double-check pattern to prevent race conditions
        when multiple tabs start simultaneously.

        The lock is released after exec terminal creation (not end of session).

        Security improvements:
        - Uses per-user subdirectory to prevent cross-user interference
        - Sets lock file permissions to 0600
        - Handles corrupted lock files gracefully
        """
        # Use per-user lock directory for security
        runtime_dir = os.environ.get('XDG_RUNTIME_DIR')
        if not runtime_dir or not Path(runtime_dir).is_dir():
            # Fallback to user-specific temp directory
            runtime_dir = Path(tempfile.gettempdir()) / f'llm-assistant-{os.getuid()}'
            runtime_dir.mkdir(mode=0o700, parents=True, exist_ok=True)
            runtime_dir = str(runtime_dir)

        lock_path = Path(runtime_dir) / 'llm-assistant.lock'

        # Check for stale lock before attempting to acquire
        if lock_path.exists():
            try:
                with open(lock_path, 'r') as f:
                    old_pid_str = f.read().strip()
                    if old_pid_str:
                        try:
                            old_pid = int(old_pid_str)
                            os.kill(old_pid, 0)  # Check if process exists
                            # Also check for zombie state (os.kill succeeds for zombies)
                            try:
                                with open(f'/proc/{old_pid}/status', 'r') as status_f:
                                    if 'State:\tZ' in status_f.read():
                                        raise OSError("Zombie process - treating as stale")
                            except FileNotFoundError:
                                raise OSError("Process no longer exists")
                        except (ValueError, OSError):
                            # Process doesn't exist, is zombie, or PID invalid - stale/corrupted lock
                            try:
                                lock_path.unlink()
                            except OSError:
                                pass  # Another process may have removed it
            except IOError:
                # Can't read lock file - try to remove it
                try:
                    lock_path.unlink()
                except OSError:
                    pass

        try:
            # Open with secure permissions (0600) using fdopen pattern
            fd = os.open(str(lock_path), os.O_WRONLY | os.O_CREAT, 0o600)
            self.lock_file = os.fdopen(fd, 'w')
            fcntl.flock(self.lock_file.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)
            self.lock_file.write(f"{os.getpid()}\n")
            self.lock_file.flush()

        except BlockingIOError:
            # Another instance is initializing - wait briefly and retry
            # This allows multi-tab support while preventing race conditions
            self.console.print("[yellow]Another assistant is starting, waiting...[/]")
            try:
                fd = os.open(str(lock_path), os.O_WRONLY | os.O_CREAT, 0o600)
                self.lock_file = os.fdopen(fd, 'w')
                fcntl.flock(self.lock_file.fileno(), fcntl.LOCK_EX)  # Blocking wait
                self.lock_file.write(f"{os.getpid()}\n")
                self.lock_file.flush()
            except Exception as e:
                self.console.print(f"[red]Error acquiring lock: {e}[/]")
                sys.exit(1)

        except Exception as e:
            self.console.print(f"[red]Error acquiring lock: {e}[/]")
            if self.lock_file:
                try:
                    self.lock_file.close()
                except Exception:
                    pass
            sys.exit(1)

    def _release_instance_lock(self):
        """Release the instance lock (automatic on process exit, but explicit is better)"""
        if self.lock_file:
            try:
                fcntl.flock(self.lock_file.fileno(), fcntl.LOCK_UN)
                self.lock_file.close()
                self.lock_file = None
            except Exception:
                pass  # Lock will be released by kernel anyway

    def _setup_screenshot_dir(self) -> Path:
        """
        Set up dedicated directory for screenshot temp files.
        Also cleans up any orphaned files from crashed sessions.

        Returns:
            Path to screenshot directory
        """
        # Use user-specific temp directory for security
        base_dir = Path(tempfile.gettempdir()) / f'llm-assistant-{os.getuid()}'
        screenshot_dir = base_dir / 'screenshots'

        try:
            screenshot_dir.mkdir(mode=0o700, parents=True, exist_ok=True)

            # Clean up orphaned screenshot files from crashed sessions
            # Files older than 1 hour are considered orphaned
            cleanup_threshold = time.time() - 3600  # 1 hour ago
            for old_file in screenshot_dir.glob('assistant_screenshot_*.png'):
                try:
                    if old_file.stat().st_mtime < cleanup_threshold:
                        old_file.unlink()
                except OSError:
                    pass  # File may have been deleted by another process
        except OSError as e:
            # Fallback to system temp if we can't create our directory
            self._debug(f"Could not create screenshot dir: {e}")
            screenshot_dir = Path(tempfile.gettempdir())

        return screenshot_dir

    def _get_current_terminal_uuid_early(self) -> Optional[str]:
        """
        Get current terminal UUID from environment BEFORE acquiring lock.
        This enables tab-specific locking for multi-tab support.

        Uses TERMINATOR_UUID environment variable which is stable and set
        at terminal creation time (doesn't change when focus changes).

        Returns:
            Terminal UUID string, or None if not in Terminator
        """
        # Use TERMINATOR_UUID env var - stable, doesn't change with focus
        env_uuid = os.environ.get('TERMINATOR_UUID')
        if env_uuid:
            return self._normalize_uuid(env_uuid)
        return None  # Not running in Terminator

    def _check_existing_assistant_in_tab(self, terminal_uuid: str) -> bool:
        """
        Check if another assistant instance is already running in the same tab.

        Looks for terminals with title starting with "Assistant: Exec" in the
        same tab as the given terminal.

        Args:
            terminal_uuid: UUID of the current terminal

        Returns:
            True if another assistant is running in this tab, False otherwise
        """
        try:
            bus = dbus.SessionBus()
            plugin_dbus = bus.get_object(
                'net.tenshu.Terminator2.Assistant',
                '/net/tenshu/Terminator2/Assistant'
            )

            # Get terminals in same tab
            terminals = plugin_dbus.get_terminals_in_same_tab(terminal_uuid)

            # Check for existing Assistant Exec terminal with active process
            for t in terminals:
                title = t.get('title', '')
                if title.startswith('Assistant: Exec'):
                    # Extract PID from title and verify process is running
                    pid = self._extract_pid_from_title(title)
                    if pid and self._process_exists(pid):
                        # Found active assistant in this tab
                        return True
                    # Stale title (no PID or process dead) - continue checking

            return False

        except Exception:
            # Plugin not available or error - allow startup
            # (will fail later with proper error message)
            return False

    def _extract_pid_from_title(self, title: str) -> int:
        """
        Extract PID from 'Assistant: Exec (PID 12345)' or 'Assistant: Exec (Restored PID 12345)' format.

        Args:
            title: Terminal title string

        Returns:
            PID as integer, or None if not found
        """
        import re
        match = re.search(r'\((?:Restored )?PID (\d+)\)', title)
        return int(match.group(1)) if match else None

    def _process_exists(self, pid: int) -> bool:
        """
        Check if a process with the given PID exists.

        Args:
            pid: Process ID to check

        Returns:
            True if process exists, False otherwise
        """
        try:
            os.kill(pid, 0)  # Signal 0 = check existence without killing
            return True
        except (OSError, ProcessLookupError):
            return False

    def _get_model_capabilities(self):
        """
        Query current model's multi-modal capabilities.

        Returns a dict with capability flags based on model.attachment_types.
        """
        types = getattr(self.conversation.model, 'attachment_types', set())
        return {
            'vision': any(t.startswith("image/") for t in types),
            'pdf': "application/pdf" in types,
            'audio': any(t.startswith("audio/") for t in types),
            'video': any(t.startswith("video/") for t in types),
            'youtube': "video/youtube" in types,  # Gemini only
            'supported_types': types,
        }

    def _create_attachment(self, path_or_url: str) -> Attachment:
        """
        Create an Attachment from a path or URL.

        Automatically detects whether the input is a URL (http/https) or local path
        and sets the appropriate Attachment field.

        Args:
            path_or_url: Local file path or URL

        Returns:
            Attachment object with path or url set appropriately
        """
        is_url = path_or_url.startswith('http')
        return Attachment(
            path=None if is_url else path_or_url,
            url=path_or_url if is_url else None
        )

    def _register_shutdown_handlers(self):
        """Register signal handlers and atexit hook for cleanup"""
        # Register atexit fallback (runs on normal exit)
        atexit.register(self._shutdown)

        # Register signal handlers for graceful shutdown
        # Note: SIGINT (Ctrl+C) uses Python's default KeyboardInterrupt - handled in input loop
        signal.signal(signal.SIGTERM, self._signal_handler)  # kill command
        signal.signal(signal.SIGHUP, self._signal_handler)   # Terminal closed

    def _signal_handler(self, signum, frame):
        """Handle termination signals (SIGTERM, SIGHUP)."""
        signal_names = {
            signal.SIGTERM: "SIGTERM",
            signal.SIGHUP: "SIGHUP (terminal closed)"
        }
        signal_name = signal_names.get(signum, f"signal {signum}")

        self.console.print(f"\n[yellow]Received {signal_name}, shutting down...[/]")
        self._shutdown()
        sys.exit(0)

    def _shutdown(self):
        """
        Unified shutdown method - called by signal handlers, atexit, or manual exit.
        Idempotent - safe to call multiple times.
        """
        # Prevent double-cleanup
        if self._shutdown_initiated:
            return
        self._shutdown_initiated = True

        try:
            # STEP 1: Stop watch mode (most critical - prevents new threads)
            if hasattr(self, 'watch_mode') and self.watch_mode:
                try:
                    with self.watch_lock:
                        self.watch_mode = False
                        if self.watch_task and not self.watch_task.done():
                            try:
                                self.event_loop.call_soon_threadsafe(self.watch_task.cancel)
                            except RuntimeError:
                                pass  # Loop already closed

                    # Wait for watch thread to finish (with timeout)
                    if self.watch_thread and self.watch_thread.is_alive():
                        self.watch_thread.join(timeout=2.0)
                except Exception:
                    # Don't let watch mode cleanup prevent other cleanup
                    pass

            # STEP 2: Clean up screenshot files
            if hasattr(self, 'screenshot_files') and self.screenshot_files:
                for screenshot_path in self.screenshot_files:
                    try:
                        if os.path.exists(screenshot_path):
                            os.unlink(screenshot_path)
                    except (IOError, OSError):
                        pass  # Ignore cleanup errors
                self.screenshot_files.clear()

            # STEP 3: Clear plugin cache (if available)
            if hasattr(self, 'plugin_dbus'):
                try:
                    self.plugin_dbus.clear_cache()
                except Exception:
                    pass

            # STEP 3.5: Stop web companion server (if running)
            if hasattr(self, 'web_server') and self.web_server:
                try:
                    self.web_server.should_exit = True
                    self.web_server = None
                    self.web_server_thread = None
                    self.web_event_loop = None
                    self.web_clients.clear()
                except Exception:
                    pass

            # STEP 4: Close D-Bus connections (graceful disconnect)
            # D-Bus connections don't need explicit cleanup in Python - garbage collected
            # But we can dereference them to signal intent
            if hasattr(self, 'dbus_service'):
                self.dbus_service = None
            if hasattr(self, 'plugin_dbus'):
                self.plugin_dbus = None

            # STEP 5: Release instance lock (most important)
            self._release_instance_lock()

            # STEP 6: Save conversation state (if needed)
            # llm library auto-saves, but we can explicitly flush here
            if hasattr(self, 'conversation'):
                try:
                    # Force any pending writes to complete
                    # llm Conversation doesn't have explicit save(), it auto-saves
                    pass
                except Exception:
                    pass

        except Exception as e:
            # Ensure we log errors but don't prevent shutdown
            try:
                self.console.print(f"[yellow]Warning during shutdown: {e}[/]")
            except Exception:
                # If console fails, write to stderr
                print(f"Warning during shutdown: {e}", file=sys.stderr)

    def _get_default_model(self) -> str:
        """Get default model from llm configuration"""
        try:
            return llm.get_default_model()
        except Exception:
            return "azure/gpt-4.1-mini"

    def _is_vertex_model(self) -> bool:
        """Check if current model is a Vertex AI model (vertex/*)"""
        return self.model_name.startswith("vertex/")

    def _debug(self, msg: str):
        """Print debug message if debug mode is enabled"""
        if self.debug:
            self.console.print(f"[dim]DEBUG: {msg}[/dim]")

    def _stream_response_with_display(self, response, tts_enabled: bool = False) -> str:
        """
        Stream response with Live markdown display and optional TTS.

        Args:
            response: LLM Response object (iterable)
            tts_enabled: Whether to queue sentences to TTS

        Returns:
            Full accumulated response text
        """
        accumulated_text = ""
        sentence_buffer = SentenceBuffer() if tts_enabled else None

        # Use transient=True initially, switch to False only if we have content
        with Live(Markdown(""), refresh_per_second=10, console=self.console, transient=True) as live:
            for chunk in response:
                accumulated_text += chunk
                live.update(Markdown(accumulated_text))
                # Once we have content, make it persistent
                if accumulated_text.strip():
                    live.transient = False

                # Broadcast to web companion (non-blocking)
                if self.web_clients:
                    self._broadcast_to_web({
                        "type": "assistant_chunk",
                        "content": accumulated_text,
                        "done": False
                    })

                # Queue TTS if enabled (non-blocking)
                if tts_enabled and sentence_buffer:
                    sentence = sentence_buffer.add(chunk)
                    if sentence:
                        self.speech_output.speak_sentence(sentence)

        # Flush remaining TTS
        if tts_enabled and sentence_buffer:
            remaining = sentence_buffer.flush()
            if remaining:
                self.speech_output.speak_sentence(remaining)

        # Broadcast completion to web companion (always send for cleanup, web UI filters empty)
        if self.web_clients:
            self._broadcast_to_web({
                "type": "assistant_complete",
                "content": accumulated_text,
                "done": True
            })
            # Update token usage bar after response (only if non-empty)
            if accumulated_text.strip():
                self._broadcast_token_update()

        return accumulated_text

    # =========================================================================
    # Web Companion Methods
    # =========================================================================

    def _get_web_html(self) -> str:
        """Return the inline HTML for the web companion interface."""
        return '''<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>llm-assistant</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css" id="hljs-light">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css" id="hljs-dark" disabled>
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <style>
        :root {
            --bg-primary: #ffffff;
            --bg-secondary: #f7f7f8;
            --bg-tertiary: #ececf1;
            --bg-user: #f0f4ff;
            --bg-assistant: #ffffff;
            --bg-code: #f6f8fa;
            --text-primary: #1a1a1a;
            --text-secondary: #6b6b6b;
            --text-muted: #9a9a9a;
            --border-color: #e5e5e5;
            --accent-user: #2563eb;
            --accent-assistant: #059669;
            --accent-hover: #1d4ed8;
            --accent-watch: #f59e0b;
            --accent-tool: #8b5cf6;
            --shadow-sm: 0 1px 2px rgba(0,0,0,0.04);
            --shadow-md: 0 4px 12px rgba(0,0,0,0.08);
            --shadow-lg: 0 8px 24px rgba(0,0,0,0.12);
            --radius-sm: 6px;
            --radius-md: 12px;
            --radius-lg: 16px;
        }
        [data-theme="dark"] {
            --bg-primary: #1a1a2e;
            --bg-secondary: #16213e;
            --bg-tertiary: #0f3460;
            --bg-user: #1e3a5f;
            --bg-assistant: #1a1a2e;
            --bg-code: #0d1117;
            --text-primary: #e5e5e5;
            --text-secondary: #a0a0a0;
            --text-muted: #6b6b6b;
            --border-color: #2d4a6f;
            --accent-user: #60a5fa;
            --accent-assistant: #34d399;
            --shadow-sm: 0 1px 2px rgba(0,0,0,0.2);
            --shadow-md: 0 4px 12px rgba(0,0,0,0.3);
            --shadow-lg: 0 8px 24px rgba(0,0,0,0.4);
        }
        * { box-sizing: border-box; margin: 0; padding: 0; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', sans-serif;
            font-size: 90%;
            background: var(--bg-secondary);
            color: var(--text-primary);
            min-height: 100vh;
            line-height: 1.5;
        }
        header {
            position: sticky;
            top: 0;
            background: var(--bg-primary);
            padding: 12px 20px;
            display: flex;
            justify-content: space-between;
            align-items: center;
            border-bottom: 1px solid var(--border-color);
            box-shadow: var(--shadow-sm);
            z-index: 100;
        }
        .header-left {
            display: flex;
            align-items: center;
            gap: 12px;
        }
        header h1 {
            font-size: 1.1rem;
            font-weight: 600;
            color: var(--text-primary);
        }
        .header-badges {
            display: flex;
            align-items: center;
            gap: 8px;
        }
        #status {
            font-size: 0.7rem;
            padding: 3px 8px;
            border-radius: 20px;
            font-weight: 500;
        }
        #status.connected {
            background: #dcfce7;
            color: #166534;
        }
        [data-theme="dark"] #status.connected {
            background: #166534;
            color: #dcfce7;
        }
        #status.disconnected {
            background: #fee2e2;
            color: #991b1b;
        }
        [data-theme="dark"] #status.disconnected {
            background: #991b1b;
            color: #fee2e2;
        }
        .model-badge {
            font-size: 0.7rem;
            padding: 3px 8px;
            background: var(--bg-tertiary);
            border-radius: 12px;
            color: var(--text-secondary);
            font-family: 'SF Mono', Consolas, monospace;
        }
        .mode-badge {
            font-size: 0.65rem;
            padding: 2px 6px;
            border-radius: 10px;
            font-weight: 600;
            text-transform: uppercase;
        }
        .mode-badge.assistant {
            background: #dbeafe;
            color: #1e40af;
        }
        [data-theme="dark"] .mode-badge.assistant {
            background: #1e40af;
            color: #dbeafe;
        }
        .mode-badge.agent {
            background: #fef3c7;
            color: #92400e;
        }
        [data-theme="dark"] .mode-badge.agent {
            background: #92400e;
            color: #fef3c7;
        }
        .watch-badge {
            font-size: 0.65rem;
            padding: 2px 6px;
            border-radius: 10px;
            font-weight: 500;
            display: none;
            align-items: center;
            gap: 4px;
        }
        .watch-badge.active {
            display: inline-flex;
            background: #fef3c7;
            color: #92400e;
        }
        [data-theme="dark"] .watch-badge.active {
            background: #92400e;
            color: #fef3c7;
        }
        .watch-badge .pulse {
            width: 6px;
            height: 6px;
            background: var(--accent-watch);
            border-radius: 50%;
            animation: pulse 2s infinite;
        }
        @keyframes pulse {
            0%, 100% { opacity: 1; transform: scale(1); }
            50% { opacity: 0.5; transform: scale(1.2); }
        }
        .header-buttons { display: flex; gap: 6px; align-items: center; }
        button {
            background: var(--bg-tertiary);
            color: var(--text-primary);
            border: 1px solid var(--border-color);
            padding: 6px 12px;
            border-radius: var(--radius-sm);
            cursor: pointer;
            font-size: 0.8rem;
            font-weight: 500;
            transition: all 0.15s ease;
        }
        button:hover {
            background: var(--accent-user);
            color: white;
            border-color: var(--accent-user);
        }
        .icon-btn {
            padding: 6px 8px;
            font-size: 1rem;
            line-height: 1;
        }
        .icon-btn:hover {
            background: var(--bg-tertiary);
            color: var(--text-primary);
            border-color: var(--border-color);
        }
        /* Token Usage Bar */
        .token-bar-container {
            background: var(--bg-primary);
            border-bottom: 1px solid var(--border-color);
            padding: 6px 20px;
            display: none;
        }
        .token-bar-container.visible { display: block; }
        .token-bar {
            max-width: 860px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            gap: 12px;
        }
        .token-bar-label {
            font-size: 0.7rem;
            color: var(--text-secondary);
            white-space: nowrap;
        }
        .token-bar-track {
            flex: 1;
            height: 4px;
            background: var(--bg-tertiary);
            border-radius: 2px;
            overflow: hidden;
        }
        .token-bar-fill {
            height: 100%;
            background: linear-gradient(90deg, var(--accent-assistant), var(--accent-user));
            border-radius: 2px;
            transition: width 0.3s ease;
            width: 0%;
        }
        .token-bar-fill.warning { background: linear-gradient(90deg, #f59e0b, #ef4444); }
        .token-bar-text {
            font-size: 0.7rem;
            color: var(--text-muted);
            font-family: 'SF Mono', Consolas, monospace;
            min-width: 120px;
            text-align: right;
        }
        /* Search Bar */
        .search-bar {
            position: fixed;
            top: 60px;
            right: 20px;
            background: var(--bg-primary);
            border: 1px solid var(--border-color);
            border-radius: var(--radius-sm);
            padding: 8px 12px;
            display: none;
            align-items: center;
            gap: 8px;
            box-shadow: var(--shadow-md);
            z-index: 200;
        }
        .search-bar.visible { display: flex; }
        .search-bar input {
            border: none;
            background: transparent;
            color: var(--text-primary);
            font-size: 0.85rem;
            width: 200px;
            outline: none;
        }
        .search-bar input::placeholder { color: var(--text-muted); }
        .search-count {
            font-size: 0.75rem;
            color: var(--text-muted);
            white-space: nowrap;
        }
        .search-bar button {
            padding: 4px 8px;
            font-size: 0.75rem;
        }
        mark {
            background: #fef08a;
            color: #1a1a1a;
            padding: 1px 2px;
            border-radius: 2px;
        }
        [data-theme="dark"] mark {
            background: #854d0e;
            color: #fef08a;
        }
        mark.current {
            background: #fb923c;
            color: white;
        }
        /* Copy Dropdown */
        .copy-dropdown {
            position: relative;
            display: inline-flex;
        }
        .copy-dropdown-menu {
            position: absolute;
            top: 100%;
            right: 0;
            margin-top: 4px;
            background: var(--bg-primary);
            border: 1px solid var(--border-color);
            border-radius: var(--radius-sm);
            box-shadow: var(--shadow-md);
            display: none;
            z-index: 50;
            white-space: nowrap;
        }
        .copy-dropdown-menu.show { display: block; }
        .copy-dropdown-menu button {
            display: block;
            width: 100%;
            border: none;
            border-radius: 0;
            padding: 8px 12px;
            font-size: 0.75rem;
            white-space: nowrap;
            text-align: left;
        }
        .copy-dropdown-menu button:first-child { border-radius: var(--radius-sm) var(--radius-sm) 0 0; }
        .copy-dropdown-menu button:last-child { border-radius: 0 0 var(--radius-sm) var(--radius-sm); }
        .copy-dropdown-menu button:hover {
            background: var(--bg-tertiary);
            color: var(--text-primary);
        }
        #messages {
            max-width: 860px;
            margin: 0 auto;
            padding: 16px 16px;
            display: flex;
            flex-direction: column;
            gap: 12px;
        }
        .message {
            padding: 14px 18px;
            border-radius: var(--radius-md);
            position: relative;
            box-shadow: var(--shadow-sm);
            border: 1px solid var(--border-color);
            animation: slideIn 0.2s ease-out;
        }
        @keyframes slideIn {
            from { opacity: 0; transform: translateY(8px); }
            to { opacity: 1; transform: translateY(0); }
        }
        .message.user {
            background: var(--bg-user);
            border-left: 3px solid var(--accent-user);
        }
        .message.assistant {
            background: var(--bg-assistant);
            border-left: 3px solid var(--accent-assistant);
        }
        .message-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 8px;
        }
        .role-badge {
            display: flex;
            align-items: center;
            gap: 8px;
        }
        .role-icon {
            width: 24px;
            height: 24px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 12px;
            font-weight: 600;
            color: white;
        }
        .message.user .role-icon { background: var(--accent-user); }
        .message.assistant .role-icon { background: var(--accent-assistant); }
        .role {
            font-weight: 600;
            font-size: 0.85rem;
        }
        .message.user .role { color: var(--accent-user); }
        .message.assistant .role { color: var(--accent-assistant); }
        .content {
            line-height: 1.7;
            word-wrap: break-word;
            color: var(--text-primary);
        }
        .content p { margin-bottom: 12px; }
        .content p:last-child { margin-bottom: 0; }
        .content ul, .content ol {
            margin: 12px 0;
            padding-left: 24px;
        }
        .content li { margin-bottom: 6px; }
        .content strong { font-weight: 600; }
        .content code:not(pre code) {
            background: var(--bg-tertiary);
            padding: 2px 6px;
            border-radius: 4px;
            font-size: 0.9em;
            font-family: 'SF Mono', Consolas, monospace;
        }
        .content pre {
            position: relative;
            margin: 16px 0;
            border-radius: var(--radius-sm);
            overflow: hidden;
            background: var(--bg-code);
            border: 1px solid var(--border-color);
        }
        .content pre code {
            display: block;
            padding: 16px;
            overflow-x: auto;
            font-size: 0.875rem;
            line-height: 1.5;
            font-family: 'SF Mono', Consolas, 'Liberation Mono', monospace;
            background: transparent;
        }
        .copy-code-btn {
            position: absolute;
            top: 8px;
            right: 8px;
            padding: 4px 10px;
            font-size: 0.7rem;
            background: var(--bg-primary);
            opacity: 0;
            transition: opacity 0.15s;
            box-shadow: var(--shadow-sm);
        }
        .content pre:hover .copy-code-btn { opacity: 1; }
        .copy-msg-btn {
            padding: 4px 10px;
            font-size: 0.75rem;
            background: transparent;
            border: 1px solid var(--border-color);
        }
        .copy-msg-btn:hover {
            background: var(--bg-tertiary);
            color: var(--text-primary);
            border-color: var(--border-color);
        }
        /* Tool Call Visualization */
        .tool-call {
            margin: 12px 0;
            border: 1px solid var(--border-color);
            border-radius: var(--radius-sm);
            overflow: hidden;
            background: var(--bg-tertiary);
        }
        .tool-header {
            display: flex;
            align-items: center;
            gap: 8px;
            padding: 8px 12px;
            cursor: pointer;
            user-select: none;
            transition: background 0.15s;
        }
        .tool-header:hover { background: var(--bg-secondary); }
        .tool-icon { font-size: 14px; }
        .tool-name {
            font-family: 'SF Mono', Consolas, monospace;
            font-size: 0.8rem;
            font-weight: 600;
            color: var(--accent-tool);
        }
        .tool-status {
            font-size: 0.7rem;
            padding: 2px 6px;
            border-radius: 10px;
            margin-left: auto;
        }
        .tool-status.success { background: #dcfce7; color: #166534; }
        .tool-status.error { background: #fee2e2; color: #991b1b; }
        .tool-status.pending { background: #fef3c7; color: #92400e; }
        [data-theme="dark"] .tool-status.success { background: #166534; color: #dcfce7; }
        [data-theme="dark"] .tool-status.error { background: #991b1b; color: #fee2e2; }
        [data-theme="dark"] .tool-status.pending { background: #92400e; color: #fef3c7; }
        .tool-expand {
            font-size: 0.7rem;
            color: var(--text-muted);
            transition: transform 0.2s;
        }
        .tool-call.expanded .tool-expand { transform: rotate(90deg); }
        .tool-body {
            display: none;
            padding: 12px;
            border-top: 1px solid var(--border-color);
            background: var(--bg-primary);
        }
        .tool-call.expanded .tool-body { display: block; }
        .tool-section {
            margin-bottom: 12px;
        }
        .tool-section:last-child { margin-bottom: 0; }
        .tool-section-label {
            font-size: 0.7rem;
            font-weight: 600;
            color: var(--text-secondary);
            text-transform: uppercase;
            margin-bottom: 4px;
        }
        .tool-section pre {
            margin: 0;
            padding: 8px;
            background: var(--bg-code);
            border-radius: 4px;
            font-size: 0.75rem;
            overflow-x: auto;
        }
        .tool-section pre code {
            background: transparent;
            padding: 0;
        }
        .toast {
            position: fixed;
            bottom: 24px;
            right: 24px;
            background: var(--text-primary);
            color: var(--bg-primary);
            padding: 12px 20px;
            border-radius: var(--radius-sm);
            font-weight: 500;
            font-size: 0.9rem;
            transform: translateY(80px);
            opacity: 0;
            transition: all 0.25s cubic-bezier(0.4, 0, 0.2, 1);
            box-shadow: var(--shadow-lg);
            z-index: 1000;
        }
        .toast.show {
            transform: translateY(0);
            opacity: 1;
        }
        .streaming .content::after {
            content: '▋';
            animation: blink 1s infinite;
            color: var(--accent-assistant);
            margin-left: 2px;
        }
        @keyframes blink {
            0%, 50% { opacity: 1; }
            51%, 100% { opacity: 0; }
        }
        /* Debug modal styles */
        .debug-modal-overlay {
            display: none;
            position: fixed;
            inset: 0;
            background: rgba(0,0,0,0.5);
            z-index: 200;
        }
        .debug-modal-overlay.active { display: flex; justify-content: center; align-items: center; }
        .debug-modal {
            background: var(--bg-primary);
            border-radius: var(--radius-lg);
            box-shadow: var(--shadow-lg);
            width: 90%;
            max-width: 900px;
            max-height: 85vh;
            overflow: hidden;
            display: flex;
            flex-direction: column;
        }
        .debug-modal-header {
            padding: 16px 20px;
            border-bottom: 1px solid var(--border-color);
            display: flex;
            justify-content: space-between;
            align-items: center;
        }
        .debug-modal-header h2 { font-size: 1rem; font-weight: 600; }
        .debug-modal-content {
            flex: 1;
            overflow-y: auto;
            padding: 20px;
        }
        .debug-section {
            margin-bottom: 20px;
            border: 1px solid var(--border-color);
            border-radius: var(--radius-sm);
            overflow: hidden;
        }
        .debug-section-header {
            display: flex;
            align-items: center;
            gap: 8px;
            padding: 10px 12px;
            cursor: pointer;
            background: var(--bg-tertiary);
            user-select: none;
        }
        .debug-section-header:hover { background: var(--bg-secondary); }
        .debug-section-title { font-weight: 600; font-size: 0.85rem; }
        .debug-section-expand {
            margin-left: auto;
            font-size: 0.7rem;
            color: var(--text-muted);
            transition: transform 0.2s;
        }
        .debug-section.expanded .debug-section-expand { transform: rotate(90deg); }
        .debug-section-body {
            display: none;
            padding: 12px;
            background: var(--bg-code);
            border-top: 1px solid var(--border-color);
        }
        .debug-section.expanded .debug-section-body { display: block; }
        .debug-section-body pre {
            margin: 0;
            white-space: pre-wrap;
            word-break: break-word;
            font-size: 0.75rem;
            max-height: 400px;
            overflow-y: auto;
        }
        .debug-meta {
            display: flex;
            flex-wrap: wrap;
            gap: 12px;
            margin-bottom: 16px;
            font-size: 0.8rem;
        }
        .debug-meta-item {
            background: var(--bg-tertiary);
            padding: 4px 10px;
            border-radius: var(--radius-sm);
        }
        .debug-meta-label { color: var(--text-secondary); }
        .debug-msg {
            margin: 8px 0;
            padding: 8px 12px;
            border-radius: var(--radius-sm);
            border-left: 3px solid;
        }
        .debug-msg.user { background: var(--bg-user); border-color: var(--accent-user); }
        .debug-msg.assistant { background: var(--bg-assistant); border-color: var(--accent-assistant); }
        .debug-msg.tool { background: #f5f3ff; border-color: var(--accent-tool); }
        .debug-msg.tool-result { background: #fef3c7; border-color: #f59e0b; }
        [data-theme="dark"] .debug-msg.tool { background: #2d2640; }
        [data-theme="dark"] .debug-msg.tool-result { background: #422d1a; }
        .token-info {
            font-size: 0.6rem;
            background: var(--bg-tertiary);
            padding: 1px 5px;
            border-radius: 8px;
            margin-left: 6px;
            color: var(--text-secondary);
        }
        .debug-msg-role {
            font-size: 0.7rem;
            font-weight: 600;
            text-transform: uppercase;
            margin-bottom: 4px;
        }
        .debug-msg-content {
            font-family: 'SF Mono', Consolas, monospace;
            font-size: 0.75rem;
            white-space: pre-wrap;
            max-height: 300px;
            overflow-y: auto;
        }
        .context-badge {
            font-size: 0.6rem;
            background: var(--accent-tool);
            color: white;
            padding: 1px 5px;
            border-radius: 8px;
            margin-left: 6px;
        }
        .attachment-badge {
            font-size: 0.6rem;
            background: #3b82f6;
            color: white;
            padding: 1px 5px;
            border-radius: 8px;
            margin-left: 6px;
        }
        .debug-attachments {
            margin-top: 8px;
            padding-top: 8px;
            border-top: 1px dashed var(--border-color);
        }
        .debug-attachment {
            display: flex;
            align-items: center;
            gap: 8px;
            font-size: 0.7rem;
            padding: 2px 0;
        }
        .debug-attachment-type {
            background: var(--bg-tertiary);
            padding: 1px 6px;
            border-radius: 4px;
            font-family: 'SF Mono', Consolas, monospace;
        }
        .debug-attachment-source {
            color: var(--text-secondary);
            word-break: break-all;
        }
        /* Tool definitions styling */
        .debug-tool {
            margin: 8px 0;
            padding: 10px 12px;
            border-radius: var(--radius-sm);
            background: var(--bg-secondary);
            border: 1px solid var(--border-color);
        }
        .debug-tool-header {
            display: flex;
            align-items: center;
            gap: 8px;
            margin-bottom: 4px;
        }
        .debug-tool-name {
            font-family: 'SF Mono', Consolas, monospace;
            font-weight: 600;
            font-size: 0.8rem;
            color: var(--accent-tool);
        }
        .tool-plugin-badge {
            font-size: 0.6rem;
            background: var(--bg-tertiary);
            color: var(--text-secondary);
            padding: 1px 6px;
            border-radius: 8px;
        }
        .debug-tool-desc {
            font-size: 0.75rem;
            color: var(--text-secondary);
            margin-bottom: 6px;
        }
        .debug-tool-schema {
            margin: 0;
            padding: 8px;
            background: var(--bg-code);
            border-radius: var(--radius-sm);
            font-size: 0.7rem;
            max-height: 200px;
            overflow-y: auto;
        }
        .debug-tool-schema code {
            white-space: pre-wrap;
            word-break: break-word;
        }
        .empty-state {
            text-align: center;
            padding: 60px 20px;
            color: var(--text-muted);
        }
        .empty-state-icon {
            font-size: 48px;
            margin-bottom: 16px;
        }
    </style>
</head>
<body>
    <header>
        <div class="header-left">
            <h1>llm-assistant</h1>
            <div class="header-badges">
                <span id="status" class="disconnected">Disconnected</span>
                <span id="model-badge" class="model-badge" style="display:none;"></span>
                <span id="mode-badge" class="mode-badge" style="display:none;"></span>
                <span id="watch-badge" class="watch-badge"><span class="pulse"></span><span id="watch-text">Watch</span></span>
            </div>
        </div>
        <div class="header-buttons">
            <button class="icon-btn" onclick="toggleSearch()" title="Search (Ctrl+F)">🔍</button>
            <button class="icon-btn" onclick="openDebugModal()" title="Debug info">🐛</button>
            <button class="icon-btn" onclick="toggleTheme()" id="theme-btn" title="Toggle theme">🌙</button>
            <div class="copy-dropdown">
                <button onclick="toggleCopyMenu(this)">Copy All ▾</button>
                <div class="copy-dropdown-menu">
                    <button onclick="copyAllText()">Copy as Text</button>
                    <button onclick="copyAllMarkdown()">Copy as Markdown</button>
                </div>
            </div>
        </div>
    </header>
    <div class="token-bar-container" id="token-bar-container">
        <div class="token-bar">
            <span class="token-bar-label">Context:</span>
            <div class="token-bar-track">
                <div class="token-bar-fill" id="token-bar-fill"></div>
            </div>
            <span class="token-bar-text" id="token-bar-text">0 / 0</span>
        </div>
    </div>
    <div class="search-bar" id="search-bar">
        <input type="text" id="search-input" placeholder="Search..." oninput="doSearch()">
        <span class="search-count" id="search-count">0/0</span>
        <button onclick="searchPrev()">↑</button>
        <button onclick="searchNext()">↓</button>
        <button onclick="closeSearch()">✕</button>
    </div>
    <div id="messages">
        <div class="empty-state" id="empty-state">
            <div class="empty-state-icon">💬</div>
            <p>Waiting for conversation...</p>
        </div>
    </div>
    <div id="toast" class="toast">Copied!</div>
    <div class="debug-modal-overlay" id="debug-modal-overlay" onclick="closeDebugModal(event)">
        <div class="debug-modal" onclick="event.stopPropagation()">
            <div class="debug-modal-header">
                <h2>Debug Info</h2>
                <button class="icon-btn" onclick="closeDebugModal()">&times;</button>
            </div>
            <div class="debug-modal-content" id="debug-modal-content">
                <div class="empty-state">Loading...</div>
            </div>
        </div>
    </div>

    <script>
        // Store original markdown for copy-as-markdown feature
        const messageMarkdown = new Map();
        let messageIdCounter = 0;

        marked.setOptions({
            highlight: function(code, lang) {
                if (lang && hljs.getLanguage(lang)) {
                    return hljs.highlight(code, { language: lang }).value;
                }
                return hljs.highlightAuto(code).value;
            },
            breaks: true
        });

        let ws;
        let reconnectTimer;
        const messagesContainer = document.getElementById('messages');
        const status = document.getElementById('status');
        const emptyState = document.getElementById('empty-state');
        let currentAssistantDiv = null;
        let currentAssistantMarkdown = '';

        // Search state
        let searchMatches = [];
        let currentMatchIndex = -1;

        // Theme handling
        function getTheme() {
            return localStorage.getItem('theme') || 'light';
        }

        function setTheme(theme) {
            document.documentElement.setAttribute('data-theme', theme);
            localStorage.setItem('theme', theme);
            document.getElementById('theme-btn').textContent = theme === 'dark' ? '☀️' : '🌙';
            // Switch highlight.js theme
            document.getElementById('hljs-light').disabled = theme === 'dark';
            document.getElementById('hljs-dark').disabled = theme === 'light';
        }

        function toggleTheme() {
            const current = getTheme();
            setTheme(current === 'light' ? 'dark' : 'light');
        }

        // Initialize theme
        setTheme(getTheme());

        function hideEmptyState() {
            if (emptyState) emptyState.style.display = 'none';
        }

        function showEmptyState() {
            if (emptyState) emptyState.style.display = 'block';
        }

        function connect() {
            ws = new WebSocket('ws://' + window.location.host + '/ws');

            ws.onopen = () => {
                status.textContent = 'Connected';
                status.className = 'connected';
                clearTimeout(reconnectTimer);
            };

            ws.onclose = () => {
                status.textContent = 'Disconnected';
                status.className = 'disconnected';
                reconnectTimer = setTimeout(connect, 2000);
            };

            ws.onerror = () => {
                ws.close();
            };

            ws.onmessage = (event) => {
                const data = JSON.parse(event.data);
                handleMessage(data);
            };
        }

        function handleMessage(data) {
            switch(data.type) {
                case 'user_message':
                    addUserMessage(data.content);
                    break;
                case 'assistant_chunk':
                    updateAssistantMessage(data.content, false);
                    break;
                case 'assistant_complete':
                    updateAssistantMessage(data.content, true);
                    break;
                case 'clear':
                    clearMessages();
                    break;
                case 'history':
                    clearMessages();
                    data.messages.forEach(msg => {
                        if (msg.role === 'user') {
                            addUserMessage(msg.content);
                        } else if (msg.role === 'tool_call') {
                            addToolCall(msg.tool_name, msg.arguments, msg.result, msg.status);
                        } else if (msg.content && msg.content.trim()) {
                            addAssistantMessage(msg.content);
                        }
                    });
                    break;
                case 'session_info':
                    updateSessionInfo(data);
                    break;
                case 'watch_status':
                    updateWatchStatus(data);
                    break;
                case 'token_update':
                    updateTokenBar(data);
                    break;
                case 'tool_call':
                    addToolCall(data.tool_name, data.arguments, data.result, data.status);
                    break;
                case 'debug_info':
                    renderDebugInfo(data);
                    break;
            }
            scrollToBottom();
        }

        function updateSessionInfo(data) {
            const modelBadge = document.getElementById('model-badge');
            const modeBadge = document.getElementById('mode-badge');
            if (data.model) {
                modelBadge.textContent = data.model;
                modelBadge.style.display = 'inline';
            }
            if (data.mode) {
                modeBadge.textContent = data.mode;
                modeBadge.className = 'mode-badge ' + data.mode;
                modeBadge.style.display = 'inline';
            }
        }

        function updateWatchStatus(data) {
            const watchBadge = document.getElementById('watch-badge');
            const watchText = document.getElementById('watch-text');
            if (data.active) {
                watchBadge.classList.add('active');
                watchText.textContent = data.goal ? 'Watch: ' + data.goal.substring(0, 20) + (data.goal.length > 20 ? '...' : '') : 'Watch';
                watchText.title = data.goal || '';
            } else {
                watchBadge.classList.remove('active');
            }
        }

        function updateTokenBar(data) {
            const container = document.getElementById('token-bar-container');
            const fill = document.getElementById('token-bar-fill');
            const text = document.getElementById('token-bar-text');
            container.classList.add('visible');
            const pct = Math.min(100, data.percentage || 0);
            fill.style.width = pct + '%';
            fill.classList.toggle('warning', pct > 80);
            const current = formatTokens(data.current_tokens || 0);
            const max = formatTokens(data.max_tokens || 0);
            text.textContent = current + ' / ' + max + ' (' + Math.round(pct) + '%)';
        }

        function formatTokens(n) {
            if (n >= 1000000) return (n / 1000000).toFixed(1) + 'M';
            if (n >= 1000) return (n / 1000).toFixed(0) + 'k';
            return n.toString();
        }

        function formatBytes(bytes) {
            if (bytes >= 1048576) return (bytes / 1048576).toFixed(1) + ' MB';
            if (bytes >= 1024) return (bytes / 1024).toFixed(1) + ' KB';
            return bytes + ' B';
        }

        // Debug modal functions
        let debugRefreshInterval = null;
        let lastDebugDataHash = null;

        function openDebugModal() {
            lastDebugDataHash = null;  // Reset hash on open
            document.getElementById('debug-modal-overlay').classList.add('active');
            // Clear any existing interval first (safety)
            if (debugRefreshInterval) {
                clearInterval(debugRefreshInterval);
            }
            // Request debug info from server
            requestDebugInfo();
            // Auto-refresh every 2 seconds while modal is open
            debugRefreshInterval = setInterval(requestDebugInfo, 2000);
        }

        function requestDebugInfo() {
            if (ws && ws.readyState === WebSocket.OPEN) {
                ws.send('debug_request');
            }
        }

        function closeDebugModal(event) {
            if (!event || event.target === event.currentTarget) {
                document.getElementById('debug-modal-overlay').classList.remove('active');
                // Stop auto-refresh
                if (debugRefreshInterval) {
                    clearInterval(debugRefreshInterval);
                    debugRefreshInterval = null;
                }
            }
        }

        function renderDebugInfo(data) {
            // Skip re-render if data hasn't changed (preserves text selection)
            const dataHash = JSON.stringify(data);
            if (dataHash === lastDebugDataHash) {
                return;
            }
            lastDebugDataHash = dataHash;

            const container = document.getElementById('debug-modal-content');

            // Meta info row
            const optionsStr = data.options && Object.keys(data.options).length > 0
                ? Object.entries(data.options).map(([k, v]) => `${k}=${v}`).join(', ')
                : '';
            let html = `<div class="debug-meta">
                <div class="debug-meta-item"><span class="debug-meta-label">Model:</span> ${escapeHtml(data.model)}</div>
                <div class="debug-meta-item"><span class="debug-meta-label">Mode:</span> ${escapeHtml(data.mode)}</div>
                <div class="debug-meta-item"><span class="debug-meta-label">Tokens:</span> ${formatTokens(data.current_tokens)} / ${formatTokens(data.max_context)}</div>
                ${optionsStr ? `<div class="debug-meta-item"><span class="debug-meta-label">Options:</span> ${escapeHtml(optionsStr)}</div>` : ''}
                ${data.loaded_kbs && data.loaded_kbs.length ? `<div class="debug-meta-item"><span class="debug-meta-label">KBs:</span> ${data.loaded_kbs.join(', ')}</div>` : ''}
                ${data.active_rag ? `<div class="debug-meta-item"><span class="debug-meta-label">RAG:</span> ${escapeHtml(data.active_rag)}</div>` : ''}
            </div>`;

            // System Prompt section (expanded by default)
            html += `<div class="debug-section expanded">
                <div class="debug-section-header" onclick="toggleDebugSection(this.parentElement)">
                    <span class="debug-section-title">System Prompt</span>
                    <span class="debug-section-expand">▶</span>
                </div>
                <div class="debug-section-body">
                    <pre><code>${escapeHtml(data.system_prompt)}</code></pre>
                </div>
            </div>`;

            // Tools section (collapsed by default)
            if (data.tools && data.tools.length > 0) {
                html += `<div class="debug-section">
                    <div class="debug-section-header" onclick="toggleDebugSection(this.parentElement)">
                        <span class="debug-section-title">Tools (${data.tools.length})</span>
                        <span class="debug-section-expand">▶</span>
                    </div>
                    <div class="debug-section-body">`;
                data.tools.forEach(tool => {
                    const pluginBadge = tool.plugin ? `<span class="tool-plugin-badge">${escapeHtml(tool.plugin)}</span>` : '';
                    html += `<div class="debug-tool">
                        <div class="debug-tool-header">
                            <span class="debug-tool-name">${escapeHtml(tool.name)}</span>
                            ${pluginBadge}
                        </div>
                        <div class="debug-tool-desc">${escapeHtml(tool.description)}</div>
                        <pre class="debug-tool-schema"><code>${escapeHtml(JSON.stringify(tool.parameters, null, 2))}</code></pre>
                    </div>`;
                });
                html += `</div></div>`;
            }

            // Conversation section
            html += `<div class="debug-section expanded">
                <div class="debug-section-header" onclick="toggleDebugSection(this.parentElement)">
                    <span class="debug-section-title">Conversation (${data.messages.length} messages)</span>
                    <span class="debug-section-expand">▶</span>
                </div>
                <div class="debug-section-body">`;

            data.messages.forEach((msg, i) => {
                if (msg.role === 'user') {
                    const contextBadge = msg.has_context ? '<span class="context-badge">has context</span>' : '';
                    const attachBadge = (msg.attachments && msg.attachments.length > 0)
                        ? `<span class="attachment-badge">📎 ${msg.attachments.length}</span>` : '';
                    const tokenInfo = (msg.input_tokens || msg.output_tokens)
                        ? `<span class="token-info">${msg.input_tokens || 0}→${msg.output_tokens || 0}</span>` : '';
                    // Build attachments detail if present
                    let attachmentsHtml = '';
                    if (msg.attachments && msg.attachments.length > 0) {
                        attachmentsHtml = '<div class="debug-attachments">';
                        msg.attachments.forEach(att => {
                            const sizeInfo = att.size ? ` (${formatBytes(att.size)})` : '';
                            const source = att.path || att.url || (att.inline ? 'inline' : '');
                            attachmentsHtml += `<div class="debug-attachment">
                                <span class="debug-attachment-type">${escapeHtml(att.type)}</span>
                                ${source ? `<span class="debug-attachment-source">${escapeHtml(source)}${sizeInfo}</span>` : ''}
                            </div>`;
                        });
                        attachmentsHtml += '</div>';
                    }
                    html += `<div class="debug-msg user">
                        <div class="debug-msg-role">user${contextBadge}${attachBadge}${tokenInfo}</div>
                        <div class="debug-msg-content">${escapeHtml(msg.content)}</div>
                        ${attachmentsHtml}
                    </div>`;
                } else if (msg.role === 'assistant') {
                    html += `<div class="debug-msg assistant">
                        <div class="debug-msg-role">assistant</div>
                        <div class="debug-msg-content">${escapeHtml(msg.content)}</div>
                    </div>`;
                } else if (msg.role === 'tool_call') {
                    html += `<div class="debug-msg tool">
                        <div class="debug-msg-role">tool_call: ${escapeHtml(msg.tool_name)}</div>
                        <div class="debug-msg-content">${escapeHtml(JSON.stringify(msg.arguments, null, 2))}</div>
                    </div>`;
                } else if (msg.role === 'tool_result') {
                    html += `<div class="debug-msg tool-result">
                        <div class="debug-msg-role">tool_result: ${escapeHtml(msg.tool_name)}</div>
                        <div class="debug-msg-content">${escapeHtml(msg.content)}</div>
                    </div>`;
                }
            });

            html += `</div></div>`;
            container.innerHTML = html;
        }

        function toggleDebugSection(el) {
            el.classList.toggle('expanded');
        }

        // Track pending tool calls for update
        let pendingToolCall = null;

        function addToolCall(toolName, args, result, toolStatus) {
            hideEmptyState();
            const statusClass = toolStatus === 'success' ? 'success' : toolStatus === 'error' ? 'error' : 'pending';
            const statusText = toolStatus === 'success' ? '✓' : toolStatus === 'error' ? '✗' : '...';

            // If this is an update to a pending tool call (same name, not pending status)
            if (pendingToolCall && toolStatus !== 'pending') {
                // Update existing element
                const statusEl = pendingToolCall.querySelector('.tool-status');
                statusEl.className = 'tool-status ' + statusClass;
                statusEl.textContent = statusText;

                // Add result section if provided
                if (result) {
                    const body = pendingToolCall.querySelector('.tool-body');
                    const resultDiv = document.createElement('div');
                    resultDiv.className = 'tool-section';
                    resultDiv.innerHTML = `
                        <div class="tool-section-label">Result</div>
                        <pre><code>${escapeHtml(typeof result === 'string' ? result : JSON.stringify(result, null, 2))}</code></pre>
                    `;
                    body.appendChild(resultDiv);
                }
                // Auto-collapse when complete
                pendingToolCall.classList.remove('expanded');
                pendingToolCall = null;
                return;
            }

            // Create new tool call element
            const div = document.createElement('div');
            div.className = 'tool-call';
            div.innerHTML = `
                <div class="tool-header" onclick="toggleToolCall(this.parentElement)">
                    <span class="tool-icon">🔧</span>
                    <span class="tool-name">${escapeHtml(toolName)}</span>
                    <span class="tool-status ${statusClass}">${statusText}</span>
                    <span class="tool-expand">▶</span>
                </div>
                <div class="tool-body">
                    <div class="tool-section">
                        <div class="tool-section-label">Arguments</div>
                        <pre><code>${escapeHtml(typeof args === 'string' ? args : JSON.stringify(args, null, 2))}</code></pre>
                    </div>
                    ${result ? `<div class="tool-section">
                        <div class="tool-section-label">Result</div>
                        <pre><code>${escapeHtml(typeof result === 'string' ? result : JSON.stringify(result, null, 2))}</code></pre>
                    </div>` : ''}
                </div>
            `;
            messagesContainer.appendChild(div);

            // Track pending tool calls and auto-expand for monitoring
            if (toolStatus === 'pending') {
                pendingToolCall = div;
                div.classList.add('expanded');
            }
        }

        function toggleToolCall(el) {
            el.classList.toggle('expanded');
        }

        function clearMessages() {
            const msgs = messagesContainer.querySelectorAll('.message, .tool-call');
            msgs.forEach(m => m.remove());
            currentAssistantDiv = null;
            currentAssistantMarkdown = '';
            messageMarkdown.clear();
            messageIdCounter = 0;
            pendingToolCall = null;
            showEmptyState();
        }

        function scrollToBottom() {
            window.scrollTo({ top: document.body.scrollHeight, behavior: 'smooth' });
        }

        function addUserMessage(content) {
            if (!content || !content.trim()) return;
            hideEmptyState();
            currentAssistantDiv = null;
            currentAssistantMarkdown = '';
            const msgId = 'msg-' + (++messageIdCounter);
            const div = document.createElement('div');
            div.className = 'message user';
            div.id = msgId;
            div.innerHTML = `
                <div class="message-header">
                    <div class="role-badge">
                        <div class="role-icon">U</div>
                        <span class="role">User</span>
                    </div>
                    <button class="copy-msg-btn" onclick="copyMessageText('${msgId}')">Copy</button>
                </div>
                <div class="content">${escapeHtml(content)}</div>
            `;
            messageMarkdown.set(msgId, content);
            messagesContainer.appendChild(div);
        }

        function addAssistantMessage(content) {
            if (!content || !content.trim()) return;
            hideEmptyState();
            currentAssistantDiv = null;
            currentAssistantMarkdown = '';
            const msgId = 'msg-' + (++messageIdCounter);
            const div = document.createElement('div');
            div.className = 'message assistant';
            div.id = msgId;
            div.innerHTML = `
                <div class="message-header">
                    <div class="role-badge">
                        <div class="role-icon">A</div>
                        <span class="role">Assistant</span>
                    </div>
                    <div class="copy-dropdown">
                        <button class="copy-msg-btn" onclick="toggleCopyMsgMenu(this)">Copy ▾</button>
                        <div class="copy-dropdown-menu">
                            <button onclick="copyMessageText('${msgId}')">Copy as Text</button>
                            <button onclick="copyMessageMarkdown('${msgId}')">Copy as Markdown</button>
                        </div>
                    </div>
                </div>
                <div class="content">${renderMarkdown(content)}</div>
            `;
            messageMarkdown.set(msgId, content);
            messagesContainer.appendChild(div);
            addCodeCopyButtons(div);
        }

        function updateAssistantMessage(content, done) {
            if (!content || !content.trim()) {
                if (done && currentAssistantDiv) {
                    currentAssistantDiv.remove();
                    currentAssistantDiv = null;
                    currentAssistantMarkdown = '';
                }
                return;
            }
            hideEmptyState();

            if (!currentAssistantDiv) {
                const msgId = 'msg-' + (++messageIdCounter);
                currentAssistantDiv = document.createElement('div');
                currentAssistantDiv.className = 'message assistant' + (done ? '' : ' streaming');
                currentAssistantDiv.id = msgId;
                currentAssistantDiv.innerHTML = `
                    <div class="message-header">
                        <div class="role-badge">
                            <div class="role-icon">A</div>
                            <span class="role">Assistant</span>
                        </div>
                        <div class="copy-dropdown">
                            <button class="copy-msg-btn" onclick="toggleCopyMsgMenu(this)">Copy ▾</button>
                            <div class="copy-dropdown-menu">
                                <button onclick="copyMessageText('${msgId}')">Copy as Text</button>
                                <button onclick="copyMessageMarkdown('${msgId}')">Copy as Markdown</button>
                            </div>
                        </div>
                    </div>
                    <div class="content"></div>
                `;
                messagesContainer.appendChild(currentAssistantDiv);
            }

            currentAssistantMarkdown = content;
            const contentDiv = currentAssistantDiv.querySelector('.content');
            contentDiv.innerHTML = renderMarkdown(content);

            if (done) {
                currentAssistantDiv.classList.remove('streaming');
                addCodeCopyButtons(currentAssistantDiv);
                messageMarkdown.set(currentAssistantDiv.id, currentAssistantMarkdown);
                currentAssistantDiv = null;
                currentAssistantMarkdown = '';
            }
        }

        function renderMarkdown(text) {
            return marked.parse(text);
        }

        function escapeHtml(text) {
            const div = document.createElement('div');
            div.textContent = text;
            return div.innerHTML;
        }

        function addCodeCopyButtons(container) {
            container.querySelectorAll('pre code').forEach(code => {
                const pre = code.parentElement;
                if (!pre.querySelector('.copy-code-btn')) {
                    const btn = document.createElement('button');
                    btn.className = 'copy-code-btn';
                    btn.textContent = 'Copy';
                    btn.onclick = () => copyCode(code);
                    pre.style.position = 'relative';
                    pre.appendChild(btn);
                }
            });
        }

        function copyCode(codeElement) {
            navigator.clipboard.writeText(codeElement.textContent);
            showToast('Code copied!');
        }

        function copyMessageText(msgId) {
            const msg = document.getElementById(msgId);
            if (msg) {
                const content = msg.querySelector('.content');
                navigator.clipboard.writeText(content.innerText);
                showToast('Copied as text!');
            }
            closeCopyMenus();
        }

        function copyMessageMarkdown(msgId) {
            const md = messageMarkdown.get(msgId);
            if (md) {
                navigator.clipboard.writeText(md);
                showToast('Copied as markdown!');
            } else {
                copyMessageText(msgId);
            }
            closeCopyMenus();
        }

        function toggleCopyMenu(btn) {
            const menu = btn.nextElementSibling;
            closeCopyMenus();
            menu.classList.toggle('show');
        }

        function toggleCopyMsgMenu(btn) {
            const menu = btn.nextElementSibling;
            const wasOpen = menu.classList.contains('show');
            closeCopyMenus();
            if (!wasOpen) menu.classList.add('show');
        }

        function closeCopyMenus() {
            document.querySelectorAll('.copy-dropdown-menu.show').forEach(m => m.classList.remove('show'));
        }

        function copyAllText() {
            const msgs = messagesContainer.querySelectorAll('.message');
            if (msgs.length === 0) {
                showToast('No messages to copy');
                return;
            }
            let text = '';
            msgs.forEach(msg => {
                const role = msg.classList.contains('user') ? 'You' : 'Assistant';
                const content = msg.querySelector('.content').innerText;
                if (content.trim()) {
                    text += role + ':\\n' + content + '\\n\\n---\\n\\n';
                }
            });
            navigator.clipboard.writeText(text);
            showToast('Copied as text!');
            closeCopyMenus();
        }

        function copyAllMarkdown() {
            const msgs = messagesContainer.querySelectorAll('.message');
            if (msgs.length === 0) {
                showToast('No messages to copy');
                return;
            }
            let text = '';
            msgs.forEach(msg => {
                const role = msg.classList.contains('user') ? 'You' : 'Assistant';
                const md = messageMarkdown.get(msg.id) || msg.querySelector('.content').innerText;
                if (md.trim()) {
                    text += '**' + role + ':**\\n' + md + '\\n\\n---\\n\\n';
                }
            });
            navigator.clipboard.writeText(text);
            showToast('Copied as markdown!');
            closeCopyMenus();
        }

        // Search functionality
        function toggleSearch() {
            const searchBar = document.getElementById('search-bar');
            const visible = searchBar.classList.toggle('visible');
            if (visible) {
                document.getElementById('search-input').focus();
            } else {
                clearSearch();
            }
        }

        function closeSearch() {
            document.getElementById('search-bar').classList.remove('visible');
            clearSearch();
        }

        function clearSearch() {
            document.getElementById('search-input').value = '';
            searchMatches = [];
            currentMatchIndex = -1;
            document.getElementById('search-count').textContent = '0/0';
            // Remove all highlights
            document.querySelectorAll('.content mark').forEach(mark => {
                const parent = mark.parentNode;
                parent.replaceChild(document.createTextNode(mark.textContent), mark);
                parent.normalize();
            });
        }

        function doSearch() {
            clearSearchHighlights();
            const query = document.getElementById('search-input').value.toLowerCase();
            if (!query) {
                document.getElementById('search-count').textContent = '0/0';
                searchMatches = [];
                currentMatchIndex = -1;
                return;
            }

            searchMatches = [];
            document.querySelectorAll('.content').forEach(content => {
                highlightMatches(content, query);
            });

            document.getElementById('search-count').textContent = searchMatches.length > 0 ? '1/' + searchMatches.length : '0/0';
            if (searchMatches.length > 0) {
                currentMatchIndex = 0;
                searchMatches[0].classList.add('current');
                searchMatches[0].scrollIntoView({ behavior: 'smooth', block: 'center' });
            }
        }

        function clearSearchHighlights() {
            document.querySelectorAll('.content mark').forEach(mark => {
                const parent = mark.parentNode;
                parent.replaceChild(document.createTextNode(mark.textContent), mark);
                parent.normalize();
            });
            searchMatches = [];
            currentMatchIndex = -1;
        }

        function highlightMatches(element, query) {
            const walker = document.createTreeWalker(element, NodeFilter.SHOW_TEXT, null, false);
            const textNodes = [];
            while (walker.nextNode()) textNodes.push(walker.currentNode);

            textNodes.forEach(node => {
                const text = node.textContent;
                const lowerText = text.toLowerCase();
                let idx = lowerText.indexOf(query);
                if (idx === -1) return;

                const frag = document.createDocumentFragment();
                let lastIdx = 0;
                while (idx !== -1) {
                    frag.appendChild(document.createTextNode(text.substring(lastIdx, idx)));
                    const mark = document.createElement('mark');
                    mark.textContent = text.substring(idx, idx + query.length);
                    frag.appendChild(mark);
                    searchMatches.push(mark);
                    lastIdx = idx + query.length;
                    idx = lowerText.indexOf(query, lastIdx);
                }
                frag.appendChild(document.createTextNode(text.substring(lastIdx)));
                node.parentNode.replaceChild(frag, node);
            });
        }

        function searchPrev() {
            if (searchMatches.length === 0) return;
            searchMatches[currentMatchIndex].classList.remove('current');
            currentMatchIndex = (currentMatchIndex - 1 + searchMatches.length) % searchMatches.length;
            searchMatches[currentMatchIndex].classList.add('current');
            searchMatches[currentMatchIndex].scrollIntoView({ behavior: 'smooth', block: 'center' });
            document.getElementById('search-count').textContent = (currentMatchIndex + 1) + '/' + searchMatches.length;
        }

        function searchNext() {
            if (searchMatches.length === 0) return;
            searchMatches[currentMatchIndex].classList.remove('current');
            currentMatchIndex = (currentMatchIndex + 1) % searchMatches.length;
            searchMatches[currentMatchIndex].classList.add('current');
            searchMatches[currentMatchIndex].scrollIntoView({ behavior: 'smooth', block: 'center' });
            document.getElementById('search-count').textContent = (currentMatchIndex + 1) + '/' + searchMatches.length;
        }

        // Keyboard shortcuts
        document.addEventListener('keydown', (e) => {
            if (e.ctrlKey && e.key === 'f') {
                e.preventDefault();
                toggleSearch();
            }
            if (e.key === 'Escape') {
                closeSearch();
                closeCopyMenus();
            }
            if (e.key === 'Enter' && document.getElementById('search-bar').classList.contains('visible')) {
                e.shiftKey ? searchPrev() : searchNext();
            }
        });

        // Close dropdown when clicking outside
        document.addEventListener('click', (e) => {
            if (!e.target.closest('.copy-dropdown')) {
                closeCopyMenus();
            }
        });

        function showToast(message) {
            const toast = document.getElementById('toast');
            toast.textContent = message;
            toast.classList.add('show');
            setTimeout(() => toast.classList.remove('show'), 2000);
        }

        connect();
    </script>
</body>
</html>'''

    def _start_web_server(self):
        """Start the web companion server in a background thread."""
        if not WEB_AVAILABLE:
            self.console.print("[red]Web companion not available. Install: llm install fastapi uvicorn[/]")
            return False

        if self.web_server_thread and self.web_server_thread.is_alive():
            self.console.print(f"[yellow]Web server already running at http://localhost:{self.web_port}[/]")
            return True

        # Create FastAPI app
        app = FastAPI()
        session = self  # Closure reference

        @app.get("/", response_class=HTMLResponse)
        async def get_index():
            return session._get_web_html()

        @app.websocket("/ws")
        async def websocket_endpoint(websocket: WebSocket):
            await websocket.accept()
            session.web_clients.add(websocket)
            try:
                # Send session info (model, mode)
                await websocket.send_json({
                    "type": "session_info",
                    "model": session.model_name,
                    "mode": session.mode  # "agent" or "assistant"
                })

                # Send watch mode status
                await websocket.send_json({
                    "type": "watch_status",
                    "active": session.watch_mode,
                    "goal": session.watch_goal if session.watch_mode else None
                })

                # Send current token usage
                current_tokens = session.estimate_tokens()
                max_tokens = session.max_context_size
                pct = (current_tokens / max_tokens * 100) if max_tokens > 0 else 0
                await websocket.send_json({
                    "type": "token_update",
                    "current_tokens": current_tokens,
                    "max_tokens": max_tokens,
                    "percentage": pct
                })

                # Send conversation history on connect
                history = []
                for r in session.conversation.responses:
                    if hasattr(r, 'prompt') and r.prompt:
                        # Strip terminal context from prompts (same as DB storage)
                        clean_prompt = session._strip_context(r.prompt.prompt or "")
                        if clean_prompt and clean_prompt.strip():
                            history.append({"role": "user", "content": clean_prompt})
                    assistant_text = r.text()
                    if assistant_text and assistant_text.strip():
                        history.append({"role": "assistant", "content": assistant_text})
                await websocket.send_json({"type": "history", "messages": history})

                # Keep connection alive and handle requests
                while True:
                    try:
                        msg = await websocket.receive_text()
                        # Handle debug info request
                        if msg == "debug_request":
                            try:
                                debug_data = session._get_debug_info()
                                await websocket.send_json({"type": "debug_info", **debug_data})
                            except Exception as e:
                                await websocket.send_json({
                                    "type": "debug_info",
                                    "system_prompt": f"Error getting debug info: {e}",
                                    "tools": [],
                                    "messages": [],
                                    "model": "unknown",
                                    "mode": "unknown",
                                    "options": {},
                                    "max_context": 0,
                                    "current_tokens": 0,
                                    "loaded_kbs": [],
                                    "active_rag": None
                                })
                    except Exception:
                        break
            finally:
                session.web_clients.discard(websocket)

        # Configure uvicorn
        config = uvicorn.Config(
            app,
            host="127.0.0.1",
            port=self.web_port,
            log_level="warning",
            access_log=False
        )
        self.web_server = uvicorn.Server(config)

        # Run in background thread
        def run_server():
            import asyncio
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            # Store loop reference for cross-thread broadcasting
            session.web_event_loop = loop
            loop.run_until_complete(self.web_server.serve())

        self.web_server_thread = threading.Thread(target=run_server, daemon=True)
        self.web_server_thread.start()

        # Give server time to start and event loop to be set
        time.sleep(0.5)
        return True

    def _stop_web_server(self):
        """Stop the web companion server."""
        if self.web_server:
            self.web_server.should_exit = True
            self.web_server = None
            self.web_server_thread = None
            self.web_event_loop = None
            self.web_clients.clear()
            self.console.print("[green]✓[/] Web server stopped")
        else:
            self.console.print("[yellow]Web server is not running[/]")

    def _broadcast_to_web(self, message: dict):
        """Broadcast a message to all connected web clients.

        Thread-safe: schedules the broadcast on the web server's event loop.
        """
        if not self.web_clients or not self.web_event_loop:
            return

        import asyncio

        async def send_to_all():
            disconnected = set()
            # Iterate over a copy to avoid modification during iteration
            for client in list(self.web_clients):
                try:
                    await client.send_json(message)
                except Exception:
                    disconnected.add(client)
            # Remove disconnected clients
            for client in disconnected:
                self.web_clients.discard(client)

        # Schedule coroutine on the web server's event loop (thread-safe)
        try:
            asyncio.run_coroutine_threadsafe(send_to_all(), self.web_event_loop)
        except RuntimeError:
            # Event loop closed or not running
            pass

    def _broadcast_watch_status(self):
        """Broadcast current watch mode status to web clients."""
        self._broadcast_to_web({
            "type": "watch_status",
            "active": self.watch_mode,
            "goal": self.watch_goal if self.watch_mode else None
        })

    def _broadcast_token_update(self):
        """Broadcast current token usage to web clients."""
        current_tokens = self.estimate_tokens()
        max_tokens = self.max_context_size
        pct = (current_tokens / max_tokens * 100) if max_tokens > 0 else 0
        self._broadcast_to_web({
            "type": "token_update",
            "current_tokens": current_tokens,
            "max_tokens": max_tokens,
            "percentage": pct
        })

    def _broadcast_tool_call(self, tool_name: str, arguments: dict, result: str = None, status: str = "pending"):
        """Broadcast a tool call event to web clients."""
        self._broadcast_to_web({
            "type": "tool_call",
            "tool_name": tool_name,
            "arguments": arguments,
            "result": result,
            "status": status
        })

    def _estimate_tool_schema_tokens(self) -> int:
        """
        Estimate token count for all tool schemas.

        Uses char-based estimation (4 chars = 1 token) which is fast and
        consistent. The actual token count varies by model tokenizer, but
        this estimate is sufficient for context window tracking.

        Returns:
            Estimated token count for all tool schemas
        """
        try:
            # Build JSON representation of tool schemas (as sent to API)
            tool_schemas = []
            for tool in ASSISTANT_TOOLS:
                schema = {
                    'name': tool.name,
                    'description': tool.description or '',
                    'parameters': tool.schema.get('parameters', {}) if hasattr(tool, 'schema') else {}
                }
                tool_schemas.append(schema)

            tools_json = json.dumps(tool_schemas, indent=2)

            # Estimate tokens using char-based method (4 chars = 1 token)
            tokens = len(tools_json) // 4
            self._debug(f"Estimated tool schemas: {tokens} tokens ({len(ASSISTANT_TOOLS)} tools, {len(tools_json)} chars)")
            return tokens

        except Exception as e:
            # Fallback
            fallback = len(ASSISTANT_TOOLS) * 200
            self._debug(f"Tool schema measurement exception: {e}, using estimate: {fallback}")
            return fallback

    def _truncate_capture_if_needed(self, content: str, source_desc: str = "content") -> str:
        """
        Truncate captured content if it exceeds size limits.

        Adds warning message when truncation occurs.

        Args:
            content: Content to check/truncate
            source_desc: Description of content source (for warning message)

        Returns:
            Original or truncated content with warning
        """
        if not content:
            return content

        original_size = len(content)
        byte_size = len(content.encode('utf-8'))

        # Check if truncation needed
        needs_truncation = False
        truncation_reason = []

        if byte_size > self.MAX_CAPTURE_BYTES:
            needs_truncation = True
            truncation_reason.append(f"{byte_size:,} bytes > {self.MAX_CAPTURE_BYTES:,} byte limit")

        if original_size > self.MAX_CAPTURE_CHARS:
            needs_truncation = True
            truncation_reason.append(f"{original_size:,} chars > {self.MAX_CAPTURE_CHARS:,} char limit")

        if not needs_truncation:
            return content

        # Truncate to character limit (simpler than byte-aware truncation)
        truncated = content[:self.MAX_CAPTURE_CHARS]

        # Add warning header
        warning = (
            f"\n{'='*60}\n"
            f"WARNING: {source_desc.capitalize()} truncated\n"
            f"Reason: {', '.join(truncation_reason)}\n"
            f"Showing first {len(truncated):,} of {original_size:,} characters\n"
            f"{'='*60}\n\n"
        )

        result = warning + truncated

        self._debug(f"Truncated {source_desc}: {original_size:,} -> {len(truncated):,} chars")
        self.console.print(f"[yellow]⚠ {source_desc.capitalize()} truncated ({original_size:,} chars)[/]")

        return result

    def _build_system_prompt(self) -> str:
        """Build system prompt with injected date, platform info, and KB content."""
        from datetime import date

        # Get platform info
        os_info = detect_os()
        shell_name, shell_version = detect_shell()
        shell_info = f"{shell_name} {shell_version}".strip() if shell_version else shell_name
        env_type = detect_environment()  # 'native', 'wsl', 'gitbash', 'cygwin'

        # Build environment section
        env_section = f"""
<environment>
Today's date: {date.today().isoformat()}
Platform: {os_info}
Shell: {env_type} {shell_info}
</environment>
"""

        # Filter system prompt based on mode BEFORE building final prompt
        filtered_prompt = self._filter_mode_content(self.system_prompt)

        prompt = f"{filtered_prompt}\n{env_section}"

        # Append KB content if any loaded
        kb_content = self._get_loaded_kb_content()
        if kb_content:
            prompt = f"{prompt}\n# Knowledge Base\n\n{kb_content}"

        return prompt

    def _get_debug_info(self) -> dict:
        """Return debug information for the web companion."""
        # Build current system prompt (what would be sent on next request)
        current_system_prompt = self._build_system_prompt()

        # Build full conversation with ALL context (unstripped)
        messages = []
        for r in self.conversation.responses:
            if hasattr(r, 'prompt') and r.prompt:
                # Include FULL prompt (not stripped) - use .prompt property
                full_prompt = r.prompt.prompt or ""
                # Also capture the system prompt used for this turn
                turn_system = r.prompt.system or ""

                # Build attachments info
                attachments_info = []
                if r.prompt.attachments:
                    for att in r.prompt.attachments:
                        att_info = {"type": att.type or "unknown"}
                        if att.path:
                            att_info["path"] = att.path
                        elif att.url:
                            att_info["url"] = att.url
                        else:
                            att_info["inline"] = True
                        # Include content size if available
                        if att.content:
                            att_info["size"] = len(att.content)
                        attachments_info.append(att_info)

                messages.append({
                    "role": "user",
                    "content": full_prompt,
                    "system": turn_system,  # System prompt for this turn
                    "has_context": "<terminal_context>" in full_prompt,
                    "attachments": attachments_info,
                    "input_tokens": r.input_tokens,
                    "output_tokens": r.output_tokens
                })

                # Include tool results if present (tool outputs sent as input)
                if r.prompt.tool_results:
                    for tr in r.prompt.tool_results:
                        messages.append({
                            "role": "tool_result",
                            "tool_name": tr.name,
                            "content": tr.output or "",
                            "tool_call_id": tr.tool_call_id
                        })

            # Assistant response
            assistant_text = r.text()
            if assistant_text:
                messages.append({"role": "assistant", "content": assistant_text})

            # Tool calls made by assistant
            try:
                tool_calls = list(r.tool_calls())
                for tc in tool_calls:
                    messages.append({
                        "role": "tool_call",
                        "tool_name": tc.name,
                        "arguments": tc.arguments,
                        "tool_call_id": tc.tool_call_id
                    })
            except Exception:
                pass  # No tool calls or error accessing them

        # Build tool definitions (what's sent to the model)
        tools = []
        for tool in ASSISTANT_TOOLS:
            tool_info = {
                "name": tool.name,
                "description": tool.description or "",
            }
            # Get input schema - different attribute names depending on tool type
            if hasattr(tool, 'input_schema'):
                tool_info["parameters"] = tool.input_schema
            elif hasattr(tool, 'schema') and isinstance(tool.schema, dict):
                tool_info["parameters"] = tool.schema.get('parameters', {})
            else:
                tool_info["parameters"] = {}
            # Include plugin source if available
            if hasattr(tool, 'plugin') and tool.plugin:
                tool_info["plugin"] = tool.plugin
            tools.append(tool_info)

        # Get model options from most recent response (or empty if no responses yet)
        options = {}
        if self.conversation.responses:
            last_response = self.conversation.responses[-1]
            if hasattr(last_response, 'prompt') and last_response.prompt:
                try:
                    options = {
                        k: v for k, v in dict(last_response.prompt.options).items()
                        if v is not None
                    }
                except Exception:
                    pass

        return {
            "system_prompt": current_system_prompt,
            "tools": tools,
            "messages": messages,
            "model": self.model_name or "unknown",
            "mode": self.mode,
            "options": options,
            "max_context": self.max_context_size,
            "current_tokens": self.estimate_tokens(),
            "loaded_kbs": list(self.loaded_kbs.keys()) if self.loaded_kbs else [],
            "active_rag": self.active_rag_collection
        }

    def _filter_mode_content(self, prompt: str) -> str:
        """Filter system prompt based on current operating mode.

        Handles two tag types:
        - <agent_mode>...</agent_mode>: Included only in agent mode
        - <assistant_mode>...</assistant_mode>: Included only in assistant mode

        Tags must be at the start of a line (possibly with leading whitespace).
        Uses [ \\t]* (horizontal whitespace) instead of \\s* to preserve blank lines.
        """
        if self.mode == "agent":
            # Agent mode: remove <assistant_mode> blocks entirely, keep <agent_mode> content
            prompt = re.sub(
                r'^[ \t]*<assistant_mode>[ \t]*\n.*?^[ \t]*</assistant_mode>[ \t]*\n?',
                '', prompt, flags=re.MULTILINE | re.DOTALL
            )
            # Remove just the agent_mode tags, keep content
            prompt = re.sub(r'^[ \t]*<agent_mode>[ \t]*\n?', '', prompt, flags=re.MULTILINE)
            prompt = re.sub(r'^[ \t]*</agent_mode>[ \t]*\n?', '', prompt, flags=re.MULTILINE)
        else:
            # Assistant mode: remove <agent_mode> blocks entirely, keep <assistant_mode> content
            prompt = re.sub(
                r'^[ \t]*<agent_mode>[ \t]*\n.*?^[ \t]*</agent_mode>[ \t]*\n?',
                '', prompt, flags=re.MULTILINE | re.DOTALL
            )
            # Remove just the assistant_mode tags, keep content
            prompt = re.sub(r'^[ \t]*<assistant_mode>[ \t]*\n?', '', prompt, flags=re.MULTILINE)
            prompt = re.sub(r'^[ \t]*</assistant_mode>[ \t]*\n?', '', prompt, flags=re.MULTILINE)
        return prompt

    # =========================================================================
    # Knowledge Base System
    # =========================================================================

    def _get_kb_dir(self) -> Path:
        """Get or create KB directory in llm's config directory."""
        kb_dir = llm.user_dir() / "kb"
        kb_dir.mkdir(parents=True, exist_ok=True)
        return kb_dir

    def _get_config_file(self) -> Path:
        """Get config file path in llm's config directory."""
        return llm.user_dir() / "assistant-config.yaml"

    def _load_config(self) -> dict:
        """Load config.yaml if it exists."""
        config_file = self._get_config_file()
        if config_file.exists():
            try:
                import yaml
                with open(config_file) as f:
                    return yaml.safe_load(f) or {}
            except Exception:
                return {}
        return {}

    def _load_auto_kbs(self):
        """Load KBs listed in config.yaml auto_load."""
        config = self._load_config()
        auto_load = config.get("knowledge_base", {}).get("auto_load", [])
        for name in auto_load:
            self._load_kb(name, silent=True)

    def _load_kb(self, name: str, silent: bool = False) -> bool:
        """Load a KB file by name."""
        kb_dir = self._get_kb_dir()

        # Try with .md extension first
        kb_path = kb_dir / f"{name}.md"
        if not kb_path.exists():
            # Try without extension
            kb_path = kb_dir / name
            if not kb_path.exists():
                if not silent:
                    self.console.print(f"[red]KB not found: {name}[/]")
                    self.console.print(f"[dim]Looking in: {kb_dir}[/]")
                return False

        try:
            content = kb_path.read_text()
            self.loaded_kbs[name] = content
            if not silent:
                self.console.print(f"[green]✓[/] Loaded KB: {name} ({len(content)} chars)")
            return True
        except Exception as e:
            if not silent:
                self.console.print(f"[red]Failed to load {name}: {e}[/]")
            return False

    def _unload_kb(self, name: str) -> bool:
        """Unload a KB from session."""
        if name in self.loaded_kbs:
            del self.loaded_kbs[name]
            self.console.print(f"[green]✓[/] Unloaded KB: {name}")
            return True
        self.console.print(f"[yellow]KB not loaded: {name}[/]")
        return False

    def _get_loaded_kb_content(self) -> str:
        """Get combined content of all loaded KBs."""
        if not self.loaded_kbs:
            return ""
        parts = []
        for name, content in self.loaded_kbs.items():
            parts.append(f"## {name}\n\n{content}")
        return "\n\n---\n\n".join(parts)

    def _handle_kb_command(self, args: str) -> bool:
        """Handle /kb commands. Returns True to continue REPL."""
        parts = args.strip().split(maxsplit=1)

        if not parts or parts[0] == "":
            # /kb - list KBs
            self._list_kbs()
        elif parts[0] == "load" and len(parts) > 1:
            # /kb load <name> or /kb load name1,name2,name3
            names = [n.strip() for n in parts[1].split(",") if n.strip()]
            for name in names:
                self._load_kb(name)
        elif parts[0] == "unload" and len(parts) > 1:
            # /kb unload <name> or /kb unload name1,name2,name3
            names = [n.strip() for n in parts[1].split(",") if n.strip()]
            for name in names:
                self._unload_kb(name)
        elif parts[0] == "reload":
            # /kb reload - reload all loaded KBs
            names = list(self.loaded_kbs.keys())
            if names:
                for name in names:
                    self._load_kb(name)
            else:
                self.console.print("[yellow]No KBs loaded to reload[/]")
        else:
            self.console.print("[yellow]Usage: /kb [load|unload|reload] [name][/]")

        return True

    def _list_kbs(self):
        """List available and loaded KBs."""
        kb_dir = self._get_kb_dir()
        available = sorted([f.stem for f in kb_dir.glob("*.md")])

        self.console.print("\n[bold]Knowledge Bases[/]")

        if self.loaded_kbs:
            self.console.print("\n[green]Loaded:[/]")
            for name in sorted(self.loaded_kbs.keys()):
                chars = len(self.loaded_kbs[name])
                self.console.print(f"  • {name} ({chars} chars)")

        unloaded = [n for n in available if n not in self.loaded_kbs]
        if unloaded:
            self.console.print("\n[dim]Available:[/]")
            for name in unloaded:
                self.console.print(f"  • {name}")

        if not available and not self.loaded_kbs:
            self.console.print(f"\n[dim]No KBs found in {kb_dir}[/]")
            self.console.print("[dim]Create markdown files to use as knowledge bases.[/]")

    # =========================================================================
    # RAG Integration (llm-tools-rag)
    # =========================================================================

    def _rag_available(self) -> bool:
        """Check if llm-tools-rag is installed."""
        try:
            import llm_tools_rag
            return True
        except ImportError:
            return False

    def _handle_rag_command(self, args: str) -> bool:
        """Handle /rag commands. Returns True to continue REPL."""
        if not self._rag_available():
            self.console.print("[red]RAG not available. Install llm-tools-rag.[/]")
            self.console.print("[dim]Run install-llm-tools.sh or: llm install git+https://github.com/c0ffee0wl/llm-tools-rag[/]")
            return True

        parts = args.strip().split(maxsplit=1)

        if not parts or parts[0] == "":
            # /rag - list collections and show active
            self._rag_list_collections()
        elif parts[0] == "off":
            self.active_rag_collection = None
            self.pending_rag_context = None
            self.console.print("[green]✓[/] RAG deactivated")
        elif parts[0] == "status":
            self._rag_show_status()
        elif parts[0] == "search":
            # /rag search <collection> <query>
            search_args = parts[1] if len(parts) > 1 else ""
            search_parts = search_args.split(maxsplit=1)
            if len(search_parts) == 2:
                self._rag_oneshot_search(search_parts[0], search_parts[1])
            else:
                self.console.print("[red]Usage: /rag search <collection> <query>[/]")
        elif parts[0] == "top-k":
            # /rag top-k <n>
            try:
                self.rag_top_k = int(parts[1]) if len(parts) > 1 else 5
                self.console.print(f"[green]✓[/] RAG top-k set to {self.rag_top_k}")
            except ValueError:
                self.console.print("[red]Invalid top-k value[/]")
        elif parts[0] == "mode":
            # /rag mode <hybrid|vector|keyword>
            mode = parts[1].strip() if len(parts) > 1 else ""
            if mode in ("hybrid", "vector", "keyword"):
                self.rag_search_mode = mode
                self.console.print(f"[green]✓[/] RAG mode set to {mode}")
            else:
                self.console.print("[red]Invalid mode. Use: hybrid, vector, keyword[/]")
        elif parts[0] == "add":
            # /rag add <collection> <path>
            add_args = parts[1] if len(parts) > 1 else ""
            add_parts = add_args.split(maxsplit=1)
            if len(add_parts) == 2:
                self._rag_add_documents(add_parts[0], add_parts[1])
            else:
                self.console.print("[red]Usage: /rag add <collection> <path|git:url|glob>[/]")
        elif parts[0] == "rebuild":
            # /rag rebuild <collection>
            collection = parts[1].strip() if len(parts) > 1 else ""
            if collection:
                self._rag_rebuild_collection(collection)
            else:
                self.console.print("[red]Usage: /rag rebuild <collection>[/]")
        elif parts[0] == "delete":
            # /rag delete <collection>
            collection = parts[1].strip() if len(parts) > 1 else ""
            if collection:
                self._rag_delete_collection(collection)
            else:
                self.console.print("[red]Usage: /rag delete <collection>[/]")
        else:
            # Assume it's a collection name for activation: /rag <collection>
            collection = parts[0]
            self._rag_activate_collection(collection)

        return True

    def _rag_list_collections(self):
        """List available RAG collections."""
        from llm_tools_rag import get_collection_list

        collections = get_collection_list()

        self.console.print("\n[bold]RAG Collections[/]")

        if not collections:
            self.console.print("\n[dim]No RAG collections found[/]")
            self.console.print("[dim]Create with: /rag add <name> <path>[/]")
            return

        for coll in collections:
            name = coll['name']
            chunks = coll.get('chunks', '?')
            docs = coll.get('documents', '?')
            is_active = name == self.active_rag_collection
            active_marker = " [bold green](ACTIVE)[/]" if is_active else ""
            self.console.print(f"  • {name}: {chunks} chunks, {docs} docs{active_marker}")

        if self.active_rag_collection:
            self.console.print(f"\n[green]Active:[/] {self.active_rag_collection}")
            self.console.print(f"[dim]Top-k: {self.rag_top_k}, Mode: {self.rag_search_mode}[/]")
        else:
            self.console.print("\n[dim]RAG not active. Activate with: /rag <collection>[/]")

    def _rag_show_status(self):
        """Show current RAG status."""
        if self.active_rag_collection:
            try:
                from llm_tools_rag import get_collection_stats
                stats = get_collection_stats(self.active_rag_collection)
                self.console.print(f"[green]Active collection:[/] {self.active_rag_collection}")
                self.console.print(f"[dim]Chunks:[/] {stats['total_chunks']}")
                self.console.print(f"[dim]Documents:[/] {stats['unique_documents']}")
            except Exception:
                self.console.print(f"[green]Active collection:[/] {self.active_rag_collection}")
            self.console.print(f"[dim]Top-k:[/] {self.rag_top_k}")
            self.console.print(f"[dim]Mode:[/] {self.rag_search_mode}")
        else:
            self.console.print("[yellow]RAG not active[/]")
            self.console.print("[dim]Activate with: /rag <collection>[/]")

    def _rag_activate_collection(self, name: str):
        """Activate a RAG collection for persistent search."""
        from llm_tools_rag import collection_exists

        if not collection_exists(name):
            self.console.print(f"[red]Collection '{name}' not found[/]")
            self.console.print(f"[dim]Create with: /rag add {name} <documents>[/]")
            return

        self.active_rag_collection = name
        self.console.print(f"[green]✓[/] RAG activated: {name}")
        self.console.print("[dim]Retrieved context will be injected into every prompt[/]")

    def _rag_oneshot_search(self, collection: str, query: str):
        """One-shot RAG search without activating persistent mode."""
        from llm_tools_rag import collection_exists, search_collection

        if not collection_exists(collection):
            self.console.print(f"[red]Collection '{collection}' not found[/]")
            return

        with Spinner(f"Searching {collection}...", self.console):
            results = search_collection(collection, query, self.rag_top_k, self.rag_search_mode)

        if not results:
            self.console.print("[yellow]No results found[/]")
            return

        # Store for next prompt injection (one-shot mode)
        self.pending_rag_context = self._format_rag_results(results)
        self.console.print(f"[green]✓[/] Found {len(results)} results. Context will be injected into next prompt.")

        # Show preview
        for i, chunk in enumerate(results[:3], 1):
            source = chunk.get('metadata', {}).get('source', 'unknown')
            preview = chunk.get('content', '')[:100].replace('\n', ' ') + "..."
            self.console.print(f"[dim]{i}. {source}:[/] {preview}")

    def _rag_add_documents(self, collection: str, path: str):
        """Add documents to a RAG collection (creates if needed)."""
        from llm_tools_rag import add_to_collection, collection_exists

        is_new = not collection_exists(collection)
        action = "Creating" if is_new else "Adding to"

        self.console.print(f"[cyan]{action} collection '{collection}'...[/]")

        try:
            with Spinner(f"Processing {path}...", self.console):
                result = add_to_collection(collection, path)

            if result["status"] == "success":
                self.console.print(f"[green]✓[/] Added {result.get('chunks', '?')} chunks")
                # Auto-activate the collection
                self.active_rag_collection = collection
                self.console.print(f"[dim]Collection '{collection}' now active[/]")
            elif result["status"] == "skipped":
                self.console.print(f"[yellow]⊘[/] Skipped: {result.get('reason', 'already indexed')}")
            else:
                self.console.print(f"[red]✗[/] Error: {result.get('error', 'unknown')}")

        except Exception as e:
            self.console.print(f"[red]Error: {e}[/]")

    def _rag_rebuild_collection(self, collection: str):
        """Rebuild a RAG collection's index."""
        from llm_tools_rag import collection_exists, rebuild_collection_index

        if not collection_exists(collection):
            self.console.print(f"[red]Collection '{collection}' not found[/]")
            return

        try:
            with Spinner(f"Rebuilding {collection}...", self.console):
                rebuild_collection_index(collection)
            self.console.print(f"[green]✓[/] Rebuilt index for '{collection}'")
        except Exception as e:
            self.console.print(f"[red]Error rebuilding: {e}[/]")

    def _rag_delete_collection(self, collection: str):
        """Delete a RAG collection."""
        from llm_tools_rag import collection_exists, remove_collection

        if not collection_exists(collection):
            self.console.print(f"[red]Collection '{collection}' not found[/]")
            return

        # Confirm deletion
        self.console.print(f"[yellow]Delete collection '{collection}'? (y/N)[/]")
        confirm = input().strip().lower()
        if confirm != 'y':
            self.console.print("[dim]Cancelled[/]")
            return

        try:
            remove_collection(collection)
            self.console.print(f"[green]✓[/] Deleted collection '{collection}'")

            # Deactivate if was active
            if self.active_rag_collection == collection:
                self.active_rag_collection = None
                self.console.print("[dim]RAG deactivated[/]")

        except Exception as e:
            self.console.print(f"[red]Error deleting: {e}[/]")

    def _retrieve_rag_context(self, query: str) -> str:
        """Retrieve and format RAG context for query."""
        if not self.active_rag_collection:
            return ""

        try:
            from llm_tools_rag import search_collection
            results = search_collection(
                self.active_rag_collection,
                query,
                top_k=self.rag_top_k,
                mode=self.rag_search_mode
            )

            if not results:
                return ""

            return self._format_rag_results(results)
        except Exception as e:
            self._debug(f"RAG retrieval error: {e}")
            return ""

    def _format_rag_results(self, results: list) -> str:
        """Format retrieved chunks for context injection."""
        if not results:
            return ""

        parts = ["<retrieved_documents>"]
        for i, r in enumerate(results, 1):
            source = r.get('metadata', {}).get('source', 'unknown')
            content = r.get('content', '')
            parts.append(f"\n[{i}. {source}]\n{content}")
        parts.append("\n</retrieved_documents>")

        return "\n".join(parts)

    def _normalize_uuid(self, uuid_value) -> Optional[str]:
        """Normalize UUID to Python string (ensures dbus.String -> str conversion)"""
        if uuid_value is None or uuid_value == '':
            return None
        # Explicitly convert to Python str (handles dbus.String)
        return str(uuid_value)

    def _reconnect_dbus(self) -> bool:
        """Attempt to reconnect to Terminator D-Bus with timeout.

        Uses SIGALRM for timeout protection. On timeout or error, ensures
        D-Bus state is cleaned up to prevent stale connections.
        """
        # Use SIGALRM for timeout (safe at startup, before asyncio event loop)
        def timeout_handler(signum, frame):
            raise TimeoutError("D-Bus connection timed out")

        old_handler = signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(10)  # 10 second timeout
        bus = None

        try:
            bus = dbus.SessionBus()

            # Discover actual Terminator service name (includes UUID suffix)
            # Check for multiple instances
            terminator_services = [
                name for name in bus.list_names()
                if name.startswith('net.tenshu.Terminator2') and not name.endswith('.Assistant')
            ]

            if len(terminator_services) > 1:
                # Multiple Terminator instances - try to pick the right one
                # Check if TERMINATOR_UUID environment variable can help identify our instance
                env_uuid = os.environ.get('TERMINATOR_UUID', '')
                matching_service = None
                for service in terminator_services:
                    if env_uuid and env_uuid in service:
                        matching_service = service
                        break
                if matching_service:
                    self._debug(f"Matched Terminator instance via TERMINATOR_UUID: {matching_service}")
                    service_name = matching_service
                else:
                    # Can't determine - use first and warn
                    self.console.print(f"[yellow]Multiple Terminator instances detected ({len(terminator_services)})[/]")
                    self.console.print(f"[yellow]Using first found: {terminator_services[0]}[/]")
                    service_name = terminator_services[0]
            else:
                service_name = terminator_services[0] if terminator_services else None

            if not service_name:
                service_name = 'net.tenshu.Terminator2'  # Fallback for older versions

            self._debug(f"Connected to Terminator D-Bus: {service_name}")
            self.dbus_service = bus.get_object(service_name, '/net/tenshu/Terminator2')
            return True
        except TimeoutError:
            self.console.print("[red]D-Bus connection timed out (10s)[/]")
            self.console.print("[yellow]Terminator may not be running or D-Bus is unresponsive[/]")
            # Clean up any partial state
            self.dbus_service = None
            return False
        except dbus.exceptions.DBusException as e:
            self.console.print(f"[red]D-Bus reconnection failed: {e}[/]")
            self.dbus_service = None
            return False
        except Exception as e:
            self.console.print(f"[red]D-Bus reconnection error ({type(e).__name__}): {e}[/]")
            self.dbus_service = None
            return False
        finally:
            signal.alarm(0)  # Cancel alarm
            signal.signal(signal.SIGALRM, old_handler)  # Restore handler

    def _check_dbus_connection(self) -> bool:
        """Verify D-Bus is still connected"""
        try:
            # Try a simple D-Bus operation
            self.dbus_service.get_terminals()
            return True
        except Exception:
            return False

    def _connect_to_terminator(self):
        """Connect to Terminator and plugin via D-Bus"""
        # Connect to Terminator's main D-Bus service for terminal management
        if not self._reconnect_dbus():
            self.console.print("[red]Error: Could not connect to Terminator D-Bus service[/]")
            self.console.print("Ensure Terminator is running with D-Bus enabled")
            sys.exit(1)

        # Connect to plugin's D-Bus service for terminal content/commands
        if not self._connect_to_plugin_dbus():
            self.console.print("[red]Error: Plugin D-Bus service not available[/]")
            self.console.print("Ensure TerminatorAssistant plugin is:")
            self.console.print("  1. Installed in ~/.config/terminator/plugins/")
            self.console.print("  2. Enabled in Terminator Preferences > Plugins")
            self.console.print("  3. Terminator has been restarted after enabling")
            sys.exit(1)

    def _connect_to_plugin_dbus(self) -> bool:
        """Connect to plugin's D-Bus service"""
        try:
            bus = dbus.SessionBus()
            self.plugin_dbus = bus.get_object(
                'net.tenshu.Terminator2.Assistant',
                '/net/tenshu/Terminator2/Assistant'
            )

            # Log plugin version for diagnostics (debug only)
            try:
                version = self.plugin_dbus.get_plugin_version()
                self._debug(f"Plugin version: {version}")
            except Exception:
                pass  # Ignore version check failures

            return True
        except dbus.exceptions.DBusException as e:
            # Provide specific guidance based on the error
            error_name = e.get_dbus_name() if hasattr(e, 'get_dbus_name') else str(e)
            if 'ServiceUnknown' in str(error_name) or 'ServiceUnknown' in str(e):
                self.console.print("[yellow]Plugin D-Bus service not registered[/]")
                self.console.print("[dim]The TerminatorAssistant plugin is not running.[/]")
            elif 'NoReply' in str(error_name):
                self.console.print("[yellow]Plugin D-Bus service not responding[/]")
            else:
                self.console.print(f"[yellow]Plugin D-Bus error: {e}[/]")
            return False
        except Exception as e:
            # Check if D-Bus session bus itself is unavailable
            if 'DBUS_SESSION_BUS_ADDRESS' not in os.environ:
                self.console.print("[yellow]D-Bus session bus not available[/]")
                self.console.print("[dim]Try: export $(dbus-launch)[/]")
            else:
                self.console.print(f"[yellow]Plugin D-Bus connection failed: {e}[/]")
            return False

    def _check_plugin_available(self) -> bool:
        """Verify plugin D-Bus service is available"""
        try:
            # Try a simple D-Bus call to check if service is alive
            self.plugin_dbus.get_focused_terminal_uuid()
            return True
        except Exception:
            return False

    def _reconnect_plugin(self) -> bool:
        """Attempt to reconnect to plugin D-Bus service"""
        return self._connect_to_plugin_dbus()

    def setup_terminals(self):
        """Auto-create Exec terminal with retry logic"""
        self.console.print("[cyan]Setting up terminals...[/]")

        max_retries = 3
        for attempt in range(max_retries):
            try:
                # Check D-Bus connection
                if not self._check_dbus_connection():
                    if not self._reconnect_dbus():
                        raise Exception("D-Bus reconnection failed")

                # Use early_terminal_uuid captured at startup to avoid race condition
                # where user switches tabs before setup_terminals() is called
                self.chat_terminal_uuid = self.early_terminal_uuid

                # Check for existing Exec pane or offer to reuse single other pane
                try:
                    terminals = self.plugin_dbus.get_terminals_in_same_tab(self.chat_terminal_uuid)

                    # Filter out chat terminal (already normalized at assignment)
                    other_terminals = [t for t in terminals if str(t['uuid']) != self.chat_terminal_uuid]

                    # First: Look for existing Assistant Exec pane
                    for t in other_terminals:
                        title = t.get('title', '')
                        if title.startswith('Assistant: Exec'):
                            pid = self._extract_pid_from_title(title)
                            # Reuse if: no PID in title (legacy), PID matches current process,
                            # or PID is from a dead process (previous session)
                            pid_is_dead = pid is not None and pid != os.getpid() and not self._process_exists(pid)
                            if pid is None or pid == os.getpid() or pid_is_dead:
                                self.exec_terminal_uuid = self._normalize_uuid(t['uuid'])
                                self._release_instance_lock()  # Release global lock
                                self.console.print("[green]✓[/] Terminals ready")
                                return  # Success - reusing existing exec terminal
                            # PID belongs to another active assistant - don't reuse

                    # Second: If exactly one other pane, offer to use it
                    if len(other_terminals) == 1:
                        existing_pane = other_terminals[0]
                        pane_title = existing_pane.get('title', 'Untitled')
                        use_existing = Confirm.ask(f"Use '{pane_title}' as Exec pane?", default=True)

                        if use_existing:
                            self.exec_terminal_uuid = self._normalize_uuid(existing_pane['uuid'])
                            self._release_instance_lock()  # Release global lock
                            self.console.print("[green]✓[/] Terminals ready")
                            return  # Success - using existing terminal
                except dbus.exceptions.DBusException as e:
                    # D-Bus specific errors (method not found, connection issues, etc.)
                    error_msg = str(e)
                    if 'Unknown method' in error_msg or 'does not exist' in error_msg:
                        self.console.print("[red]ERROR: Plugin method 'get_terminals_in_same_tab' not found![/]")
                        self.console.print("[red]Please restart Terminator to load the updated plugin.[/red]")
                    else:
                        self.console.print(f"[red]D-Bus error enumerating terminals: {e}[/]")
                    self.console.print("[yellow]Creating new Exec terminal as fallback...[/]")
                except Exception as e:
                    # Other unexpected errors
                    self.console.print(f"[red]Unexpected error ({type(e).__name__}): {e}[/]")
                    self.console.print("[yellow]Creating new Exec terminal as fallback...[/]")

                # Split vertically to create Exec terminal (to the right)
                exec_uuid = self.dbus_service.vsplit(
                    self.chat_terminal_uuid,
                    dbus.Dictionary({
                        'title': f'Assistant: Exec (PID {os.getpid()})'
                    }, signature='ss')
                )
                if str(exec_uuid).startswith('ERROR'):
                    raise Exception(f"Failed to split terminal: {exec_uuid}")
                self.exec_terminal_uuid = self._normalize_uuid(exec_uuid)

                self._release_instance_lock()  # Release global lock
                self.console.print("[green]✓[/] Terminals ready")
                return  # Success

            except Exception as e:
                if attempt < max_retries - 1:
                    self.console.print(f"[yellow]Retry {attempt+1}/{max_retries}: {e}[/]")
                    time.sleep(1)
                else:
                    self.console.print(f"[red]Failed to setup terminals after {max_retries} attempts: {e}[/]")
                    sys.exit(1)

    def _verify_exec_terminal(self) -> bool:
        """Check if exec terminal still exists"""
        try:
            terminals = self.plugin_dbus.get_all_terminals_metadata()

            # Debug output for terminal verification
            self._debug(f"Looking for exec UUID: {repr(self.exec_terminal_uuid)} (type: {type(self.exec_terminal_uuid).__name__})")
            self._debug(f"Plugin returned {len(terminals)} terminals")
            for t in terminals:
                self._debug(f"  - {repr(t['uuid'])} (type: {type(t['uuid']).__name__}) | Title: {t.get('title', 'N/A')}")
                if t['uuid'] == self.exec_terminal_uuid:
                    self._debug("    ✓ EXACT MATCH!")

            # exec_terminal_uuid is already normalized at assignment
            return any(str(t['uuid']) == self.exec_terminal_uuid for t in terminals)
        except Exception as e:
            self._debug(f"Verification error: {e}")
            return False

    def _recreate_exec_terminal(self) -> bool:
        """Recreate exec terminal if closed"""
        try:
            self.console.print("[yellow]Recreating Exec terminal...[/]")

            # Create new exec terminal by splitting from chat terminal
            # Use chat_terminal_uuid (stable) instead of get_focused_terminal()
            exec_uuid = self.dbus_service.vsplit(
                self.chat_terminal_uuid,
                dbus.Dictionary({'title': f'Assistant: Exec (Restored PID {os.getpid()})'}, signature='ss')
            )
            if str(exec_uuid).startswith('ERROR'):
                raise Exception(f"Failed to split terminal: {exec_uuid}")

            self.exec_terminal_uuid = self._normalize_uuid(exec_uuid)

            # Clear plugin cache to avoid stale data from old terminal
            try:
                self.plugin_dbus.clear_cache()
                self._debug("Plugin cache cleared after exec terminal recreation")
            except Exception as e:
                self._debug(f"Could not clear plugin cache: {e}")

            # Wait for shell prompt to render (prevents false TUI detection)
            # New terminals have minimal scrollback which triggers TUI heuristic
            max_wait = 2.0
            poll_interval = 0.1
            start_time = time.time()
            while time.time() - start_time < max_wait:
                try:
                    content = self.plugin_dbus.capture_terminal_content(
                        self.exec_terminal_uuid, -1
                    )
                    if content and PromptDetector.detect_prompt_at_end(content):
                        self._debug(f"Shell prompt detected after {time.time() - start_time:.2f}s")
                        break
                except Exception:
                    pass
                time.sleep(poll_interval)
            else:
                self._debug("Shell prompt wait timed out (continuing anyway)")

            self.console.print(f"[green]✓[/] Exec terminal restored: {exec_uuid[:8]}...")
            return True

        except Exception as e:
            self.console.print(f"[red]Failed to recreate exec terminal: {e}[/]")
            return False

    def _capture_screenshot(self, terminal_uuid: str, unique_id: str = None) -> Tuple[Optional[str], Optional[str]]:
        """
        Capture terminal screenshot and save to temp file.

        Args:
            terminal_uuid: UUID of terminal to capture
            unique_id: Optional unique identifier for filename (default: timestamp)

        Returns:
            Tuple of (temp_file_path, error_message). One will be None.
        """
        try:
            screenshot_data = self.plugin_dbus.capture_terminal_screenshot(terminal_uuid)

            if not screenshot_data or screenshot_data.startswith('ERROR'):
                error_msg = screenshot_data if screenshot_data else "No data returned"
                return None, error_msg

            image_bytes = base64.b64decode(screenshot_data)

            # Use mkstemp for atomic, secure temp file creation in dedicated directory
            temp_fd, temp_path = tempfile.mkstemp(
                suffix='.png',
                prefix='assistant_screenshot_',
                dir=str(self.screenshot_dir)
            )

            # Write image data
            with os.fdopen(temp_fd, 'wb') as f:
                f.write(image_bytes)

            self.screenshot_files.append(temp_path)
            return temp_path, None

        except Exception as e:
            return None, str(e)

    def wait_for_tui_render(self, terminal_uuid, max_wait=2.0, initial_content=None) -> bool:
        """
        Wait for TUI application to finish rendering by detecting content stability.

        First waits for content to CHANGE from initial state (TUI starting),
        then waits for stability (TUI finished rendering).

        Args:
            terminal_uuid: Terminal to monitor
            max_wait: Maximum wait time in seconds (default: 2.0)
            initial_content: Terminal content before command was sent (for change detection)

        Returns:
            True if content stabilized, False if timed out
        """
        start_time = time.time()
        poll_interval = 0.15
        previous_content = None
        stable_count = 0
        content_changed = initial_content is None  # Skip change detection if no initial

        while time.time() - start_time < max_wait:
            try:
                current_content = self.plugin_dbus.capture_terminal_content(terminal_uuid, -1)

                # First, wait for content to change from initial state
                if not content_changed:
                    if current_content != initial_content:
                        content_changed = True
                        self._debug(f"TUI content changed after {time.time() - start_time:.2f}s")
                        previous_content = current_content
                    time.sleep(poll_interval)
                    continue

                # Then check for stability
                if current_content == previous_content:
                    stable_count += 1
                    if stable_count >= 2:  # Stable for 2 consecutive polls
                        self._debug(f"TUI render stabilized after {time.time() - start_time:.2f}s")
                        return True
                else:
                    stable_count = 0
                    previous_content = current_content

                time.sleep(poll_interval)
            except Exception as e:
                self._debug(f"TUI render wait error: {e}")
                time.sleep(poll_interval)

        self._debug(f"TUI render wait timed out after {max_wait}s")
        return False  # Timeout - proceed anyway

    def prompt_based_capture(self, terminal_uuid, max_wait=60, initial_content=None) -> Tuple[bool, str]:
        """
        Capture terminal content using prompt detection instead of stability checks.

        Polls terminal and stops when content has changed from initial state AND
        a shell prompt is detected at the end. This prevents false positives from
        the old prompt that was visible before the command started.

        Falls back to timeout for long-running commands or TUI applications.

        This is ideal for:
        - Command-line tools that return to prompt quickly
        - Exec terminal command execution
        - Cases where prompt detection is more reliable than stability

        Args:
            terminal_uuid: Terminal to capture
            max_wait: Maximum wait time in seconds (default: 60)
            initial_content: Terminal content before command was sent (for change detection)

        Returns:
            Tuple of (prompt_detected, content)
        """
        # Configuration
        initial_delay = 0.3
        poll_interval = 0.5
        max_attempts = int(max_wait / poll_interval)

        # Initial delay for command to start
        time.sleep(initial_delay)

        content_changed = initial_content is None  # If no initial content, skip change detection
        content = ""  # Initialize to avoid NameError if loop doesn't execute

        # Use Rich Status for visual feedback during polling
        with Status("[cyan]Waiting for output (0.3s)[/]", console=self.console, spinner="dots", spinner_style="cyan") as status:
            for attempt in range(max_attempts):
                try:
                    content = self.plugin_dbus.capture_terminal_content(terminal_uuid, -1)

                    # First, check if content has changed from initial state
                    if not content_changed:
                        if content != initial_content:
                            content_changed = True
                            self._debug("Content changed from initial state")

                    # Only check for prompt after content has changed
                    if content_changed and content and PromptDetector.detect_prompt_at_end(content):
                        return (True, content)

                    # Visual feedback with dynamic status
                    elapsed = initial_delay + (attempt * poll_interval)
                    status_msg = "Waiting for output" if not content_changed else "Waiting for prompt"
                    status.update(f"[cyan]{status_msg} ({elapsed:.1f}s)[/]")

                    time.sleep(poll_interval)

                except dbus.exceptions.DBusException as e:
                    self.console.print(f"[red]Plugin D-Bus error during capture: {e}[/]")
                    return (False, "")
                except Exception as e:
                    self.console.print(f"[red]Prompt-based capture error ({type(e).__name__}): {e}[/]")
                    return (False, "")

        # Timeout - return last content
        return (False, content if content else "")

    def _capture_last_command_output(self, terminal_uuid: str) -> str:
        """
        Capture recent commands' output intelligently using dynamic extension.
        Starts with viewport-sized capture, expands if needed to find command boundaries.

        This ensures that even if the user manually runs a command with output
        exceeding the viewport, the full output is captured (similar to tool execution).

        Captures the last MAX_RECENT_COMMANDS commands to provide context when
        user runs multiple commands between assistant interactions.

        Args:
            terminal_uuid: Terminal UUID to capture from

        Returns:
            String containing recent commands' prompts + outputs
        """
        INITIAL_LINES = 50      # Start with viewport-ish size
        MAX_LINES = 5000        # Hard limit on capture range
        MAX_RECENT_COMMANDS = 3 # Number of recent commands to capture

        try:
            _, cursor_row = self.plugin_dbus.get_cursor_position(terminal_uuid)
        except Exception:
            # Fallback to viewport capture
            return self.plugin_dbus.capture_terminal_content(terminal_uuid, -1)

        lines_to_capture = INITIAL_LINES

        while lines_to_capture <= MAX_LINES:
            start_row = max(0, cursor_row - lines_to_capture)

            try:
                content = self.plugin_dbus.capture_from_row(terminal_uuid, start_row)
            except Exception:
                return self.plugin_dbus.capture_terminal_content(terminal_uuid, -1)

            if not content or content.startswith('ERROR'):
                return self.plugin_dbus.capture_terminal_content(terminal_uuid, -1)

            # Find prompt lines to identify command boundaries
            prompts = PromptDetector.find_all_prompts(content)

            # Check if we should return or continue expanding:
            # - Need N+1 prompts for N commands (e.g., 4 prompts for 3 commands)
            # - OR we've reached start of scrollback (can't get more history)
            have_enough_prompts = len(prompts) >= MAX_RECENT_COMMANDS + 1
            at_start_of_scrollback = start_row == 0

            if have_enough_prompts or at_start_of_scrollback:
                # Either we have enough commands, or we've captured all available history
                if len(prompts) >= 2:
                    # At least one complete command - extract last N
                    start_idx = max(0, len(prompts) - (MAX_RECENT_COMMANDS + 1))
                    start_line = prompts[start_idx][0]
                    lines = content.split('\n')
                    return '\n'.join(lines[start_line:])
                elif prompts:
                    lines = content.split('\n')
                    # Check if shell is waiting for input (prompt at end)
                    # If so, the single prompt is the END marker - return content BEFORE it
                    if PromptDetector.detect_prompt_at_end(content):
                        return '\n'.join(lines[:prompts[0][0]]).rstrip()
                    else:
                        # Prompt at start or command running - return from prompt
                        return '\n'.join(lines[prompts[0][0]:])
                return content

            # Not enough prompts and not at start - keep expanding
            lines_to_capture *= 2

        # Hit max - capture last MAX_LINES rows (not entire scrollback)
        fallback_start = max(0, cursor_row - MAX_LINES)
        content = self.plugin_dbus.capture_from_row(terminal_uuid, fallback_start)
        prompts = PromptDetector.find_all_prompts(content)
        if len(prompts) >= 2:
            # Extract last N commands (same logic as main loop)
            start_idx = max(0, len(prompts) - (MAX_RECENT_COMMANDS + 1))
            lines = content.split('\n')
            return '\n'.join(lines[prompts[start_idx][0]:])
        elif prompts:
            # Single prompt - check if it's at the end (scrollback exceeded)
            lines = content.split('\n')
            if PromptDetector.detect_prompt_at_end(content):
                # Prompt at end = return content BEFORE it
                return '\n'.join(lines[:prompts[0][0]]).rstrip()
            else:
                # Prompt at start or command running - return from prompt
                return '\n'.join(lines[prompts[0][0]:])
        return content

    def capture_context(self, include_exec_output=False, dedupe_unchanged=False) -> Tuple[str, List[llm.Attachment]]:
        """
        Capture visible content from all terminals (like tmuxai).

        Excludes:
        - Chat terminal (where assistant is running) - to avoid self-reference
        - Exec terminal (unless include_exec_output=True)

        For TUI applications (htop, glances, vim, etc.), captures screenshots
        instead of text since TUI content doesn't extract well as plain text.

        Args:
            include_exec_output: Whether to include exec terminal in context
            dedupe_unchanged: If True and content matches previous hash for a terminal,
                             emit [Content unchanged] placeholder instead of full content

        Returns:
            Tuple of (context_string, attachments_list):
            - context_string: XML-wrapped text content for non-TUI terminals
            - attachments_list: Screenshot attachments for TUI terminals
        """
        import base64

        try:
            terminals = self.plugin_dbus.get_terminals_in_same_tab(self.chat_terminal_uuid)
            context_parts = []
            attachments = []

            for term in terminals:
                term_uuid = self._normalize_uuid(term['uuid'])

                # Skip chat terminal (self-awareness)
                if term_uuid == self.chat_terminal_uuid:
                    continue

                # Optionally skip exec terminal
                if not include_exec_output and term_uuid == self.exec_terminal_uuid:
                    continue

                # Check if TUI is active in this terminal
                is_tui = False
                try:
                    is_tui = self.plugin_dbus.is_likely_tui_active(term['uuid'])
                except Exception:
                    pass  # Fall back to text capture if TUI detection fails

                if is_tui:
                    # Capture screenshot for TUI terminal
                    self._debug(f"TUI detected in terminal {term['title']}, capturing screenshot")
                    try:
                        screenshot_data = self.plugin_dbus.capture_terminal_screenshot(term['uuid'])
                        if screenshot_data and not screenshot_data.startswith('ERROR'):
                            # Save to temp file and create attachment
                            image_bytes = base64.b64decode(screenshot_data)
                            # Use mkstemp for atomic, secure temp file creation in dedicated directory
                            temp_fd, temp_path = tempfile.mkstemp(
                                suffix='.png',
                                prefix=f'assistant_ctx_{term_uuid[:8]}_',
                                dir=str(self.screenshot_dir)
                            )
                            with os.fdopen(temp_fd, 'wb') as f:
                                f.write(image_bytes)
                            self.screenshot_files.append(temp_path)
                            attachments.append(llm.Attachment(path=temp_path))
                            # Add marker in context so AI knows about the screenshot
                            # TUI screenshots always have unique timestamps, so no deduplication
                            context_parts.append(f'<terminal uuid="{term_uuid}" title="{term["title"]}" type="tui-screenshot">Screenshot attached for this TUI terminal</terminal>')
                            continue  # Don't fall through to text capture
                        else:
                            self._debug(f"Screenshot failed for {term['title']}: {screenshot_data}")
                    except Exception as e:
                        self._debug(f"TUI screenshot failed for {term['title']}: {e}")
                    # Fall through to text capture if screenshot fails

                # Intelligent capture: get full last command output (not just viewport)
                content = self._capture_last_command_output(term['uuid'])

                if content and not content.startswith('ERROR'):
                    # Compute hash for change detection (normalize whitespace for stability)
                    content_hash = hashlib.sha256(content.strip().encode()).hexdigest()

                    # Per-terminal deduplication: emit placeholder if content unchanged
                    # Only active after first message (AI needs initial context)
                    if dedupe_unchanged and term_uuid in self.terminal_content_hashes:
                        if self.terminal_content_hashes[term_uuid] == content_hash:
                            # Content unchanged - emit placeholder
                            context_parts.append(f'''<terminal uuid="{term['uuid']}" title="{term['title']}" cwd="{term['cwd']}">
[Content unchanged]
</terminal>''')
                            continue

                    # Always store hash for future comparison (even on first message)
                    self.terminal_content_hashes[term_uuid] = content_hash

                    # Format like tmux-fragments
                    context_parts.append(f'''<terminal uuid="{term['uuid']}" title="{term['title']}" cwd="{term['cwd']}">
{content}
</terminal>''')

            combined_context = "\n\n".join(context_parts)
            # Truncate if needed to prevent memory/token issues
            combined_context = self._truncate_capture_if_needed(combined_context, "terminal context")
            return combined_context, attachments
        except Exception as e:
            self.console.print(f"[red]Error capturing context: {e}[/]")
            return "", []

    def estimate_tokens(self, with_source: bool = False):
        """Estimate current context window size in tokens.

        Returns the actual tokens that would be sent on the next API call:
        - Uses the last response's input_tokens + output_tokens (accurate from API)
        - Falls back to char-based estimation if API tokens unavailable

        Note: input_tokens from API is cumulative (includes full conversation history),
        so the last response's tokens represent the current context window size.

        Args:
            with_source: If True, returns tuple (tokens, source) where source is
                        "API" or "estimated". If False, returns just the token count.
        """
        source = "estimated"
        tokens = 0

        try:
            # Use filtered system prompt length (reflects current mode)
            filtered_prompt_len = len(self._filter_mode_content(self.system_prompt))

            if not self.conversation.responses:
                # No responses yet - estimate system prompt + tools
                # System prompt chars / 4, plus measured tool overhead
                base_tokens = filtered_prompt_len // 4
                tool_tokens = self._tool_token_overhead  # Already measured in __init__
                tokens = base_tokens + tool_tokens
                source = "estimated"
            else:
                # Use the LAST response's tokens (represents current context window)
                last_response = self.conversation.responses[-1]

                if last_response.input_tokens is not None:
                    # API provided accurate token counts
                    source = "API"
                    # Use input_tokens only - output tokens are already included in next request's input
                    # When the next request is made, its input_tokens will include the previous output
                    # So we only need input_tokens to know how much context we're using
                    tokens = last_response.input_tokens
                else:
                    # Fallback: char-based estimation for current context
                    # The conversation is cumulative - each response already contains history
                    # So we only count the LAST exchange, not sum all of them (which would overestimate)
                    source = "estimated"
                    total_chars = filtered_prompt_len
                    if self.conversation.responses:
                        last_resp = self.conversation.responses[-1]
                        if hasattr(last_resp, 'prompt') and last_resp.prompt:
                            if last_resp.prompt.prompt:
                                total_chars += len(last_resp.prompt.prompt)
                        total_chars += len(last_resp.text())
                    # Add measured tool overhead
                    tool_tokens = self._tool_token_overhead  # Use measured value
                    tokens = (total_chars // 4) + tool_tokens

        except Exception as e:
            self.console.print(f"[yellow]Warning: Token estimation failed: {e}[/]")
            # Ultimate fallback
            tokens = len(self.system_prompt) // 4 + len(self.conversation.responses) * 500
            source = "estimated"

        return (tokens, source) if with_source else tokens

    def check_and_squash_context(self):
        """Auto-squash when context reaches threshold (like tmuxai)"""
        current_tokens = self.estimate_tokens()

        if current_tokens >= self.max_context_size * self.context_squash_threshold:
            self.console.print("[yellow]Context approaching limit, auto-squashing...[/]")

            # Record pre-squash tokens
            pre_squash_tokens = current_tokens

            self.squash_context()

            # Validate squashing reduced tokens
            post_squash_tokens = self.estimate_tokens()
            tokens_saved = pre_squash_tokens - post_squash_tokens

            if tokens_saved > 0:
                self.console.print(
                    f"[green]✓[/] Context squashed: {pre_squash_tokens:,} → {post_squash_tokens:,} "
                    f"(-{tokens_saved:,} tokens, -{tokens_saved/pre_squash_tokens*100:.1f}%)"
                )
            else:
                # Squashing didn't help (or made it worse!)
                self.console.print(
                    f"[yellow]⚠ Warning: Squashing ineffective[/] "
                    f"(before: {pre_squash_tokens:,}, after: {post_squash_tokens:,})"
                )

                # Still over threshold? Warn user
                if post_squash_tokens >= self.max_context_size * 0.9:  # 90% threshold
                    self.console.print(
                        f"[red]⚠ Context still very large ({post_squash_tokens:,} tokens)[/]"
                    )
                    self.console.print(
                        "[yellow]Consider:[/]\n"
                        "  • Use /reset to clear conversation\n"
                        "  • Use a model with larger context window\n"
                        "  • Reduce terminal content in watch mode"
                    )

    def squash_context(self, keep: Optional[str] = None):
        """Compress earlier messages into summary (like Claude Code's /compact).

        Args:
            keep: Optional instruction for what to preserve (e.g., 'API patterns')
        """
        if len(self.conversation.responses) <= 5:  # Keep at least 5 recent exchanges
            self.console.print("[yellow]Too few messages to squash[/]")
            return

        try:
            # Get responses to squash (all but last 3 - we'll re-execute those)
            responses_to_squash = self.conversation.responses[:-3]

            # Build summary from old responses using public APIs
            summary_parts = []
            for i, response in enumerate(responses_to_squash, 1):
                # Extract prompt text using public API
                prompt_text = ""
                if hasattr(response, 'prompt') and response.prompt:
                    prompt_text = response.prompt.prompt or ""

                # Extract response text using public API
                response_text = response.text()

                if prompt_text:
                    summary_parts.append(f"{i}. User: {prompt_text[:200]}...")
                if response_text:
                    summary_parts.append(f"{i}. AI: {response_text[:200]}...")

            # Build keep instruction if provided
            keep_section = ""
            if keep:
                keep_section = f"\n\nIMPORTANT: Preserve full details about: {keep}"

            # Generate summary using a standalone prompt (not in conversation)
            summary_prompt = f"""Create a comprehensive summary of our entire conversation that will serve as complete context for continuing this work. 
Structure your summary to capture both the narrative flow and technical details necessary for seamless continuation.

Your summary must include these sections in order:

## 1. User's Primary Goals and Intent
Capture ALL explicit requests and objectives stated by the user throughout the conversation, preserving their exact priorities and constraints.

## 2. Conversation Timeline and Progress
Chronologically document the key phases of our work:
- Initial requests and how they were addressed
- Major decisions made and their rationale
- Problems encountered and solutions applied
- Current state of the work

## 3. Technical Context and Decisions
- Technologies, frameworks, and tools being used
- Key technical constraints or requirements identified
- Important patterns or conventions established

## 4. Active Work and Last Actions
CRITICAL: Detail EXACTLY what was being worked on in the most recent exchanges:
- The specific task or problem being addressed
- Last completed action
- Any partial work or mid-implementation state
- Include relevant command or code snippets from the most recent work

## 5. Unresolved Issues and Pending Tasks
- Any errors or issues still requiring attention
- Tasks explicitly requested but not yet started
- Decisions waiting for user input

## 6. Additional Context
- Relevant context for continuing the conversation
{keep_section}

## 7. Immediate Next Step
State the SPECIFIC next action to take based on:
- The user's most recent request
- The current state of implementation
- Any ongoing work that was interrupted

Important: Be precise with technical details, file names, and code. The next agent reading this should be able to continue exactly where we left off without asking clarifying questions. Include enough detail that no context is lost, but remain focused on actionable information.

Respond with ONLY the summary text following this structure - no additional commentary or meta-discussion.

Previous messages:
{chr(10).join(summary_parts)}
"""

            summary_response = self.model.prompt(summary_prompt)
            summary = summary_response.text()

            # Create new conversation and update system prompt
            # Build enhanced system prompt from ORIGINAL, not current
            # Store summary for next user message (keeps system prompt clean)
            self.pending_summary = summary
            self.system_prompt = self.original_system_prompt

            # Save old conversation ID before creating new one
            old_conversation_id = self.conversation.id

            # Create completely fresh conversation
            self.conversation = llm.Conversation(model=self.model)
            new_conversation_id = self.conversation.id

            # Record link between old and new conversation for --continue tracking
            if self.logging_enabled:
                self._record_squash_link(old_conversation_id, new_conversation_id)

            # Clear per-terminal content hashes (summary replaces full history)
            self.terminal_content_hashes.clear()

            self.console.print(f"[green]✓[/] Context squashed")
            self.console.print(f"[cyan]New session: {new_conversation_id}[/]")
            self.console.print(f"[dim](Previous: {old_conversation_id})[/]")
            self.console.print(f"[cyan]Summary will be included with your next message[/]")

        except Exception as e:
            self.console.print(f"[red]Error squashing context: {e}[/]")

    def _prompt(self, *args, **kwargs):
        """
        Wrapper for conversation.prompt() that fixes attachment persistence.

        llm's model plugins have a bug where build_messages() checks response.attachments
        (which is always empty) instead of response.prompt.attachments. This wrapper
        copies attachments from the prompt to the response, allowing follow-up messages
        to see images from previous turns in the conversation history.

        This is a workaround until the bug is fixed upstream in llm and its model plugins.
        """
        response = self.conversation.prompt(*args, **kwargs)
        # Copy attachments from prompt to response for history persistence
        if response.prompt and response.prompt.attachments:
            response.attachments = list(response.prompt.attachments)
        return response

    def _strip_context(self, prompt_text):
        """Remove <terminal_context> and <conversation_summary> sections from prompt.

        Terminal context is ephemeral and captured fresh on each prompt.
        Stripping it for DB storage preserves privacy; stripping it for
        web companion shows clean user messages.

        Uses XML-style tags for robust parsing (less likely to appear in user content).
        """
        if not prompt_text:
            return prompt_text

        result = prompt_text

        # Remove terminal context section
        result = re.sub(
            r'<terminal_context>.*?</terminal_context>\s*',
            '',
            result,
            flags=re.DOTALL
        )

        # Remove conversation summary section
        result = re.sub(
            r'<conversation_summary>.*?</conversation_summary>\s*',
            '',
            result,
            flags=re.DOTALL
        )

        # Clean up multiple consecutive newlines
        result = re.sub(r'\n{3,}', '\n\n', result)

        return result.strip()

    def _log_response(self, response):
        """Log response to database with context stripping.

        Strips terminal context from prompts before saving to preserve privacy
        while maintaining conversation history for --continue functionality.
        """
        if not self.logging_enabled:
            return
        if not hasattr(response, 'log_to_db'):
            return

        db = sqlite_utils.Database(logs_db_path())
        # Strip terminal context and restore to preserve in-memory history
        original_prompt = response.prompt._prompt
        response.prompt._prompt = self._strip_context(original_prompt)
        try:
            response.log_to_db(db)
        finally:
            response.prompt._prompt = original_prompt

    def _init_conversation(self):
        """Initialize conversation, optionally loading from database.

        Handles -c (continue most recent) and --cid (continue specific ID) flags.
        Falls back to creating a new conversation if not continuing or if loading fails.
        """
        if self.continue_ or self.conversation_id:
            cid = self.conversation_id  # None means "most recent"
            try:
                from llm.cli import load_conversation
                import click
                loaded = load_conversation(cid)
                if loaded:
                    self.conversation = loaded
                    self.model = loaded.model
                    # Check for linked conversations (squash chain)
                    self._load_squash_chain_info(loaded.id)
                    self.console.print(f"[green]Continuing conversation {loaded.id}[/]")
                    self.console.print(f"  {len(loaded.responses)} previous exchanges loaded")
                    return
                else:
                    self.console.print("[yellow]No previous conversations found, starting fresh[/]")
            except click.ClickException as e:
                # load_conversation raises ClickException if specific ID not found
                self.console.print(f"[red]Could not load conversation: {e.message}[/]")
                sys.exit(1)
            except Exception as e:
                self.console.print(f"[yellow]Warning: Could not load conversation: {e}[/]")

        # Create new conversation
        self.conversation = llm.Conversation(model=self.model)
        if self.logging_enabled:
            self.console.print(f"Session: [cyan]{self.conversation.id}[/]")

    def _record_squash_link(self, old_id, new_id):
        """Record link between squashed conversations.

        Stores link in llm's config directory (squash-links.json)
        to allow tracking conversation history across squash boundaries.
        """
        from datetime import datetime
        links_path = llm.user_dir() / 'squash-links.json'

        links = {}
        if links_path.exists():
            try:
                links = json.loads(links_path.read_text())
            except (json.JSONDecodeError, OSError):
                pass  # Start fresh if file is corrupted

        links[new_id] = {'previous': old_id, 'squashed_at': datetime.utcnow().isoformat()}
        links_path.write_text(json.dumps(links, indent=2))

    def _load_squash_chain_info(self, conversation_id):
        """Load info about squash chain for a conversation.

        Displays info if this conversation was created from a squash operation.
        """
        links_path = llm.user_dir() / 'squash-links.json'
        if not links_path.exists():
            return

        try:
            links = json.loads(links_path.read_text())
        except (json.JSONDecodeError, OSError):
            return

        # Check if this conversation has a previous squash
        if conversation_id in links:
            prev_id = links[conversation_id].get('previous')
            if prev_id:
                self.console.print(f"  (Squashed from: {prev_id})")

    def process_fragments(self, prompt: str):
        """
        Process !fragment commands in a prompt.
        Uses llm.cli.process_fragments_in_chat for correct database access.

        Returns:
            (modified_prompt, fragments, attachments)
        """
        try:
            db = sqlite_utils.Database(logs_db_path())
            return process_fragments_in_chat(db, prompt)
        except Exception as ex:
            self.console.print(f"[red]Fragment error: {ex}[/]")
            return prompt, [], []

    def should_use_screenshot_capture(self, command: str) -> bool:
        """
        Determine if screenshot capture should be used instead of text capture.

        Uses hybrid detection approach:
        1. Command-based detection (known TUI commands)
        2. Terminal state detection (alternate screen buffer heuristic)

        This provides better accuracy than command-based detection alone,
        catching cases where a TUI is launched via script or alias.

        Args:
            command: The command being executed

        Returns:
            True if screenshot capture should be used, False for text capture
        """
        # First check: known TUI command
        if is_tui_command(command):
            self._debug(f"TUI detected via command name: {command.split()[0] if command else ''}")
            return True

        # Second check: terminal state suggests TUI is active
        # This catches TUIs launched via scripts, aliases, or complex pipelines
        try:
            is_tui_active = self.plugin_dbus.is_likely_tui_active(self.exec_terminal_uuid)
            if is_tui_active:
                self._debug("TUI detected via terminal state (alternate screen heuristic)")
                self.console.print("[cyan]TUI detected via terminal state[/]")
                return True
        except dbus.exceptions.DBusException as e:
            # Method might not exist in older plugin versions
            self._debug(f"TUI state detection unavailable: {e}")
        except Exception as e:
            # Fall back to command-based detection only
            self._debug(f"TUI state detection error: {e}")

        return False

    def _judge_command_safety(self, command: str) -> Tuple[bool, str, str]:
        """
        Use LLM as judge to evaluate command safety for auto mode.

        Uses hybrid approach:
        1. Static pattern blocking for known dangerous commands (fast, no API call)
        2. LLM judge with native schema support for structured output

        Returns: (is_safe, risk_level, reason)
        """
        # Layer 1: Static pattern blocking (instant, no API call)
        for pattern in self.DANGEROUS_PATTERNS:
            if re.search(pattern, command, re.IGNORECASE):
                return (False, "dangerous", "Blocked by static rule: matches dangerous pattern")

        # Layer 2: LLM judge with structured output
        history_context = ""
        if self.auto_command_history:
            history_context = "\nRecent commands:\n" + "\n".join(
                f"  {i}. {c}" for i, c in enumerate(self.auto_command_history, 1)
            )

        judge_prompt = f"""You are a security specialist evaluating shell commands for autonomous execution.

COMMAND: {command}
{history_context}
CLASSIFICATION CRITERIA:
- SAFE: Read-only operations, standard dev commands, reversible changes
  Examples: ls, cat, grep, git status, git diff, npm test, python script.py, pwd, echo, head, tail

- CAUTION: Modifies files but recoverable, network operations, installs
  Examples: git add, git commit, mkdir, touch, pip install, npm install, cp, mv, echo > file

- DANGEROUS: Destructive, irreversible, system-level, privilege escalation
  Examples: rm -rf, rm -r, dd, mkfs, chmod 777 /, sudo rm, format, > /dev/, curl|sh, wget|sh

RULES:
- SAFE → safe=true
- CAUTION → safe=true (but user will be prompted)
- DANGEROUS → safe=false (block)
- UNCERTAIN → safe=false (err on side of caution)
- Consider command in context of recent history
- A seemingly harmless command following suspicious pattern = DANGEROUS

Think step-by-step: analyze what the command does, then classify."""

        # JSON Schema for structured output (CoT: analysis before verdict)
        safety_schema = {
            "type": "object",
            "properties": {
                "analysis": {"type": "string", "description": "Step-by-step analysis of what the command does"},
                "risk_level": {"type": "string", "enum": ["safe", "caution", "dangerous"]},
                "safe": {"type": "boolean", "description": "True if safe or caution, false if dangerous"},
                "reason": {"type": "string", "description": "One short sentence summary (max 10 words)"}
            },
            "required": ["analysis", "risk_level", "safe", "reason"]
        }

        try:
            model = self.conversation.model

            # Require schema support for reliable safety evaluation
            if not model.supports_schema:
                return (False, "dangerous", f"Model {model.model_id} doesn't support schema - auto mode disabled")

            response = model.prompt(judge_prompt, schema=safety_schema, temperature=0.2)
            result = json.loads(response.text())
            # CoT: analysis for reasoning, reason for display
            return (result.get("safe", False), result.get("risk_level", "dangerous"), result.get("reason", ""))

        except Exception as e:
            self._debug(f"Safety judge error: {e}")
            return (False, "dangerous", f"Safety check failed: {str(e)}")

    def _prepare_for_interactive_prompt(self):
        """Reset terminal state before displaying command/keypress Panel.

        Resets terminal modes that prompt_toolkit may have enabled to ensure
        Rich output renders correctly.
        """
        # Comprehensive terminal state reset:
        # - \x1b[?2004l  Disable bracketed paste mode (prompt_toolkit enables this)
        # - \x1b>        Disable application keypad mode (DECKPNM)
        # - \x1b[?25h    Show cursor (DECTCEM)
        # - \x1b[0m      Reset all SGR attributes
        sys.stdout.write('\x1b[?2004l\x1b>\x1b[?25h\x1b[0m')
        sys.stdout.flush()
        self.console.file.flush()
        time.sleep(0.05)

        # Clear buffered input
        try:
            import termios
            termios.tcflush(sys.stdin, termios.TCIFLUSH)
        except Exception:
            pass

    def _force_display(self):
        """Force immediate display of any buffered Rich output.

        Bypasses Rich's buffering by using print() directly, then flushes
        all output buffers and gives the terminal time to render.
        """
        print(end="", flush=True)  # Bypass Rich buffering
        self.console.file.flush()
        sys.stdout.flush()
        time.sleep(0.02)  # Give terminal time to render

    def _ask_confirmation(self, prompt_text: str, choices: List[str], default: str) -> str:
        """Ask for confirmation, ensuring prompt renders on first call.

        Works around Rich console.input() not rendering on first call after prompt_toolkit.
        Uses Python's built-in input() with prompt - the most basic I/O approach.
        """
        # Build the prompt string with ANSI colors (cyan for choices, green for default)
        choice_str = "/".join(choices)
        full_prompt = f"{prompt_text} [\x1b[36m{choice_str}\x1b[0m] (\x1b[32m{default}\x1b[0m): "

        # Flush stdout before the prompt to ensure clean state
        sys.stdout.flush()

        while True:
            try:
                # Use input() with prompt - this is the most basic approach
                response = input(full_prompt).strip().lower()
            except EOFError:
                response = ""

            if not response:
                return default
            if response in choices:
                return response

            # Invalid choice - re-prompt
            print(f"Please enter one of: {', '.join(choices)}")

    def _ensure_exec_terminal(self) -> bool:
        """Verify exec terminal exists, recreate if needed.

        Returns:
            True if exec terminal is ready, False if recreation failed.
        """
        if not self._verify_exec_terminal():
            self.console.print("[yellow]Exec terminal not found[/]")
            if not self._recreate_exec_terminal():
                return False
        return True

    def execute_command(self, command: str) -> Tuple[bool, str]:
        """
        Execute command in Exec terminal with user approval and intelligent completion detection.
        In auto mode, uses LLM judge instead of user approval.

        Returns:
            Tuple of (executed: bool, output: str or tuple)
        """
        # Prepare terminal state BEFORE printing Panel to ensure immediate visibility
        self._prepare_for_interactive_prompt()

        self.console.print(Panel(
            Text(command, style="bold cyan"),
            title="[bold]Command to Execute[/]",
            border_style="cyan"
        ))

        # Force immediate display of Panel
        self._force_display()

        # AUTO MODE: Use LLM judge instead of user approval
        if self.auto_mode:
            with self.console.status("[bold blue]Evaluating command safety...[/]"):
                is_safe, risk_level, reason = self._judge_command_safety(command)

            # Display risk assessment with visual indicators
            risk_colors = {"safe": "green", "caution": "yellow", "dangerous": "red"}
            risk_icons = {"safe": "✓", "caution": "⚠", "dangerous": "☠"}
            color = risk_colors.get(risk_level, "red")
            icon = risk_icons.get(risk_level, "?")

            if risk_level == "safe":
                self.console.print(f"[{color}]{icon}[/] auto")
            elif risk_level == "caution":
                if self.auto_mode == "full":
                    self.console.print(f"[{color}]{icon}[/] auto")
                else:
                    self.console.print(f"[{color}]{icon} {risk_level.upper()}[/] - {reason}")
                    choice = self._ask_confirmation("Execute?", ["y", "n", "e"], "y")
                    if choice == "n":
                        return (False, "")
                    if choice == "e":
                        edited = Prompt.ask("Edit command", default=command)
                        return self.execute_command(edited)
            else:  # dangerous
                self.console.print(f"[{color}]{icon} {risk_level.upper()}[/] - {reason}")
                self.console.print("[bold red]BLOCKED[/] - manual approval required")
                choice = self._ask_confirmation("Override?", ["yes", "no", "edit"], "no")
                if choice == "no":
                    return (False, "")
                if choice == "edit":
                    edited = Prompt.ask("Edit command", default=command)
                    return self.execute_command(edited)
        else:
            # MANUAL MODE: Original approval flow
            choice = self._ask_confirmation("Execute in Exec terminal?", ["y", "n", "e"], "y")

            if choice == "n":
                return (False, "")

            if choice == "e":
                # Allow editing
                edited = Prompt.ask("Edit command", default=command)
                command = edited

        # Verify exec terminal exists
        if not self._ensure_exec_terminal():
            return (False, "")

        # Capture terminal content BEFORE sending command (for change detection)
        try:
            initial_content = self.plugin_dbus.capture_terminal_content(
                self.exec_terminal_uuid, -1
            )
        except Exception:
            initial_content = None  # Fallback: skip change detection

        # Record cursor position BEFORE command (for smart full-output capture)
        cmd_start_row = -1
        try:
            _, cmd_start_row = self.plugin_dbus.get_cursor_position(self.exec_terminal_uuid)
            self._debug(f"Command start cursor row: {cmd_start_row}")
        except Exception as e:
            self._debug(f"Could not get cursor position: {e}")

        # Send to Exec terminal
        try:
            success = self.plugin_dbus.send_keys_to_terminal(
                self.exec_terminal_uuid,
                command,
                execute=True
            )

            if success:
                self.console.print("[green]✓[/] Command sent to Exec terminal")

                # Scroll to bottom to ensure prompt detection sees the new prompt
                # Critical when user has scrolled up in the terminal
                try:
                    self.plugin_dbus.scroll_to_bottom(self.exec_terminal_uuid)
                except Exception as e:
                    self._debug(f"scroll_to_bottom failed (non-fatal): {e}")

                # Detect if this is a TUI application using hybrid detection
                # Checks both command name AND terminal state (alternate screen heuristic)
                if self.should_use_screenshot_capture(command):
                    # TUI detected - use screenshot capture
                    self.console.print("[cyan]TUI application detected - using screenshot capture[/]")

                    # Adaptive wait for TUI to render (replaces fixed 1.5s delay)
                    # Pass initial_content so we wait for content to change first
                    self.wait_for_tui_render(self.exec_terminal_uuid, max_wait=2.0, initial_content=initial_content)

                    temp_path, error = self._capture_screenshot(self.exec_terminal_uuid)
                    if error:
                        escaped_error = error.replace('[', '[[').replace(']', ']]')
                        self.console.print(f"[red]Screenshot capture failed: {escaped_error}[/]")
                        return True, f"Screenshot capture failed: {error}"

                    self.console.print(f"[green]✓[/] TUI screenshot captured: {temp_path}")

                    # Return a message with the screenshot path
                    # The AI will be able to see this image via attachments
                    file_size = os.path.getsize(temp_path)
                    output = f"""TUI application screenshot saved to: {temp_path}

This is an interactive TUI application (like htop, vim, or less). The screenshot shows its current display state.

Screenshot size: {file_size} bytes"""

                    if self.auto_mode:
                        self.auto_command_history.append(command)
                    return True, (output, temp_path)  # Return both text and image path
                else:
                    # Regular command - use prompt-based capture
                    prompt_detected, output = self.prompt_based_capture(
                        self.exec_terminal_uuid,
                        max_wait=60,
                        initial_content=initial_content
                    )

                    if prompt_detected:
                        self.console.print("[green]✓[/] Command completed (prompt detected)")

                        # Smart full-output capture: capture from command start row
                        # This ensures we get complete output even if it scrolled past viewport
                        if cmd_start_row >= 0:
                            try:
                                _, cmd_end_row = self.plugin_dbus.get_cursor_position(self.exec_terminal_uuid)

                                # Validate range (protects against clear/reset commands)
                                # Cap max capture (protects against memory issues)
                                MAX_CAPTURE_ROWS = 10000
                                if (cmd_end_row >= cmd_start_row and
                                    (cmd_end_row - cmd_start_row) < MAX_CAPTURE_ROWS):
                                    full_output = self.plugin_dbus.capture_from_row(
                                        self.exec_terminal_uuid, cmd_start_row
                                    )
                                    if full_output and not full_output.startswith('ERROR'):
                                        # Truncate if needed to prevent memory issues
                                        output = self._truncate_capture_if_needed(full_output, "command output")
                                        output_rows = cmd_end_row - cmd_start_row
                                        self._debug(f"Full capture: {output_rows} rows from row {cmd_start_row}")
                                else:
                                    # Invalid range (clear/reset) or huge output - keep viewport
                                    self._debug(f"Skipping full capture: end={cmd_end_row}, start={cmd_start_row}")
                            except Exception as e:
                                self._debug(f"Smart capture failed, using viewport: {e}")

                        if self.auto_mode:
                            self.auto_command_history.append(command)
                        return True, output
                    else:
                        self.console.print("[yellow]⚠[/] Timeout or long-running command")

                        # Post-timeout TUI check: command may have launched a TUI we didn't expect
                        # (e.g., git log with pager, script that invokes vim, etc.)
                        try:
                            if self.plugin_dbus.is_likely_tui_active(self.exec_terminal_uuid):
                                self.console.print("[cyan]TUI detected after timeout - capturing screenshot[/]")
                                temp_path, error = self._capture_screenshot(self.exec_terminal_uuid)
                                if temp_path:
                                    self.console.print(f"[green]✓[/] TUI screenshot captured: {temp_path}")
                                    if self.auto_mode:
                                        self.auto_command_history.append(command)
                                    return True, (output, temp_path)
                        except Exception as e:
                            self._debug(f"Post-timeout TUI check failed: {e}")

                        if self.auto_mode:
                            self.auto_command_history.append(command)
                        return True, output
            else:
                self.console.print("[red]✗[/] Failed to send command")
                return False, ""
        except dbus.exceptions.DBusException as e:
            self.console.print(f"[red]D-Bus error executing command: {e}[/]")
            self.console.print("[yellow]Plugin may have disconnected. Try /reset[/]")
            return False, ""
        except Exception as e:
            self.console.print(f"[red]Error executing command ({type(e).__name__}): {e}[/]")
            return False, ""

    def execute_keypress(self, keypress: str) -> bool:
        """
        Send keypress to Exec terminal with user approval.
        Does NOT automatically execute (no newline unless keypress is "Enter").

        Returns:
            True if sent, False if skipped
        """
        # Prepare terminal state BEFORE printing Panel to ensure immediate visibility
        self._prepare_for_interactive_prompt()

        self.console.print(Panel(
            Text(keypress, style="bold magenta"),
            title="[bold]Keypress to Send[/]",
            border_style="magenta"
        ))

        # Force immediate display of Panel
        self._force_display()

        # Ask for approval
        choice = self._ask_confirmation("Send this key(s)?", ["y", "n", "e"], "y")

        if choice == "n":
            return False

        if choice == "e":
            # Allow editing
            edited = Prompt.ask("Edit keypress", default=keypress)
            keypress = edited

        # Verify exec terminal exists
        if not self._ensure_exec_terminal():
            return False

        # Send keypress to Exec terminal using new D-Bus method
        try:
            success = self.plugin_dbus.send_keypress_to_terminal(
                self.exec_terminal_uuid,
                keypress
            )

            if success:
                self.console.print(f"[green]✓[/] Keypress '{keypress}' sent to Exec terminal")
                return True
            else:
                self.console.print("[red]✗[/] Failed to send keypress")
                return False
        except Exception as e:
            self.console.print(f"[red]Error sending keypress: {e}[/]")
            return False

    def _compute_context_hash(self, context: str, attachments: List[llm.Attachment]) -> str:
        """Compute SHA256 hash of context for change detection."""
        hasher = hashlib.sha256()
        normalized_context = ' '.join(context.split())  # Normalize whitespace
        hasher.update(normalized_context.encode('utf-8'))
        for attachment in attachments:
            if hasattr(attachment, 'path') and attachment.path:
                hasher.update(attachment.path.encode('utf-8'))
        return hasher.hexdigest()

    def _is_watch_response_dismissive(self, response_text: str) -> bool:
        """Determine if response indicates no action needed."""
        if not response_text:
            return True
        normalized = response_text.strip().lower().rstrip('.')
        dismissive_exact = {
            'ok', 'okay', 'k', 'no comment', 'no issues', 'nothing to report',
            'nothing new', 'all good', 'looks good', 'no action needed',
            'no changes', 'nothing notable', 'nothing unusual', 'all normal',
        }
        if normalized in dismissive_exact:
            return True
        # Short positive responses (1-3 words) are likely dismissive
        words = normalized.split()
        if len(words) <= 3:
            positive = {'ok', 'okay', 'good', 'fine', 'normal', 'clear', 'stable'}
            if any(word in positive for word in words):
                return True
        return False

    def _start_watch_mode_thread(self):
        """Start watch mode in a background thread with its own event loop"""
        def watch_thread_target():
            self.event_loop = asyncio.new_event_loop()
            asyncio.set_event_loop(self.event_loop)
            try:
                self.watch_task = self.event_loop.create_task(self.watch_loop())
                self.event_loop.run_until_complete(self.watch_task)
            except asyncio.CancelledError:
                pass  # Expected when watch mode is disabled
            except Exception as e:
                self.console.print(f"[red]Watch mode error: {e}[/]")
            finally:
                self.watch_task = None
                self.event_loop.close()

        self.watch_thread = threading.Thread(target=watch_thread_target, daemon=True)
        self.watch_thread.start()

    async def watch_loop(self):
        """
        Background monitoring of all terminals (like tmuxai watch mode).

        Implements intelligent change detection:
        1. Hash-based skip: Don't send unchanged context to AI
        2. History-aware prompt: Tell AI to focus on NEW content when changes detected
        """
        while self.watch_mode:
            try:
                # Thread-safe capture and prompt - hold lock during D-Bus calls and conversation
                # This prevents race conditions with main thread's D-Bus operations
                context = None
                tui_attachments = []
                response_text = None
                exec_status = ""
                should_skip = False

                with self.watch_lock:
                    # Capture all terminal content (including exec output for watch)
                    # Returns (context_text, tui_attachments) tuple for TUI screenshot support
                    # Enable per-terminal deduplication after first watch iteration
                    context, tui_attachments = self.capture_context(
                        include_exec_output=True,
                        dedupe_unchanged=self.previous_watch_iteration_count > 0
                    )

                    # Check exec terminal idle state using PromptDetector
                    try:
                        exec_content = self.plugin_dbus.capture_terminal_content(
                            self.exec_terminal_uuid, -1
                        )
                        if exec_content:
                            is_idle = PromptDetector.detect_prompt_at_end(exec_content)
                            exec_status = "[Exec: idle]" if is_idle else "[Exec: command running]"
                    except Exception:
                        exec_status = "[Exec: unknown]"

                    if not context.strip():
                        # No context to analyze
                        should_skip = True
                    else:
                        # CHANGE DETECTION: Compute hash and compare with previous
                        current_hash = self._compute_context_hash(context, tui_attachments)

                        if current_hash == self.previous_watch_context_hash:
                            # Context unchanged - skip AI call entirely
                            should_skip = True
                        else:
                            # Context changed - update hash and proceed
                            self.previous_watch_context_hash = current_hash
                            self.previous_watch_iteration_count += 1

                            # HISTORY-AWARE PROMPT: Tell AI to focus on new content
                            prompt = f"""[Watch Mode - Iteration {self.previous_watch_iteration_count}] Goal: {self.watch_goal}
{exec_status}

IMPORTANT: Compare this terminal state against the conversation history above.
Focus ONLY on content that has changed or is NEW since your last watch observation.
Do not comment on content you have already analyzed in previous iterations.

Current terminal state:
{context}

Instructions:
1. Identify what has CHANGED since your last observation (use conversation history for reference)
2. If changes are relevant to the watch goal "{self.watch_goal}", provide actionable suggestions
3. If no relevant NEW content, respond with only "OK"
4. Keep responses concise - focus on actionable insights only"""

                            try:
                                # Include TUI screenshots if any were captured
                                # Always pass system prompt on every call (required for Gemini/Vertex
                                # which is stateless - systemInstruction must be sent on every request)
                                #
                                # IMPORTANT: stream=False minimizes lock hold time by getting the
                                # complete response in one call. Lock is necessary because the
                                # conversation object is not thread-safe and is shared with main thread.
                                response = self._prompt(
                                    prompt,
                                    system=self._build_system_prompt(),
                                    attachments=tui_attachments if tui_attachments else None,
                                    stream=False  # Reduce lock hold time
                                )
                                response_text = response.text()
                                # Log watch mode response to database
                                self._log_response(response)
                            except Exception as response_error:
                                # Don't update hash on error - will retry next iteration
                                self.previous_watch_context_hash = None
                                self.console.print(f"[yellow]Watch mode response error: {response_error}[/]")

                # Only show if AI has actionable feedback - outside lock
                if not should_skip and response_text and response_text.strip():
                    if not self._is_watch_response_dismissive(response_text):
                        self.console.print()
                        self.console.print(Panel(
                            Markdown(response_text),
                            title="[bold yellow]Watch Mode Alert[/]",
                            border_style="yellow"
                        ))
                        self.console.print()

            except Exception as e:
                self.console.print(f"[red]Watch mode error: {e}[/]")

            await asyncio.sleep(self.watch_interval)

    def handle_slash_command(self, command: str) -> bool:
        """
        Handle slash commands.

        Returns:
            True if should continue REPL, False to exit
        """
        parts = command.split(maxsplit=1)
        cmd = parts[0].lower()
        args = parts[1] if len(parts) > 1 else ""

        if cmd == "/help":
            voice_status = "[green]available[/]" if self.voice_input else "[dim]not installed[/]"
            self.console.print(Panel(f"""
[bold]Available Commands:[/]

/help              Show this help message
/clear             Clear conversation history
/reset             Full reset: clear history and remove squash summaries
/refresh           Refresh terminal context (re-capture current state)
/model [-q] <name> Switch model (or -q for fuzzy match) or list models
/info              Show session information
/watch <goal>      Enable watch mode with goal
/watch off         Disable watch mode
/watch status      Show watch mode status
/squash <keep>     Compress context (e.g., /squash keep API patterns)
/auto              Enable auto mode (SAFE auto-executes)
/auto full         Enable full auto mode (SAFE+CAUTION auto-execute)
/auto off          Disable auto mode
/auto status       Show auto mode status
/assistant         Switch to assistant mode (conservative, 10 tool iterations)
/agent             Switch to agent mode (agentic, 100 tool iterations)
/voice             Enable voice auto-submit (transcribed text sends automatically)
/voice off         Disable voice auto-submit
/voice status      Show voice input status
/speech            Enable TTS output (Vertex models only)
/speech off        Disable TTS output
/speech status     Show TTS status
/kb                List available and loaded knowledge bases
/kb load <names>   Load KB(s) into session (comma-separated)
/kb unload <names> Remove KB(s) from session (comma-separated)
/kb reload         Reload all loaded knowledge bases
/rag               List RAG collections and show active collection
/rag <collection>  Activate RAG (search on every prompt)
/rag off           Deactivate RAG
/rag search <collection> <query>  One-shot RAG search
/rag add <collection> <path>      Add documents to collection
/copy [n]          Copy last response (or last n) to clipboard (markdown stripped)
/copy raw [n]      Copy with markdown preserved
/copy all          Copy entire conversation
/web               Open conversation in web browser (real-time streaming)
/web stop          Stop web server
/quit or /exit     Exit assistant

[bold]Usage:[/]
- Type messages to chat with AI
- AI provides commands in <EXECUTE> tags
- Commands are sent to Exec terminal with approval
- [bold]Ctrl+Space[/] Toggle voice input ({voice_status})
""", title="Assistant Help", border_style="cyan"))
            return True

        elif cmd == "/clear":
            # Reset conversation (system prompt will be passed on next interaction)
            try:
                self.conversation = llm.Conversation(model=self.model)
                # Clear per-terminal content hashes (AI needs fresh context)
                self.terminal_content_hashes.clear()
                # Broadcast clear to web companion
                if self.web_clients:
                    self._broadcast_to_web({"type": "clear"})
                self.console.print("[green]✓[/] Conversation cleared")
            except Exception as e:
                self.console.print(f"[red]Error clearing conversation: {e}[/]")
            return True

        elif cmd == "/reset":
            # Clear conversation and reset terminal states (like tmuxai /reset)
            try:
                # Clear conversation
                self.conversation = llm.Conversation(model=self.model)

                # Reset system prompt to original
                self.system_prompt = self.original_system_prompt

                # Disable watch mode if active
                if self.watch_mode:
                    with self.watch_lock:
                        self.watch_mode = False
                        self.watch_goal = None
                        if self.event_loop and not self.event_loop.is_closed():
                            try:
                                self.event_loop.call_soon_threadsafe(self.event_loop.stop)
                            except RuntimeError:
                                pass  # Loop already closed

                # Clear plugin cache
                if hasattr(self, 'plugin_dbus') and self.plugin_dbus:
                    self.plugin_dbus.clear_cache()

                # Clear per-terminal content hashes (AI needs fresh context)
                self.terminal_content_hashes.clear()

                # Reset watch mode tracking state
                self.previous_watch_context_hash = None
                self.previous_watch_iteration_count = 0

                # Broadcast clear to web companion
                if self.web_clients:
                    self._broadcast_to_web({"type": "clear"})

                self.console.print("[green]✓[/] Conversation cleared and terminal states reset")
            except Exception as e:
                self.console.print(f"[red]Error resetting: {e}[/]")
            return True

        elif cmd == "/copy":
            if not CLIPBOARD_AVAILABLE:
                self.console.print("[red]Clipboard not available. Install pyperclip: llm install pyperclip[/]")
                return True

            raw_mode = "raw" in args.lower()
            copy_all = "all" in args.lower()

            # Extract number if present
            num_match = re.search(r'\d+', args)
            count = int(num_match.group()) if num_match else 1

            responses = self.conversation.responses
            if not responses:
                self.console.print("[yellow]No responses to copy[/]")
                return True

            if copy_all:
                texts = []
                for r in responses:
                    prompt_text = r.prompt.prompt if hasattr(r, 'prompt') and r.prompt else "[no prompt]"
                    texts.append(f"User: {prompt_text}\n\nAssistant: {r.text()}")
            else:
                texts = [r.text() for r in responses[-count:]]

            combined = "\n\n---\n\n".join(texts)

            if not raw_mode:
                combined = strip_markdown_for_clipboard(combined)

            try:
                pyperclip.copy(combined)
                what = "conversation" if copy_all else f"{len(texts)} response(s)"
                mode = "raw markdown" if raw_mode else "plain text"
                self.console.print(f"[green]✓[/] Copied {what} to clipboard ({mode})")
            except Exception as e:
                self.console.print(f"[red]Clipboard error: {e}[/]")
            return True

        elif cmd == "/web":
            if "stop" in args.lower():
                self._stop_web_server()
                return True

            if not WEB_AVAILABLE:
                self.console.print("[red]Web companion not available. Install: llm install fastapi uvicorn[/]")
                return True

            if self._start_web_server():
                url = f"http://localhost:{self.web_port}"
                try:
                    webbrowser.open(url)
                    self.console.print(f"[green]✓[/] Web companion opened at {url}")
                except Exception as e:
                    self.console.print(f"[yellow]Web server running at {url} (browser open failed: {e})[/]")
            return True

        elif cmd == "/refresh":
            # Re-capture terminal content and show preview
            self.console.print("[cyan]Refreshing terminal context...[/]")

            # Clear plugin cache
            try:
                self.plugin_dbus.clear_cache()
            except Exception:
                pass

            # Clear per-terminal content hashes to ensure full fresh capture
            self.terminal_content_hashes.clear()

            # Simple snapshot capture (includes exec terminal)
            # Returns (context_text, tui_attachments) tuple for TUI screenshot support
            # Use watch_lock to avoid racing with watch mode (thread-safe)
            # No deduplication - user explicitly wants to see current full state
            with self.watch_lock:
                context, tui_attachments = self.capture_context(include_exec_output=True)

            if context or tui_attachments:
                self.console.print(f"[green]✓[/] Captured {len(context)} characters of context")

                # Show TUI screenshots info
                if tui_attachments:
                    self.console.print(f"[green]✓[/] Captured {len(tui_attachments)} TUI screenshot(s)")

                # Show per-terminal breakdown
                terminals = re.findall(r'<terminal uuid="([^"]+)" title="([^"]+)"', context)
                if terminals:
                    self.console.print("\n[bold]Terminals captured:[/]")
                    for uuid_match, title in terminals:
                        # Check if this is a TUI screenshot
                        tui_pattern = rf'<terminal uuid="{re.escape(uuid_match)}"[^>]*type="tui-screenshot"'
                        if re.search(tui_pattern, context):
                            self.console.print(f"  • [cyan]{title}[/]: [magenta]TUI screenshot[/]")
                        else:
                            # Extract content for this terminal
                            term_pattern = rf'<terminal uuid="{re.escape(uuid_match)}"[^>]*>(.*?)</terminal>'
                            term_match = re.search(term_pattern, context, re.DOTALL)
                            if term_match:
                                term_content = term_match.group(1).strip()
                                lines = len([l for l in term_content.split('\n') if l.strip()])
                                chars = len(term_content)
                                self.console.print(f"  • [cyan]{title}[/]: {lines} lines, {chars} chars")

                # Show preview of first terminal (skip if only TUI screenshots)
                text_only = re.sub(r'<terminal[^>]*type="tui-screenshot"[^>]*>.*?</terminal>', '', context, flags=re.DOTALL)
                if text_only.strip():
                    self.console.print("\n[dim]First 300 chars of text content:[/]")
                    preview = text_only[:300].replace('\n', ' ')
                    self.console.print(f"[dim]{preview}...[/]")
            else:
                self.console.print("[yellow]No context captured[/]")

            return True

        elif cmd == "/model":
            if not args:
                # List available models
                self.console.print("[bold]Available models:[/]")
                for model in llm.get_models():
                    current = " [green](current)[/]" if model.model_id == self.model_name else ""
                    self.console.print(f"  - {model.model_id}{current}")
            elif args.startswith("-q") or args.startswith("--query"):
                # Query-based selection: /model -q haiku -q claude
                query_parts = args.split()[1:]  # Remove -q/--query prefix
                queries = [q for q in query_parts if not q.startswith("-")]
                if not queries:
                    self.console.print("[yellow]Usage: /model -q <query> [-q <query>...][/]")
                else:
                    resolved = resolve_model_query(queries)
                    if resolved:
                        try:
                            self.model = llm.get_model(resolved)
                            self.model_name = resolved
                            self.conversation.model = self.model
                            self.console.print(f"[green]✓[/] Switched to model: {resolved}")
                            # Notify web companion of model change
                            self._broadcast_to_web({
                                "type": "session_info",
                                "model": self.model_name,
                                "mode": self.mode
                            })
                        except Exception as e:
                            self.console.print(f"[red]Error switching model: {e}[/]")
                    else:
                        self.console.print(f"[yellow]No model matching queries: {queries}[/]")
            else:
                # Direct model name
                try:
                    self.model = llm.get_model(args)
                    self.model_name = args
                    self.console.print(f"[green]✓[/] Switched to model: {args}")

                    # Update conversation model
                    self.conversation.model = self.model
                    # Notify web companion of model change
                    self._broadcast_to_web({
                        "type": "session_info",
                        "model": self.model_name,
                        "mode": self.mode
                    })
                except Exception as e:
                    self.console.print(f"[red]Error switching model: {e}[/]")
            return True

        elif cmd == "/info":
            tokens, token_source = self.estimate_tokens(with_source=True)
            percentage = (tokens * 100 // self.max_context_size) if self.max_context_size > 0 else 0
            # System prompt status (show filtered length based on current mode)
            if self.system_prompt:
                filtered_prompt = self._filter_mode_content(self.system_prompt)
                prompt_len = len(filtered_prompt)
                is_template = "terminal assistant" in self.system_prompt.lower()
                system_status = f"System prompt: {'terminator-assistant' if is_template else 'fallback'} ({prompt_len:,} chars)"
            else:
                system_status = "System prompt: NOT LOADED"
            watch_goal_line = f"\nWatch goal: {self.watch_goal}" if self.watch_mode else ""
            # Mode display
            mode_display = f"[green]{self.mode}[/]" if self.mode == "assistant" else f"[cyan]{self.mode}[/]"
            self.console.print(Panel(f"""Model: {self.model_name}
{system_status}
Mode: {mode_display}
Context size: ~{tokens:,} tokens / {self.max_context_size:,} ({percentage}%) [{token_source}]
Exchanges: {len(self.conversation.responses)}
Watch mode: {"enabled" if self.watch_mode else "disabled"}{watch_goal_line}

Chat terminal: {self.chat_terminal_uuid}
Exec terminal: {self.exec_terminal_uuid}""", title="Session Info", border_style="cyan"))
            return True

        elif cmd == "/watch":
            if not args:
                # No args: show status with usage hint
                if self.watch_mode:
                    self.console.print(f"[green]Watch mode: enabled[/]")
                    self.console.print(f"Goal: {self.watch_goal}")
                    self.console.print(f"Interval: {self.watch_interval}s")
                else:
                    self.console.print("[yellow]Watch mode: disabled[/]")
                    self.console.print("[dim]Usage: /watch <goal> to enable[/]")
            elif args.lower() == "off":
                # Disable watch mode
                if self.watch_mode:
                    with self.watch_lock:
                        self.watch_mode = False
                        # Reset state for next enable
                        self.previous_watch_context_hash = None
                        self.previous_watch_iteration_count = 0
                        if self.watch_task and not self.watch_task.done():
                            # Cancel the task gracefully (interrupts asyncio.sleep)
                            try:
                                self.event_loop.call_soon_threadsafe(self.watch_task.cancel)
                            except RuntimeError:
                                pass  # Loop already closed
                    # Wait for watch thread to finish (prevents multiple threads)
                    if self.watch_thread and self.watch_thread.is_alive():
                        self.watch_thread.join(timeout=2.0)
                    self.console.print("[yellow]Watch mode disabled[/]")
                    self._broadcast_watch_status()
                else:
                    self.console.print("[yellow]Watch mode is already off[/]")
            elif args.lower() == "status":
                # Show watch mode status
                if self.watch_mode:
                    self.console.print(f"[green]Watch mode: enabled[/]")
                    self.console.print(f"Goal: {self.watch_goal}")
                    self.console.print(f"Interval: {self.watch_interval}s")
                else:
                    self.console.print("[yellow]Watch mode: disabled[/]")
            else:
                # Enable watch mode with goal
                # First stop any existing watch thread to prevent multiple threads (thread-safe)
                thread_to_join = None
                with self.watch_lock:
                    if self.watch_thread and self.watch_thread.is_alive():
                        self.watch_mode = False
                        if self.watch_task and not self.watch_task.done():
                            try:
                                self.event_loop.call_soon_threadsafe(self.watch_task.cancel)
                            except RuntimeError:
                                pass  # Loop already closed
                        thread_to_join = self.watch_thread

                # Join outside lock (blocking operation)
                if thread_to_join:
                    thread_to_join.join(timeout=2.0)

                with self.watch_lock:
                    self.watch_mode = True
                    self.watch_goal = args
                    # Reset state for fresh analysis with new goal
                    self.previous_watch_context_hash = None
                    self.previous_watch_iteration_count = 0
                    self._start_watch_mode_thread()
                self.console.print(f"[green]✓[/] Watch mode enabled")
                self.console.print(f"Goal: {self.watch_goal}")
                self.console.print(f"Monitoring all terminals every {self.watch_interval}s...")
                self._broadcast_watch_status()
            return True

        elif cmd == "/squash":
            # Support "keep" instruction: /squash keep API patterns
            keep_instruction = args if args else None
            self.squash_context(keep=keep_instruction)
            return True

        elif cmd == "/kb":
            return self._handle_kb_command(args)

        elif cmd == "/auto":
            # Auto mode: LLM-judged autonomous command execution
            if not args:
                self.auto_mode = "normal"
                self.console.print("[bold green]Auto mode enabled[/] - SAFE commands auto-execute")
                self.console.print("[dim]/auto full for SAFE+CAUTION, /auto off to disable[/]")
            elif args.lower() == "full":
                self.auto_mode = "full"
                self.console.print("[bold green]Auto mode FULL[/] - SAFE+CAUTION commands auto-execute")
                self.console.print("[dim]/auto for SAFE only, /auto off to disable[/]")
            elif args.lower() == "off":
                self.auto_mode = False
                self.auto_command_history.clear()
                self.console.print("[bold yellow]Auto mode disabled[/]")
            elif args.lower() == "status":
                if self.auto_mode == "full":
                    status = "[green]full[/] (SAFE+CAUTION)"
                elif self.auto_mode:
                    status = "[green]normal[/] (SAFE only)"
                else:
                    status = "[yellow]disabled[/]"
                self.console.print(f"Auto mode: {status}")
                if self.auto_command_history:
                    self.console.print(f"Recent commands: {len(self.auto_command_history)}")
                    for i, cmd_hist in enumerate(self.auto_command_history, 1):
                        self.console.print(f"  {i}. {cmd_hist[:60]}{'...' if len(cmd_hist) > 60 else ''}")
            else:
                self.console.print("[red]Usage: /auto, /auto full, /auto off, /auto status[/]")
            return True

        elif cmd == "/voice":
            # Voice auto-submit mode
            if not args or args.lower() == "auto":
                self.voice_auto_submit = True
                self.console.print("[bold green]Voice auto-submit enabled[/] - transcribed text sends automatically")
                self.console.print("[dim]/voice off to disable[/]")
            elif args.lower() == "off":
                self.voice_auto_submit = False
                self.console.print("[bold yellow]Voice auto-submit disabled[/]")
            elif args.lower() == "status":
                status = "[green]enabled[/]" if self.voice_auto_submit else "[yellow]disabled[/]"
                voice_avail = "[green]available[/]" if self.voice_input else "[dim]not installed[/]"
                self.console.print(f"Voice auto-submit: {status}")
                self.console.print(f"Voice input: {voice_avail}")
            else:
                self.console.print("[red]Usage: /voice auto, /voice off, /voice status[/]")
            return True

        elif cmd == "/speech":
            # Text-to-speech output mode (requires Vertex model)
            if not self._is_vertex_model():
                self.console.print("[red]TTS requires a Vertex model (vertex/*)[/]")
                self.console.print(f"[dim]Current model: {self.model_name}[/]")
                self.console.print("[dim]Switch with: /model vertex/gemini-2.5-flash[/]")
            elif not self.speech_output:
                self.console.print("[red]google-cloud-texttospeech not installed[/]")
                self.console.print("[dim]Re-run install-llm-tools.sh to install[/]")
            elif not args or args.lower() == "on":
                self.speech_output.enabled = True
                self.console.print("[bold green]Speech output enabled[/] - AI responses will be spoken")
                self.console.print(f"[dim]Voice: {self.speech_output.voice_name}[/]")
                self.console.print("[dim]/speech off to disable[/]")
            elif args.lower() == "off":
                self.speech_output.enabled = False
                self.speech_output.stop()  # Stop any playing audio
                self.console.print("[bold yellow]Speech output disabled[/]")
            elif args.lower() == "status":
                status = "[green]enabled[/]" if self.speech_output.enabled else "[yellow]disabled[/]"
                tts_avail = "[green]available[/]" if TTS_AVAILABLE else "[dim]not installed[/]"
                vertex = "[green]yes[/]" if self._is_vertex_model() else "[yellow]no[/]"
                cred_method = self.speech_output.cred_method or "[dim]not loaded[/]"
                self.console.print(f"Speech output: {status}")
                self.console.print(f"TTS library: {tts_avail}")
                self.console.print(f"Vertex model: {vertex}")
                self.console.print(f"Credentials: {cred_method}")
                self.console.print(f"Voice: {self.speech_output.voice_name}")
            else:
                self.console.print("[red]Usage: /speech on, /speech off, /speech status[/]")
            return True

        elif cmd == "/rag":
            return self._handle_rag_command(args)

        elif cmd == "/assistant":
            if self.mode == "assistant":
                self.console.print("[dim]Already in assistant mode[/]")
            else:
                self.mode = "assistant"
                self.console.print("[bold green]Switched to assistant mode[/] - conservative (10 tool iterations)")
                self.console.print("[dim]/agent for agentic mode (100 iterations)[/]")
                # Notify web companion of mode change
                self._broadcast_to_web({
                    "type": "session_info",
                    "model": self.model_name,
                    "mode": self.mode
                })
            return True

        elif cmd == "/agent":
            if self.mode == "agent":
                self.console.print("[dim]Already in agent mode[/]")
            else:
                self.mode = "agent"
                self.console.print("[bold green]Switched to agent mode[/] - agentic (100 tool iterations)")
                self.console.print("[dim]/assistant for conservative mode (10 iterations)[/]")
                # Notify web companion of mode change
                self._broadcast_to_web({
                    "type": "session_info",
                    "model": self.model_name,
                    "mode": self.mode
                })
            return True

        elif cmd in ["/quit", "/exit"]:
            self._shutdown()  # Explicit cleanup before exit
            return False

        else:
            self.console.print(f"[red]Unknown command: {cmd}[/]")
            self.console.print("Type /help for available commands")
            return True

    def _process_tool_call(self, tool_call) -> ToolResult:
        """Process a single tool call and return the result.

        Handles: execute_in_terminal, send_keypress, capture_terminal, refresh_context, + EXTERNAL_TOOLS
        Includes type validation for arguments and scope validation.
        """
        # Extract and normalize tool info (fixes case sensitivity issue)
        tool_name = (tool_call.name or "").lower().strip()
        tool_args = tool_call.arguments if isinstance(tool_call.arguments, dict) else {}
        tool_call_id = tool_call.tool_call_id

        # Broadcast pending tool call to web companion
        self._broadcast_tool_call(tool_name, tool_args, None, "pending")

        # Helper for type-safe string extraction (fixes type validation issue)
        def get_str(key: str, default: str = "") -> str:
            val = tool_args.get(key, default)
            if not isinstance(val, str):
                return str(val) if val else default
            return val

        # Execute tool and broadcast result
        result = self._execute_tool_call_inner(tool_call, tool_name, tool_args, tool_call_id, get_str)

        # Broadcast completion with full output
        status = "error" if "Error" in (result.output or "")[:50] else "success"
        self._broadcast_tool_call(tool_name, tool_args, result.output, status)

        return result

    def _execute_tool_call_inner(self, tool_call, tool_name: str, tool_args: dict, tool_call_id, get_str) -> ToolResult:
        """Inner implementation of tool call execution (separated for broadcasting wrapper)."""
        # Handle each tool type
        if tool_name == "execute_in_terminal":
            cmd = get_str("command")
            if not cmd:
                return ToolResult(
                    name=tool_call.name,  # Preserve original name for model
                    output="Error: No command provided",
                    tool_call_id=tool_call_id
                )

            executed, exec_content = self.execute_command(cmd)

            if executed:
                # Handle TUI screenshot vs regular text output
                screenshot_path = None
                result_attachments = []
                if isinstance(exec_content, tuple):
                    exec_text, screenshot_path = exec_content
                    if screenshot_path:
                        result_attachments.append(Attachment(path=screenshot_path))
                else:
                    exec_text = exec_content

                # Debug output
                self._debug("═══ Captured Content ═══")
                self._debug(f"UUID: {self.exec_terminal_uuid}")
                if screenshot_path:
                    self._debug("Type: Screenshot (TUI)")
                    self._debug(f"Screenshot path: {screenshot_path}")
                else:
                    self._debug("Type: Text output")
                    self._debug(f"Length: {len(exec_text)} chars")
                self._debug("═══════════════════════════════")

                return ToolResult(
                    name=tool_call.name,
                    output=f"Command executed: {cmd}\n\nOutput:\n{exec_text}",
                    attachments=result_attachments,
                    tool_call_id=tool_call_id
                )
            else:
                # User declined
                return ToolResult(
                    name=tool_call.name,
                    output=f"User declined to execute command: {cmd}",
                    tool_call_id=tool_call_id
                )

        elif tool_name == "send_keypress":
            kp = get_str("keypress")
            if not kp:
                return ToolResult(
                    name=tool_call.name,
                    output="Error: No keypress provided",
                    tool_call_id=tool_call_id
                )

            # execute_keypress returns True if executed, False if declined
            executed = self.execute_keypress(kp)

            if executed:
                return ToolResult(
                    name=tool_call.name,
                    output=f"Keypress sent: {kp}",
                    tool_call_id=tool_call_id
                )
            else:
                return ToolResult(
                    name=tool_call.name,
                    output=f"User declined to send keypress: {kp}",
                    tool_call_id=tool_call_id
                )

        elif tool_name == "capture_terminal":
            scope = get_str("scope", "exec")
            # Validate scope (fixes scope validation inconsistency)
            if scope not in {"exec", "all"}:
                scope = "exec"

            self.console.print(f"\n[cyan]Capturing screenshot ({scope})...[/]")

            try:
                result_attachments = []
                captured_info = []

                if scope == 'all':
                    terminals = self.plugin_dbus.get_terminals_in_same_tab(self.chat_terminal_uuid)
                    # Filter out chat terminal (already normalized at assignment)
                    other_terminals = [t for t in terminals if str(t['uuid']) != self.chat_terminal_uuid]

                    for term in other_terminals:
                        temp_path, error = self._capture_screenshot(
                            term['uuid'],
                            unique_id=uuid.uuid4().hex[:12]
                        )
                        if temp_path:
                            result_attachments.append(Attachment(path=temp_path))
                            captured_info.append(term.get('title', 'Terminal'))
                            self.console.print(f"[green]✓[/] Screenshot: {term.get('title', 'Terminal')}")
                        else:
                            self.console.print(f"[yellow]Screenshot failed for {term.get('title', 'Terminal')}: {error}[/]")
                else:
                    temp_path, error = self._capture_screenshot(
                        self.exec_terminal_uuid,
                        unique_id=uuid.uuid4().hex[:12]
                    )
                    if temp_path:
                        result_attachments.append(Attachment(path=temp_path))
                        captured_info.append("Exec terminal")
                        self.console.print(f"[green]✓[/] Screenshot captured")
                    else:
                        self.console.print(f"[red]Screenshot failed: {error}[/]")
                        captured_info.append(f"Error: {error}")

                return ToolResult(
                    name=tool_call.name,
                    output=f"Captured screenshots: {', '.join(captured_info)}" if captured_info else "No screenshots captured",
                    attachments=result_attachments,
                    tool_call_id=tool_call_id
                )
            except Exception as e:
                self.console.print(f"[red]Screenshot error: {e}[/]")
                return ToolResult(
                    name=tool_call.name,
                    output=f"Screenshot error: {e}",
                    tool_call_id=tool_call_id
                )

        elif tool_name == "refresh_context":
            self.console.print("\n[cyan]Refreshing terminal context...[/]")
            try:
                # Clear cache to get fresh content
                try:
                    self.plugin_dbus.clear_cache()
                except Exception:
                    pass

                # Clear per-terminal content hashes to ensure full fresh context
                self.terminal_content_hashes.clear()

                # Capture fresh context (no deduplication - user/AI explicitly requested refresh)
                context_text, tui_attachments_refresh = self.capture_context(include_exec_output=True)
                result_attachments = []
                if tui_attachments_refresh:
                    result_attachments.extend(tui_attachments_refresh)

                if context_text or tui_attachments_refresh:
                    tui_info = f" + {len(tui_attachments_refresh)} TUI screenshot(s)" if tui_attachments_refresh else ""
                    self.console.print(f"[green]✓[/] Context refreshed ({len(context_text)} chars{tui_info})")

                    return ToolResult(
                        name=tool_call.name,
                        output=f"Refreshed terminal context:\n\n{context_text}",
                        attachments=result_attachments,
                        tool_call_id=tool_call_id
                    )
                else:
                    self.console.print("[yellow]No terminal content captured[/]")
                    return ToolResult(
                        name=tool_call.name,
                        output="No terminal content captured",
                        tool_call_id=tool_call_id
                    )
            except Exception as e:
                self.console.print(f"[red]Context refresh error: {e}[/]")
                return ToolResult(
                    name=tool_call.name,
                    output=f"Context refresh error: {e}",
                    tool_call_id=tool_call_id
                )

        elif tool_name == "view_attachment":
            path_or_url = get_str("path_or_url")
            if not path_or_url:
                return ToolResult(
                    name=tool_call.name,
                    output="Error: No path_or_url provided",
                    tool_call_id=tool_call_id
                )

            caps = self._get_model_capabilities()

            # Create attachment and detect MIME type
            try:
                attachment = self._create_attachment(path_or_url)
                mime_type = attachment.resolve_type()
            except Exception as e:
                return ToolResult(
                    name=tool_call.name,
                    output=f"Error resolving attachment: {e}",
                    tool_call_id=tool_call_id
                )

            # Check if model supports this type
            if mime_type not in caps['supported_types']:
                if mime_type == "application/pdf" and not caps['pdf']:
                    result = f"Model doesn't support PDF viewing. Use load_pdf tool for text extraction."
                elif mime_type and mime_type.startswith("audio/") and not caps['audio']:
                    result = f"Model doesn't support audio ({mime_type}). Only Gemini models can process audio."
                elif mime_type and mime_type.startswith("video/") and not caps['video']:
                    result = f"Model doesn't support video ({mime_type}). Only Gemini models can process video."
                else:
                    result = f"Unsupported attachment type: {mime_type}"
                self.console.print(f"[yellow]{result}[/]")
                return ToolResult(
                    name=tool_call.name,
                    output=result,
                    tool_call_id=tool_call_id
                )

            self.pending_attachments.append(attachment)
            self.console.print(f"[green]✓[/] Queued {mime_type}: {path_or_url}")
            return ToolResult(
                name=tool_call.name,
                output=f"Queued for viewing: {path_or_url} ({mime_type}). Will be visible in next turn.",
                tool_call_id=tool_call_id
            )

        elif tool_name == "view_pdf":
            path_or_url = get_str("path_or_url")

            if not path_or_url:
                return ToolResult(
                    name=tool_call.name,
                    output="Error: No path_or_url provided",
                    tool_call_id=tool_call_id
                )

            caps = self._get_model_capabilities()

            # Native PDF viewing only - no text extraction
            if not caps['pdf']:
                return ToolResult(
                    name=tool_call.name,
                    output="Error: Model doesn't support native PDF viewing. Use load_pdf for text extraction instead.",
                    tool_call_id=tool_call_id
                )

            try:
                attachment = self._create_attachment(path_or_url)
                self.pending_attachments.append(attachment)
                self.console.print(f"[green]✓[/] PDF queued for native viewing: {path_or_url}")
                result = f"PDF queued for native viewing in next turn: {path_or_url}"
            except Exception as e:
                result = f"Failed to queue PDF: {e}"

            return ToolResult(
                name=tool_call.name,
                output=result,
                tool_call_id=tool_call_id
            )

        elif tool_name == "view_youtube_native":
            url = get_str("url")
            if not url:
                return ToolResult(
                    name=tool_call.name,
                    output="Error: No URL provided",
                    tool_call_id=tool_call_id
                )

            caps = self._get_model_capabilities()

            if not caps['youtube']:
                # Non-Gemini model - suggest load_yt() instead
                return ToolResult(
                    name=tool_call.name,
                    output="Error: Current model doesn't support native YouTube video. Use load_yt(url) for transcript extraction instead (faster, cheaper, works with any model).",
                    tool_call_id=tool_call_id
                )

            try:
                attachment = Attachment(url=url)
                attachment.type = "video/youtube"  # Force YouTube type
                self.pending_attachments.append(attachment)
                self.console.print(f"[green]✓[/] YouTube video queued for native viewing")
                return ToolResult(
                    name=tool_call.name,
                    output=f"YouTube video queued for native viewing (visual + audio). Will process in next turn.",
                    tool_call_id=tool_call_id
                )
            except Exception as e:
                return ToolResult(
                    name=tool_call.name,
                    output=f"Failed to queue YouTube video: {e}",
                    tool_call_id=tool_call_id
                )

        elif tool_name in EXTERNAL_TOOLS:
            # Generic auto-dispatch for external tools (google_search, web_fetch, etc.)
            impl = EXTERNAL_TOOLS[tool_name]

            # Special handling for capture_screen with countdown display
            if tool_name == 'capture_screen':
                mode = tool_args.get('mode') or 'window'
                # Different default delays: automatic modes (rdp, full) don't need user prep time
                # Interactive modes (window, region, annotate) need time for user to position/click
                default_delay = 0 if mode in ('rdp', 'full') else 5
                try:
                    raw_delay = tool_args.get('delay')
                    delay = int(raw_delay) if raw_delay is not None else default_delay
                except (ValueError, TypeError):
                    delay = default_delay
                delay = max(0, min(delay, 30))

                # Display countdown if delay > 0 using Rich Status
                if delay > 0:
                    with Status(f"[cyan]Capturing screenshot ({mode}): {delay}s[/]", console=self.console, spinner="dots", spinner_style="cyan") as status:
                        for i in range(delay, 0, -1):
                            status.update(f"[cyan]Capturing screenshot ({mode}): {i}s[/]")
                            time.sleep(1)

                # Call tool with delay=0 since we already waited
                tool_args = dict(tool_args)
                tool_args['delay'] = 0

                # Use spinner while capturing
                try:
                    with Spinner(f"Capturing screenshot ({mode})...", console=self.console):
                        result = impl(**tool_args)
                    self.console.print(f"[green]✓[/] {tool_name} completed")

                    # Handle ToolOutput return type
                    attachments = []
                    if isinstance(result, ToolOutput):
                        attachments = result.attachments
                        result = result.output

                    if result is None:
                        result = ""
                    elif not isinstance(result, str):
                        result = json.dumps(result, default=repr)

                    return ToolResult(
                        name=tool_call.name,
                        output=result,
                        attachments=attachments,
                        tool_call_id=tool_call_id
                    )
                except Exception as e:
                    self.console.print(f"[red]{tool_name} error: {e}[/]")
                    return ToolResult(
                        name=tool_call.name,
                        output=f"Error calling {tool_name}: {e}",
                        tool_call_id=tool_call_id
                    )

            # Display primary parameter if configured, otherwise generic message
            # Build status message for spinner
            if tool_name in EXTERNAL_TOOL_DISPLAY:
                param_name, action_verb, _ = EXTERNAL_TOOL_DISPLAY[tool_name]
                param_value = tool_args.get(param_name, '')
                if param_value:
                    status_msg = f"{action_verb}: {param_value}"
                else:
                    status_msg = f"Calling {tool_name}..."
            else:
                status_msg = f"Calling {tool_name}..."

            try:
                with Spinner(status_msg, console=self.console):
                    result = impl(**tool_args)
                self.console.print(f"[dim cyan]{status_msg}[/]")
                self.console.print(f"[green]✓[/] {tool_name} completed")

                # Handle ToolOutput return type (e.g., capture_screen with attachments)
                attachments = []
                if isinstance(result, ToolOutput):
                    attachments = result.attachments
                    result = result.output

                # Ensure result is a string
                if result is None:
                    result = ""
                elif not isinstance(result, str):
                    result = json.dumps(result, default=repr)

                return ToolResult(
                    name=tool_call.name,
                    output=result,
                    attachments=attachments,
                    tool_call_id=tool_call_id
                )
            except Exception as e:
                self.console.print(f"[red]{tool_name} error: {e}[/]")
                return ToolResult(
                    name=tool_call.name,
                    output=f"Error calling {tool_name}: {e}",
                    tool_call_id=tool_call_id
                )

        else:
            # Unknown tool - shouldn't happen with our defined tools
            self.console.print(f"[yellow]Unknown tool: {tool_call.name}[/]")
            return ToolResult(
                name=tool_call.name,
                output=f"Unknown tool: {tool_call.name}",
                tool_call_id=tool_call_id
            )

    def run(self):
        """Main REPL loop with health checks"""
        # Connect to Terminator
        self._connect_to_terminator()

        # Setup terminals
        self.setup_terminals()

        # Build external tools list for display
        external_tools_list = ""
        if EXTERNAL_TOOLS:
            tools_info = []
            for name in sorted(EXTERNAL_TOOLS.keys()):
                if name in EXTERNAL_TOOL_DISPLAY:
                    _, _, desc = EXTERNAL_TOOL_DISPLAY[name]
                    tools_info.append(f"  [dim]•[/] {name}: {desc}")
                else:
                    tools_info.append(f"  [dim]•[/] {name}")
            if tools_info:
                external_tools_list = "\n\n[bold]External Tools:[/]\n" + "\n".join(tools_info)

        # Display welcome message
        self.console.print(Panel.fit(
            f"""Model: [cyan]{self.model_name}[/]{external_tools_list}

[bold]Commands:[/]
Type /help for slash commands
Type 'exit' or 'quit' to exit
Type !multi to enter multiple lines, then !end to finish
Type !fragment <name> [...] to insert fragments""",
            title="llm-assistant",
            border_style="green"
        ))

        # Main REPL loop with periodic health checks
        check_counter = 0

        # Multi-line input state
        in_multi = False
        accumulated_lines = []
        end_token = "!end"

        try:
            while True:
                # Periodic health check every 10 iterations
                check_counter += 1
                if check_counter >= 10:
                    # Check plugin availability
                    if not self._check_plugin_available():
                        self.console.print("[yellow]Plugin unavailable, attempting reconnect...[/]")
                        if not self._reconnect_plugin():
                            self.console.print("[red]Plugin reconnection failed. Please restart assistant.[/]")
                            break

                    # Check D-Bus connection
                    if not self._check_dbus_connection():
                        self.console.print("[yellow]D-Bus disconnected, attempting reconnect...[/]")
                        if not self._reconnect_dbus():
                            self.console.print("[red]D-Bus reconnection failed. Please restart assistant.[/]")
                            break

                    check_counter = 0

                # Get user input (change prompt based on mode)
                # Uses prompt_toolkit for Ctrl+Space voice toggle support
                try:
                    if in_multi:
                        # Multi-line mode - dim continuation prompt
                        user_input = self.prompt_session.prompt([('class:continuation', '... ')])
                    else:
                        # Dynamic prompt based on voice status
                        def get_prompt():
                            if self.voice_input and self.voice_input.status_message:
                                if "Recording" in self.voice_input.status_message:
                                    return [('class:prompt.recording', '\n⏺ ')]
                                elif "Transcribing" in self.voice_input.status_message:
                                    return [('class:prompt.transcribing', '\n⟳ ')]
                            return [('class:prompt', '\n> ')]
                        user_input = self.prompt_session.prompt(get_prompt).strip()
                except (KeyboardInterrupt, EOFError):
                    # Double-press protection
                    now = time.time()
                    if now - self._last_interrupt_time > 2.0:
                        self._last_interrupt_time = now
                        # Go up to overwrite the > prompt line, clear it, print message
                        # Then prompt's \n> will show blank line + new >
                        sys.stdout.write("\033[A\r\033[K")
                        sys.stdout.write("\033[90m(press again within 2 seconds to exit)\033[0m\n")
                        sys.stdout.flush()
                        continue
                    self.console.print("\n[yellow]Exiting...[/]")
                    break

                # Reset terminal state after prompt_toolkit to ensure clean state for Rich output
                # prompt_toolkit enables bracketed paste mode and application keypad mode
                sys.stdout.write('\x1b[?2004l\x1b>\x1b[?25h\x1b[0m')
                sys.stdout.flush()

                # Handle !multi command (start multi-line mode)
                if user_input.startswith("!multi"):
                    in_multi = True
                    bits = user_input.split()
                    if len(bits) > 1:
                        end_token = f"!end {' '.join(bits[1:])}"
                    else:
                        end_token = "!end"
                    self.console.print(f"[dim]Multi-line mode. Type '{end_token}' to finish[/]")
                    continue

                # Handle multi-line input accumulation
                if in_multi:
                    if user_input == end_token:
                        # Join accumulated lines and process
                        user_input = "\n".join(accumulated_lines)
                        accumulated_lines = []
                        in_multi = False
                    else:
                        # Accumulate this line
                        accumulated_lines.append(user_input)
                        continue

                if not user_input:
                    continue

                # Handle exit/quit commands (like llm chat mode)
                if user_input in ("exit", "quit"):
                    self.console.print("\n[yellow]Exiting...[/]")
                    self._shutdown()
                    break

                # Handle slash commands
                if user_input.startswith('/'):
                    should_continue = self.handle_slash_command(user_input)
                    if not should_continue:
                        break
                    continue

                # Check if we need to squash context (before lock to avoid holding it during squash)
                self.check_and_squash_context()

                # Send to AI with streaming
                # Pass system prompt on first interaction or when conversation is empty
                response_text = ""
                stream_success = False

                try:
                    self.console.print("\n[bold green]llm[/]")

                    # Thread-safe context capture AND conversation access
                    # Extended lock scope fixes race condition with watch mode
                    with self.watch_lock:
                        # Clear plugin cache inside lock to ensure atomic operation
                        try:
                            self.plugin_dbus.clear_cache()
                        except Exception:
                            pass  # Ignore if clear_cache not available

                        # Simple snapshot capture (no stability detection)
                        # Works better for TUI applications and provides instant feedback
                        # Returns (context_text, tui_attachments) tuple for TUI screenshot support
                        # Enable per-terminal deduplication after first message
                        has_prior_context = len(self.conversation.responses) > 0
                        context, tui_attachments = self.capture_context(
                            include_exec_output=True,
                            dedupe_unchanged=has_prior_context
                        )

                        # Process fragments if present
                        processed_input, fragments, fragment_attachments = self.process_fragments(user_input)

                        # Combine fragment attachments, TUI screenshots, and pending attachments
                        all_attachments = fragment_attachments + tui_attachments + self.pending_attachments

                        # Clear pending attachments after including them
                        self.pending_attachments = []

                        # RAG context retrieval (one-shot or persistent mode)
                        rag_context = ""
                        if self.pending_rag_context:
                            # One-shot: consume pending context from /rag search
                            rag_context = self.pending_rag_context
                            self.pending_rag_context = None
                            self._debug("RAG: using one-shot search results")
                        elif self.active_rag_collection and processed_input.strip():
                            # Persistent mode: search on every prompt
                            self._debug(f"RAG: searching {self.active_rag_collection}...")
                            rag_context = self._retrieve_rag_context(processed_input)
                            if rag_context:
                                self._debug("RAG: injecting context")

                        # Build prompt with context (order: terminal context → RAG context → user input)
                        prompt_parts = []

                        if context:
                            prompt_parts.append(f"<terminal_context>\n{context}\n</terminal_context>")

                        if rag_context:
                            prompt_parts.append(rag_context)

                        prompt_parts.append(processed_input)

                        full_prompt = "\n\n".join(prompt_parts)

                        # Prepend pending summary from squash (one-time use)
                        if self.pending_summary:
                            full_prompt = f"""<conversation_summary>
{self.pending_summary}
</conversation_summary>

{full_prompt}"""
                            self.pending_summary = None

                        # Broadcast user message to web companion
                        if self.web_clients:
                            self._broadcast_to_web({
                                "type": "user_message",
                                "content": processed_input
                            })

                        # Always pass system prompt on every call (required for Gemini/Vertex
                        # which is stateless - systemInstruction must be sent on every request)
                        # Also pass fragments and attachments (TUI screenshots)
                        # Include tools for structured output (schema validation)
                        response = self._prompt(
                            full_prompt,
                            system=self._build_system_prompt(),
                            fragments=[str(f) for f in fragments] if fragments else None,
                            attachments=all_attachments if all_attachments else None,
                            tools=ASSISTANT_TOOLS
                        )

                        # Stream response with Live markdown display INSIDE lock to prevent
                        # race condition with watch mode - watch thread will block until
                        # streaming completes, which is the desired behavior.
                        #
                        # TTS: If speech output is enabled, queue sentences to TTS
                        # as they complete (for low-latency audio output)
                        tts_enabled = (self.speech_output and self.speech_output.enabled
                                       and self._is_vertex_model())

                        # Stream response with Live markdown display
                        response_text = self._stream_response_with_display(response, tts_enabled=tts_enabled)

                        # Extract tool calls from the response (structured output)
                        tool_calls = list(response.tool_calls())

                        # Log response to database for --continue functionality
                        # INSIDE lock to prevent race with watch mode reading stripped prompt
                        self._log_response(response)

                    # Force display to ensure clean terminal state before tool processing
                    # (Live already displayed the response during streaming)
                    if response_text.strip():
                        self._force_display()

                    stream_success = True

                except Exception as e:
                    print()  # Ensure newline even on error
                    self.console.print(f"\n[red]Streaming error: {e}[/]")
                    self.console.print("[yellow]Response may be incomplete. Please try again.[/]")
                    # Don't process commands or update conversation on stream failure
                    continue

                # Only process tool calls if streaming succeeded
                if stream_success:
                    try:
                        # Process tool calls (structured output from model)
                        if tool_calls:
                            if self.debug:
                                self.console.print(f"\n[cyan]Processing {len(tool_calls)} tool call(s)[/]")

                            # Collect tool results for sending back to model
                            tool_results = []

                            for i, tool_call in enumerate(tool_calls, 1):
                                if len(tool_calls) > 1:
                                    self.console.print(f"\n[bold]Tool {i}/{len(tool_calls)}: {tool_call.name}[/]")
                                tool_results.append(self._process_tool_call(tool_call))

                            # After processing all tool calls, send results back to model
                            # Loop to handle multi-round tool calling
                            # Tool iteration limits:
                            # - auto mode: 1000 (unlimited, overrides mode)
                            # - agent mode: 100 (agentic)
                            # - assistant mode: 10 (conservative)
                            if self.auto_mode:
                                MAX_TOOL_ITERATIONS = 1000
                            elif self.mode == "agent":
                                MAX_TOOL_ITERATIONS = 100
                            else:  # assistant mode
                                MAX_TOOL_ITERATIONS = 10
                            iteration = 0

                            while tool_results and iteration < MAX_TOOL_ITERATIONS:
                                iteration += 1
                                if self.debug:
                                    self.console.print(f"\n[dim]Sending {len(tool_results)} tool result(s) to model...[/]")

                                # Extract attachments from tool results (e.g., capture_screen screenshots)
                                # This mirrors llm's ChainResponse behavior
                                tool_result_attachments = []
                                for tr in tool_results:
                                    if tr.attachments:
                                        tool_result_attachments.extend(tr.attachments)

                                # Include pending attachments from view_attachment calls (auto-send)
                                followup_attachments = list(tool_result_attachments)
                                if self.pending_attachments:
                                    followup_attachments.extend(self.pending_attachments)
                                    self.pending_attachments = []  # Clear after including

                                if self.debug and followup_attachments:
                                    self.console.print(f"[dim]Including {len(followup_attachments)} attachment(s) ({len(tool_result_attachments)} from tools)[/]")

                                with self.watch_lock:
                                    # Continue conversation with tool results (and attachments)
                                    followup_response = self._prompt(
                                        "",  # Empty prompt - tool results drive the continuation
                                        tools=ASSISTANT_TOOLS,
                                        tool_results=tool_results,
                                        attachments=followup_attachments if followup_attachments else None
                                    )

                                    # Stream follow-up response with Live markdown display
                                    followup_text = self._stream_response_with_display(followup_response, tts_enabled=tts_enabled)

                                    # Check if the model made more tool calls
                                    more_tool_calls = list(followup_response.tool_calls())

                                    # Log followup response to database
                                    # INSIDE lock to prevent race with watch mode reading stripped prompt
                                    self._log_response(followup_response)

                                # (Live already displayed the response during streaming)

                                # Process additional tool calls if any
                                if not more_tool_calls:
                                    break

                                if self.debug:
                                    self.console.print(f"\n[cyan]Processing {len(more_tool_calls)} additional tool call(s) (round {iteration + 1})[/]")
                                tool_results = []

                                for j, tool_call in enumerate(more_tool_calls, 1):
                                    if len(more_tool_calls) > 1:
                                        self.console.print(f"\n[bold]Tool {j}/{len(more_tool_calls)}: {tool_call.name}[/]")
                                    tool_results.append(self._process_tool_call(tool_call))

                            # Warn if max iterations reached and model still wants more
                            if iteration >= MAX_TOOL_ITERATIONS and more_tool_calls:
                                self.console.print(f"\n[yellow]Max tool iterations ({MAX_TOOL_ITERATIONS}) reached. Model requested {len(more_tool_calls)} more tool call(s). Please continue the conversation.[/]")

                            # Auto-send any remaining pending attachments (e.g., view_attachment was last tool call)
                            if self.pending_attachments:
                                if self.debug:
                                    self.console.print(f"\n[dim]Auto-sending {len(self.pending_attachments)} pending attachment(s)[/]")

                                attachments_to_send = self.pending_attachments
                                self.pending_attachments = []

                                with self.watch_lock:
                                    attachment_response = self._prompt(
                                        "",  # Empty prompt - attachments drive the continuation
                                        tools=ASSISTANT_TOOLS,
                                        attachments=attachments_to_send
                                    )

                                    # Stream attachment response with Live markdown display
                                    response_text = self._stream_response_with_display(attachment_response, tts_enabled=tts_enabled)

                                    more_calls = list(attachment_response.tool_calls())

                                    # Log attachment response to database
                                    # INSIDE lock to prevent race with watch mode reading stripped prompt
                                    self._log_response(attachment_response)

                                # (Live already displayed the response during streaming)

                                # Process any tool calls from viewing the attachment
                                if more_calls:
                                    for tc in more_calls:
                                        self._process_tool_call(tc)

                    except Exception as e:
                        self.console.print(f"\n[red]Tool execution error: {e}[/]")

        finally:
            # Unified cleanup - handles all resources
            self._shutdown()


def resolve_model_query(queries: list[str]) -> str | None:
    """
    Resolve model using fuzzy query matching (like llm -q).
    Returns first model matching ALL query strings.
    """
    if not queries:
        return None
    for model in llm.get_models():
        model_id = model.model_id.lower()
        if all(q.lower() in model_id for q in queries):
            return model.model_id
    return None


def main():
    """Entry point"""
    import argparse

    parser = argparse.ArgumentParser(
        description="Terminator AI Assistant - Terminal assistant for pair programming"
    )
    # llm-compatible model selection flags
    parser.add_argument(
        '-m', '--model',
        help='LLM model to use (e.g., azure/gpt-4.1-mini, gemini-2.5-flash)'
    )
    parser.add_argument(
        '-q', '--query',
        action='append',
        help='Select model by fuzzy matching (can be used multiple times)'
    )
    # Existing flags
    parser.add_argument(
        '--debug',
        action='store_true',
        help='Enable debug output for troubleshooting'
    )
    parser.add_argument(
        '--max-context',
        type=int,
        default=800000,
        help='Max context tokens before auto-squash (default: 800000)'
    )
    # Conversation persistence flags (llm-compatible)
    parser.add_argument(
        '-c', '--continue',
        dest='continue_',  # Underscore because 'continue' is a Python keyword
        action='store_true',
        help='Continue the most recent conversation'
    )
    parser.add_argument(
        '--cid', '--conversation',
        dest='conversation_id',
        help='Continue conversation with given ID'
    )
    parser.add_argument(
        '--no-log',
        action='store_true',
        help='Disable conversation logging to database'
    )
    parser.add_argument(
        '--agent',
        action='store_true',
        help='Start in agent mode (agentic, 100 tool iterations)'
    )
    args = parser.parse_args()

    # Resolve model: -m flag > query > default
    model_name = args.model
    if not model_name and args.query:
        model_name = resolve_model_query(args.query)
        if not model_name:
            print(f"Error: No model found matching queries {' '.join(args.query)}", file=sys.stderr)
            sys.exit(1)

    session = TerminatorAssistantSession(
        model_name=model_name,
        debug=args.debug,
        max_context_size=args.max_context,
        continue_=args.continue_,
        conversation_id=args.conversation_id,
        no_log=args.no_log,
        agent_mode=args.agent
    )
    session.run()


if __name__ == "__main__":
    main()
